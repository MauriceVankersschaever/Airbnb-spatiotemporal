{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the merged and cleaned calendar data\n",
    "calendar_data = pd.read_csv(r\"C:\\Users\\matth\\OneDrive\\Documents\\KU Leuven\\Thesis\\Merged_Data\\paris_merged_calendar.csv\")\n",
    "\n",
    "# Quick check of the data\n",
    "print(\"Dataset shape:\", calendar_data.shape)\n",
    "print(\"\\nColumns:\", calendar_data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(calendar_data.head())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(calendar_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the listings data\n",
    "listings_data = pd.read_csv(r\"C:\\Users\\matth\\OneDrive\\Documents\\KU Leuven\\Thesis\\Data_Mor\\paris\\2024-09-06\\listings.csv\")\n",
    "\n",
    "# Quick check of the data\n",
    "print(\"Dataset shape:\", listings_data.shape)\n",
    "print(\"\\nColumns:\", listings_data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(listings_data.head())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(listings_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns from listings DataFrame\n",
    "listings_cleaned = listings_data[['id', 'neighbourhood_cleansed', 'latitude', 'longitude']]\n",
    "\n",
    "# Verify the structures\n",
    "print(\"Listings shape:\", listings_cleaned.shape)\n",
    "print(\"\\nListings columns:\", listings_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(calendar_path, listings_path, n_listings=500):\n",
    "    \"\"\"Load and prepare the data with basic cleaning and sampling.\"\"\"\n",
    "    # Load listings first to get the sample\n",
    "    listings_df = pd.read_csv(listings_path)\n",
    "    \n",
    "    # Randomly sample n_listings\n",
    "    sampled_listings = listings_df['id'].sample(n=n_listings, random_state=42)\n",
    "    \n",
    "    # Clean and prepare listings data\n",
    "    listings_cleaned = listings_df[listings_df['id'].isin(sampled_listings)][\n",
    "        ['id', 'neighbourhood_cleansed', 'latitude', 'longitude']\n",
    "    ]\n",
    "    listings_cleaned = listings_cleaned.rename(columns={'id': 'listing_id'})\n",
    "    \n",
    "    # Load calendar data\n",
    "    calendar_df = pd.read_csv(calendar_path)\n",
    "    \n",
    "    # Filter calendar data for sampled listings\n",
    "    calendar_df = calendar_df[calendar_df['listing_id'].isin(sampled_listings)]\n",
    "    \n",
    "    # Merge calendar with listings data\n",
    "    df = pd.merge(calendar_df, listings_cleaned, on='listing_id', how='left')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"Total listings in sample: {len(df['listing_id'].unique())}\")\n",
    "    print(f\"Total records in sample: {len(df)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function with your file paths\n",
    "calendar_path = r\"C:\\Users\\matth\\OneDrive\\Documents\\KU Leuven\\Thesis\\Merged_Data\\paris_merged_calendar.csv\"\n",
    "listings_path = r\"C:\\Users\\matth\\OneDrive\\Documents\\KU Leuven\\Thesis\\Data_Mor\\paris\\2024-09-06\\listings.csv\"\n",
    "\n",
    "# Load and prepare the data\n",
    "df = load_and_prepare_data(calendar_path, listings_path, n_listings=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "class ParisDataPreparation:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()        \n",
    "    \n",
    "    def clean_price(self, df, price_column='price'):\n",
    "        \"\"\"Clean price values\"\"\"\n",
    "        # Remove currency symbols and convert to float\n",
    "        df[price_column] = df[price_column].replace('[\\$,€,£]', '', regex=True)\n",
    "        df[price_column] = df[price_column].astype(str).str.replace(',', '')\n",
    "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values for key columns\"\"\"\n",
    "        # For numerical columns\n",
    "        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        for col in numeric_columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # For categorical columns\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def clean_coordinates(self, df):\n",
    "        \"\"\"Clean and validate coordinates for Paris\"\"\"\n",
    "        # Paris coordinate bounds\n",
    "        PARIS_LAT_MIN, PARIS_LAT_MAX = 48.8, 48.9\n",
    "        PARIS_LON_MIN, PARIS_LON_MAX = 2.2, 2.5\n",
    "        \n",
    "        # Filter invalid coordinates\n",
    "        mask = (\n",
    "            (df['latitude'] >= PARIS_LAT_MIN) & \n",
    "            (df['latitude'] <= PARIS_LAT_MAX) &\n",
    "            (df['longitude'] >= PARIS_LON_MIN) & \n",
    "            (df['longitude'] <= PARIS_LON_MAX)\n",
    "        )\n",
    "        return df[mask]\n",
    "    \n",
    "    def handle_outliers(self, df, column='price', method='iqr'):\n",
    "        \"\"\"Remove outliers using IQR method\"\"\"\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    def prepare_date_features(self, df):\n",
    "        \"\"\"Create date-related features\"\"\"\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Paris high season (summer months and December)\n",
    "        df['is_high_season'] = df['month'].isin([6, 7, 8, 12]).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_basic_features(self, df):\n",
    "        \"\"\"Prepare basic features for baseline model\"\"\"\n",
    "        basic_features = [\n",
    "            'listing_id',\n",
    "            'date',\n",
    "            'price',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            'neighbourhood_cleansed' \n",
    "        ]\n",
    "        return df[basic_features]\n",
    "    \n",
    "    def prepare_advanced_features(self, df):\n",
    "        \"\"\"Add advanced features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Calculate distance to center (Chatelet as Paris center)\n",
    "        PARIS_CENTER_LAT = 48.8589\n",
    "        PARIS_CENTER_LON = 2.3469\n",
    "        df['distance_to_center'] = np.sqrt(\n",
    "            (df['latitude'] - PARIS_CENTER_LAT)**2 + \n",
    "            (df['longitude'] - PARIS_CENTER_LON)**2\n",
    "        )\n",
    "        \n",
    "        # Add temporal features\n",
    "        df = self.prepare_date_features(df)\n",
    "        \n",
    "        # Add neighborhood statistics\n",
    "        neighborhood_stats = df.groupby('neighbourhood_cleansed')['price'].agg(\n",
    "            ['mean', 'std']\n",
    "        ).reset_index()\n",
    "        df = df.merge(\n",
    "            neighborhood_stats, \n",
    "            on='neighbourhood_cleansed', \n",
    "            how='left',\n",
    "            suffixes=('', '_neighborhood_avg')\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_features(self, df, columns_to_scale):\n",
    "        \"\"\"Scale numerical features\"\"\"\n",
    "        df[columns_to_scale] = self.scaler.fit_transform(df[columns_to_scale])\n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, df, advanced=False):\n",
    "        \"\"\"Main preparation pipeline\"\"\"\n",
    "        # Make a copy to avoid modifying original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic cleaning\n",
    "        df = self.clean_price(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.handle_outliers(df)\n",
    "        df = self.prepare_date_features(df)\n",
    "        \n",
    "        if not advanced:\n",
    "            # Prepare basic features for baseline\n",
    "            df = self.prepare_basic_features(df)\n",
    "        else:\n",
    "            # Prepare advanced features\n",
    "            df = self.prepare_advanced_features(df)\n",
    "        \n",
    "        # Scale numerical features (except listing_id and date)\n",
    "        numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        numerical_columns = numerical_columns[~numerical_columns.isin(['listing_id'])]\n",
    "        df = self.scale_features(df, numerical_columns)\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# First, import required libraries and create the preprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = ParisDataPreparation()\n",
    "\n",
    "# Let's first look at the data before preparation\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"\\nOriginal DataFrame columns:\", df.columns.tolist())\n",
    "print(\"\\nMissing values before preparation:\\n\", df.isnull().sum())\n",
    "\n",
    "# Now apply the preparation for both baseline and advanced\n",
    "baseline_data = preprocessor.prepare_data(df, advanced=False)\n",
    "advanced_data = preprocessor.prepare_data(df, advanced=True)\n",
    "\n",
    "# Check the results\n",
    "print(\"\\nBaseline DataFrame shape:\", baseline_data.shape)\n",
    "print(\"Advanced DataFrame shape:\", advanced_data.shape)\n",
    "\n",
    "# Look at the first few rows of each\n",
    "print(\"\\nBaseline features:\\n\", baseline_data.columns.tolist())\n",
    "print(\"\\nAdvanced features:\\n\", advanced_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class BaselineModels:\n",
    "    def __init__(self):\n",
    "        self.linear_model = LinearRegression()\n",
    "        self.onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Fixed parameter name\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features for linear regression\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Extract month from date\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        # One-hot encode neighbourhood\n",
    "        neighborhood_encoded = self.onehot.fit_transform(df[['neighbourhood_cleansed']])\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = np.column_stack([\n",
    "            df[['latitude', 'longitude', 'month']],\n",
    "            neighborhood_encoded\n",
    "        ])\n",
    "        \n",
    "        return X, df['price']\n",
    "    \n",
    "    def train_test_split_temporal(self, df, test_size=0.2):\n",
    "        \"\"\"Split data temporally - last test_size portion as test set\"\"\"\n",
    "        df = df.sort_values('date')\n",
    "        split_idx = int(len(df) * (1 - test_size))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    def linear_regression_baseline(self, df, test_size=0.2):\n",
    "        \"\"\"Simple linear regression baseline with temporal split\"\"\"\n",
    "        # Split data temporally\n",
    "        train_df, test_df = self.train_test_split_temporal(df, test_size)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train, y_train = self.prepare_features(train_df)\n",
    "        X_test, y_test = self.prepare_features(test_df)\n",
    "        \n",
    "        # Fit model\n",
    "        self.linear_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = self.linear_model.predict(X_train)\n",
    "        test_pred = self.linear_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def moving_average_baseline(self, df, window_size=7):\n",
    "        \"\"\"Moving average baseline using only past data\"\"\"\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Sort by listing_id and date\n",
    "        df = df.sort_values(['listing_id', 'date'])\n",
    "        \n",
    "        # Calculate moving average using shift to avoid future data leakage\n",
    "        df['ma_prediction'] = df.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window_size, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Split data temporally\n",
    "        train_df, test_df = self.train_test_split_temporal(df)\n",
    "        \n",
    "        # Calculate metrics for test set only\n",
    "        mask = ~test_df['ma_prediction'].isna()  # Remove NaN predictions\n",
    "        \n",
    "        test_rmse = np.sqrt(mean_squared_error(\n",
    "            test_df.loc[mask, 'price'],\n",
    "            test_df.loc[mask, 'ma_prediction']\n",
    "        ))\n",
    "        \n",
    "        test_r2 = r2_score(\n",
    "            test_df.loc[mask, 'price'],\n",
    "            test_df.loc[mask, 'ma_prediction']\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "def plot_model_comparison(lr_results, ma7_results, ma30_results):\n",
    "    \"\"\"Plot comparison of model results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    models = ['Linear Regression', 'Moving Avg (7-day)', 'Moving Avg (30-day)']\n",
    "    rmse_values = [lr_results['test_rmse'], ma7_results['test_rmse'], ma30_results['test_rmse']]\n",
    "    ax1.bar(models, rmse_values, color=['blue', 'green', 'orange'])\n",
    "    ax1.set_title('Model Comparison - RMSE')\n",
    "    ax1.set_ylabel('RMSE (Lower is Better)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # R² comparison\n",
    "    r2_values = [lr_results['test_r2'], ma7_results['test_r2'], ma30_results['test_r2']]\n",
    "    ax2.bar(models, r2_values, color=['blue', 'green', 'orange'])\n",
    "    ax2.set_title('Model Comparison - R² Score')\n",
    "    ax2.set_ylabel('R² Score (Higher is Better)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Usage\n",
    "baselines = BaselineModels()\n",
    "\n",
    "# Linear regression baseline\n",
    "lr_results = baselines.linear_regression_baseline(df)\n",
    "print(\"\\nLinear Regression Results:\")\n",
    "print(f\"Train RMSE: {lr_results['train_rmse']:.2f}\")\n",
    "print(f\"Test RMSE: {lr_results['test_rmse']:.2f}\")\n",
    "print(f\"Train R²: {lr_results['train_r2']:.4f}\")\n",
    "print(f\"Test R²: {lr_results['test_r2']:.4f}\")\n",
    "\n",
    "# Moving average baselines\n",
    "ma7_results = baselines.moving_average_baseline(df, window_size=7)\n",
    "ma30_results = baselines.moving_average_baseline(df, window_size=30)\n",
    "\n",
    "print(\"\\nMoving Average (7-day) Results:\")\n",
    "print(f\"Test RMSE: {ma7_results['test_rmse']:.2f}\")\n",
    "print(f\"Test R²: {ma7_results['test_r2']:.4f}\")\n",
    "\n",
    "print(\"\\nMoving Average (30-day) Results:\")\n",
    "print(f\"Test RMSE: {ma30_results['test_rmse']:.2f}\")\n",
    "print(f\"Test R²: {ma30_results['test_r2']:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig = plot_model_comparison(lr_results, ma7_results, ma30_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "class BaselineModels:\n",
    "    def __init__(self):\n",
    "        self.linear_model = LinearRegression()\n",
    "        self.rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_features(self, df, train=True):\n",
    "        \"\"\"Prepare features for models\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Extract temporal features\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Prepare numerical features\n",
    "        numerical_features = ['latitude', 'longitude', 'month', 'day_of_week']\n",
    "        \n",
    "        if train:\n",
    "            # Fit and transform on training data\n",
    "            numerical_scaled = self.scaler.fit_transform(df[numerical_features])\n",
    "            neighborhood_encoded = self.onehot.fit_transform(df[['neighbourhood_cleansed']])\n",
    "        else:\n",
    "            # Only transform on test data\n",
    "            numerical_scaled = self.scaler.transform(df[numerical_features])\n",
    "            neighborhood_encoded = self.onehot.transform(df[['neighbourhood_cleansed']])\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = np.column_stack([numerical_scaled, neighborhood_encoded])\n",
    "        \n",
    "        return X, df['price']\n",
    "    \n",
    "    def train_test_split_temporal(self, df, test_size=0.2):\n",
    "        \"\"\"Split data temporally - last test_size portion as test set\"\"\"\n",
    "        df = df.sort_values('date')\n",
    "        split_idx = int(len(df) * (1 - test_size))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    def linear_regression_baseline(self, df, test_size=0.2):\n",
    "        \"\"\"Linear regression baseline\"\"\"\n",
    "        # Split data temporally\n",
    "        train_df, test_df = self.train_test_split_temporal(df, test_size)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train, y_train = self.prepare_features(train_df, train=True)\n",
    "        X_test, y_test = self.prepare_features(test_df, train=False)\n",
    "        \n",
    "        # Fit model\n",
    "        self.linear_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = self.linear_model.predict(X_train)\n",
    "        test_pred = self.linear_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def random_forest_baseline(self, df, test_size=0.2):\n",
    "        \"\"\"Random Forest baseline\"\"\"\n",
    "        # Split data temporally\n",
    "        train_df, test_df = self.train_test_split_temporal(df, test_size)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train, y_train = self.prepare_features(train_df, train=True)\n",
    "        X_test, y_test = self.prepare_features(test_df, train=False)\n",
    "        \n",
    "        # Fit model\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = self.rf_model.predict(X_train)\n",
    "        test_pred = self.rf_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "def plot_model_comparison(lr_results, rf_results):\n",
    "    \"\"\"Plot comparison of model results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    models = ['Linear Regression', 'Random Forest']\n",
    "    train_rmse = [lr_results['train_rmse'], rf_results['train_rmse']]\n",
    "    test_rmse = [lr_results['test_rmse'], rf_results['test_rmse']]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, train_rmse, width, label='Train')\n",
    "    ax1.bar(x + width/2, test_rmse, width, label='Test')\n",
    "    ax1.set_title('Model Comparison - RMSE')\n",
    "    ax1.set_ylabel('RMSE (Lower is Better)')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # R² comparison\n",
    "    train_r2 = [lr_results['train_r2'], rf_results['train_r2']]\n",
    "    test_r2 = [lr_results['test_r2'], rf_results['test_r2']]\n",
    "    \n",
    "    ax2.bar(x - width/2, train_r2, width, label='Train')\n",
    "    ax2.bar(x + width/2, test_r2, width, label='Test')\n",
    "    ax2.set_title('Model Comparison - R² Score')\n",
    "    ax2.set_ylabel('R² Score (Higher is Better)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Usage\n",
    "print(\"Starting model evaluation...\")\n",
    "baselines = BaselineModels()\n",
    "\n",
    "# Linear regression baseline\n",
    "print(\"Running Linear Regression...\")\n",
    "lr_results = baselines.linear_regression_baseline(df)\n",
    "print(\"\\nLinear Regression Results:\")\n",
    "print(f\"Train RMSE: {lr_results['train_rmse']:.2f}\")\n",
    "print(f\"Test RMSE: {lr_results['test_rmse']:.2f}\")\n",
    "print(f\"Train R²: {lr_results['train_r2']:.4f}\")\n",
    "print(f\"Test R²: {lr_results['test_r2']:.4f}\")\n",
    "\n",
    "# Random Forest baseline\n",
    "print(\"\\nRunning Random Forest...\")\n",
    "rf_results = baselines.random_forest_baseline(df)\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Train RMSE: {rf_results['train_rmse']:.2f}\")\n",
    "print(f\"Test RMSE: {rf_results['test_rmse']:.2f}\")\n",
    "print(f\"Train R²: {rf_results['train_r2']:.4f}\")\n",
    "print(f\"Test R²: {rf_results['test_r2']:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig = plot_model_comparison(lr_results, rf_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "class SpatioTemporalEvaluation:\n",
    "    def __init__(self):\n",
    "        self.linear_model = LinearRegression()\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_features(self, df, train=True):\n",
    "        \"\"\"Prepare features for models\"\"\"\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        numerical_features = ['latitude', 'longitude', 'month']\n",
    "        \n",
    "        if train:\n",
    "            numerical_scaled = self.scaler.fit_transform(df[numerical_features])\n",
    "            neighborhood_encoded = self.onehot.fit_transform(df[['neighbourhood_cleansed']])\n",
    "        else:\n",
    "            numerical_scaled = self.scaler.transform(df[numerical_features])\n",
    "            neighborhood_encoded = self.onehot.transform(df[['neighbourhood_cleansed']])\n",
    "        \n",
    "        X = np.column_stack([numerical_scaled, neighborhood_encoded])\n",
    "        return X, df['price']\n",
    "    \n",
    "    def evaluate_temporal_performance(self, y_true, y_pred, dates):\n",
    "        \"\"\"Evaluate model performance across different time periods\"\"\"\n",
    "        dates = pd.to_datetime(dates)\n",
    "        temporal_results = {}\n",
    "        \n",
    "        # Monthly performance\n",
    "        for month in sorted(dates.dt.month.unique()):\n",
    "            mask = dates.dt.month == month\n",
    "            if sum(mask) > 0:  # Only if we have data for this month\n",
    "                rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
    "                r2 = r2_score(y_true[mask], y_pred[mask])\n",
    "                temporal_results[f'Month_{month}'] = {'rmse': rmse, 'r2': r2}\n",
    "        \n",
    "        # Calculate month-to-month stability\n",
    "        monthly_errors = pd.Series(index=dates.dt.month.unique())\n",
    "        for month in dates.dt.month.unique():\n",
    "            mask = dates.dt.month == month\n",
    "            monthly_errors[month] = np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "        \n",
    "        temporal_results['monthly_error_std'] = monthly_errors.std()\n",
    "        \n",
    "        return temporal_results\n",
    "    \n",
    "    def evaluate_spatial_performance(self, y_true, y_pred, neighborhoods):\n",
    "        \"\"\"Evaluate model performance across different neighborhoods\"\"\"\n",
    "        spatial_results = {}\n",
    "        \n",
    "        # Neighborhood-wise performance\n",
    "        for neighborhood in sorted(neighborhoods.unique()):\n",
    "            mask = neighborhoods == neighborhood\n",
    "            if sum(mask) > 0:  # Only if we have data for this neighborhood\n",
    "                rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
    "                r2 = r2_score(y_true[mask], y_pred[mask])\n",
    "                spatial_results[neighborhood] = {'rmse': rmse, 'r2': r2}\n",
    "        \n",
    "        # Calculate spatial autocorrelation of errors\n",
    "        neighborhood_errors = pd.Series(index=neighborhoods.unique())\n",
    "        for neighborhood in neighborhoods.unique():\n",
    "            mask = neighborhoods == neighborhood\n",
    "            neighborhood_errors[neighborhood] = np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "        \n",
    "        spatial_results['neighborhood_error_std'] = neighborhood_errors.std()\n",
    "        \n",
    "        return spatial_results\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, X_test, y_train, y_test, test_df):\n",
    "        \"\"\"Evaluate a model with both temporal and spatial metrics\"\"\"\n",
    "        # Fit model and make predictions\n",
    "        model.fit(X_train, y_train)\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_results = {\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        # Temporal evaluation\n",
    "        temporal_results = self.evaluate_temporal_performance(\n",
    "            y_test, test_pred, test_df['date']\n",
    "        )\n",
    "        \n",
    "        # Spatial evaluation\n",
    "        spatial_results = self.evaluate_spatial_performance(\n",
    "            y_test, test_pred, test_df['neighbourhood_cleansed']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_results,\n",
    "            'temporal': temporal_results,\n",
    "            'spatial': spatial_results\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, df):\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        # Temporal train-test split\n",
    "        df = df.sort_values('date')\n",
    "        split_idx = int(len(df) * 0.8)\n",
    "        train_df = df.iloc[:split_idx]\n",
    "        test_df = df.iloc[split_idx:]\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train, y_train = self.prepare_features(train_df, train=True)\n",
    "        X_test, y_test = self.prepare_features(test_df, train=False)\n",
    "        \n",
    "        # Evaluate Linear Regression\n",
    "        lr_results = self.evaluate_model(\n",
    "            self.linear_model, X_train, X_test, y_train, y_test, test_df\n",
    "        )\n",
    "        \n",
    "        # Evaluate Random Forest\n",
    "        rf_results = self.evaluate_model(\n",
    "            self.rf_model, X_train, X_test, y_train, y_test, test_df\n",
    "        )\n",
    "        \n",
    "        return lr_results, rf_results\n",
    "\n",
    "def plot_evaluation_results(lr_results, rf_results):\n",
    "    \"\"\"Plot comprehensive evaluation results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Overall Performance\n",
    "    models = ['Linear Regression', 'Random Forest']\n",
    "    metrics = ['test_rmse', 'test_r2']\n",
    "    values = [\n",
    "        [lr_results['overall']['test_rmse'], rf_results['overall']['test_rmse']],\n",
    "        [lr_results['overall']['test_r2'], rf_results['overall']['test_r2']]\n",
    "    ]\n",
    "    \n",
    "    ax1.bar(models, values[0])\n",
    "    ax1.set_title('Overall RMSE')\n",
    "    ax1.set_ylabel('RMSE (Lower is Better)')\n",
    "    \n",
    "    ax2.bar(models, values[1])\n",
    "    ax2.set_title('Overall R²')\n",
    "    ax2.set_ylabel('R² Score (Higher is Better)')\n",
    "    \n",
    "    # Temporal Stability\n",
    "    months = sorted([k for k in lr_results['temporal'].keys() if k.startswith('Month')])\n",
    "    lr_temporal = [lr_results['temporal'][m]['rmse'] for m in months]\n",
    "    rf_temporal = [rf_results['temporal'][m]['rmse'] for m in months]\n",
    "    \n",
    "    ax3.plot(months, lr_temporal, label='Linear Regression', marker='o')\n",
    "    ax3.plot(months, rf_temporal, label='Random Forest', marker='o')\n",
    "    ax3.set_title('Monthly RMSE')\n",
    "    ax3.set_xlabel('Month')\n",
    "    ax3.set_ylabel('RMSE')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Spatial Performance\n",
    "    neighborhoods = sorted([k for k in lr_results['spatial'].keys() \n",
    "                          if k != 'neighborhood_error_std'])[:5]  # Top 5 neighborhoods\n",
    "    lr_spatial = [lr_results['spatial'][n]['rmse'] for n in neighborhoods]\n",
    "    rf_spatial = [rf_results['spatial'][n]['rmse'] for n in neighborhoods]\n",
    "    \n",
    "    x = np.arange(len(neighborhoods))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, lr_spatial, width, label='Linear Regression')\n",
    "    ax4.bar(x + width/2, rf_spatial, width, label='Random Forest')\n",
    "    ax4.set_title('Neighborhood RMSE (Top 5)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(neighborhoods, rotation=45)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Usage\n",
    "evaluator = SpatioTemporalEvaluation()\n",
    "lr_results, rf_results = evaluator.run_evaluation(df)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nLinear Regression Results:\")\n",
    "print(f\"Overall RMSE: {lr_results['overall']['test_rmse']:.2f}\")\n",
    "print(f\"Monthly Error Stability: {lr_results['temporal']['monthly_error_std']:.2f}\")\n",
    "print(f\"Spatial Error Stability: {lr_results['spatial']['neighborhood_error_std']:.2f}\")\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Overall RMSE: {rf_results['overall']['test_rmse']:.2f}\")\n",
    "print(f\"Monthly Error Stability: {rf_results['temporal']['monthly_error_std']:.2f}\")\n",
    "print(f\"Spatial Error Stability: {rf_results['spatial']['neighborhood_error_std']:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "fig = plot_evaluation_results(lr_results, rf_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
