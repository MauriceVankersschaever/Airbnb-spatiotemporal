{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:53: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\mvk\\AppData\\Local\\Temp\\ipykernel_21280\\510446162.py:53: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  df_tsfresh['price'] = pd.to_numeric(df_tsfresh['price'].replace('[\\$,]', '', regex=True), errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Total listings in sample: 333\n",
      "Total records in sample: 69760\n",
      "Date range: 2023-06-07 00:00:00 to 2024-02-11 00:00:00\n",
      "Extracting tsfresh features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 36/36 [00:05<00:00,  6.33it/s]\n",
      "Feature Extraction: 100%|██████████| 330/330 [00:00<00:00, 2732.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 8 valid tsfresh features\n",
      "Preparing final feature set...\n",
      "\n",
      "Training XGBoost model...\n",
      "\n",
      "Model Performance Metrics:\n",
      "RMSE: 40.64\n",
      "MAE: 16.65\n",
      "R2 Score: 0.9777\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                           feature  importance\n",
      "8                      price__mean    0.263537\n",
      "5   neighbourhood_cleansed_encoded    0.261083\n",
      "3                       season_sin    0.133075\n",
      "12                  price__maximum    0.117120\n",
      "1                            month    0.059282\n",
      "6                         latitude    0.053213\n",
      "9                    price__median    0.033558\n",
      "4                       season_cos    0.019039\n",
      "7                        longitude    0.018045\n",
      "14              price__mean_change    0.015471\n",
      "Preparing final feature set...\n",
      "\n",
      "=== Tsfresh Feature Inspection ===\n",
      "\n",
      "Number of tsfresh features extracted: 8\n",
      "\n",
      "Sample of tsfresh features with statistics:\n",
      "\n",
      "Feature: price__mean_change\n",
      "Mean: 0.012\n",
      "Std: 0.264\n",
      "Min: -0.288\n",
      "Max: 4.701\n",
      "\n",
      "Feature: price__standard_deviation\n",
      "Mean: 30.080\n",
      "Std: 162.104\n",
      "Min: 0.000\n",
      "Max: 2815.327\n",
      "\n",
      "Feature: price__variance\n",
      "Mean: 27102.903\n",
      "Std: 436382.919\n",
      "Min: 0.000\n",
      "Max: 7926067.047\n",
      "\n",
      "Feature: price__mean_abs_change\n",
      "Mean: 0.374\n",
      "Std: 1.314\n",
      "Min: 0.000\n",
      "Max: 14.913\n",
      "\n",
      "Feature: price__median\n",
      "Mean: 183.791\n",
      "Std: 214.737\n",
      "Min: 10.000\n",
      "Max: 1700.000\n",
      "\n",
      "Features with very low variance: 0\n",
      "\n",
      "Number of tsfresh features in final dataset: 8\n",
      "\n",
      "Sample of tsfresh features in final dataset:\n",
      "\n",
      "price__mean:\n",
      "Mean: 177.312\n",
      "Std: 214.078\n",
      "\n",
      "price__median:\n",
      "Mean: 171.269\n",
      "Std: 190.016\n",
      "\n",
      "price__standard_deviation:\n",
      "Mean: 30.667\n",
      "Std: 144.038\n",
      "\n",
      "price__variance:\n",
      "Mean: 21687.142\n",
      "Std: 380062.901\n",
      "\n",
      "price__maximum:\n",
      "Mean: 248.329\n",
      "Std: 434.631\n",
      "\n",
      "=== Feature Set Composition ===\n",
      "Total features: 16\n",
      "Basic features: 8\n",
      "Tsfresh features: 8\n",
      "\n",
      "Feature composition summary:\n",
      "feature_type\n",
      "basic      8\n",
      "tsfresh    8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "def load_and_prepare_data(calendar_path, listings_path, n_listings=500):\n",
    "    \"\"\"Load and prepare the data with basic cleaning and sampling.\"\"\"\n",
    "    # Load listings first to get the sample\n",
    "    listings_df = pd.read_csv(listings_path)\n",
    "    sampled_listings = listings_df['id'].sample(n=n_listings, random_state=42)\n",
    "    \n",
    "    # Clean and prepare listings data\n",
    "    listings_cleaned = listings_df[listings_df['id'].isin(sampled_listings)][\n",
    "        ['id', 'neighbourhood_cleansed', 'latitude', 'longitude']\n",
    "    ]\n",
    "    listings_cleaned = listings_cleaned.rename(columns={'id': 'listing_id'})\n",
    "    \n",
    "    # Load and filter calendar data\n",
    "    calendar_df = pd.read_csv(calendar_path)\n",
    "    calendar_df = calendar_df[calendar_df['listing_id'].isin(sampled_listings)]\n",
    "    calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
    "    \n",
    "    # Filter out future dates beyond our current cutoff\n",
    "    current_date = pd.Timestamp('2024-02-11')  # Set to current date\n",
    "    calendar_df = calendar_df[calendar_df['date'] <= current_date]\n",
    "    \n",
    "    # Merge calendar with listings data\n",
    "    df = pd.merge(calendar_df, listings_cleaned, on='listing_id', how='left')\n",
    "    \n",
    "    print(f\"Total listings in sample: {len(df['listing_id'].unique())}\")\n",
    "    print(f\"Total records in sample: {len(df)}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Create temporal features from the date column.\"\"\"\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['season_sin'] = np.sin(2 * np.pi * df['date'].dt.dayofyear/365.25)\n",
    "    df['season_cos'] = np.cos(2 * np.pi * df['date'].dt.dayofyear/365.25)\n",
    "    return df\n",
    "\n",
    "def extract_tsfresh_features(df):\n",
    "    \"\"\"Extract time series features using tsfresh with improved parameters.\"\"\"\n",
    "    # Create a proper time series dataframe for tsfresh\n",
    "    df_tsfresh = df[['listing_id', 'date', 'price']].copy()\n",
    "    df_tsfresh['price'] = pd.to_numeric(df_tsfresh['price'].replace('[\\$,]', '', regex=True), errors='coerce')\n",
    "    df_tsfresh = df_tsfresh.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    # Create rolling windows\n",
    "    df_rolled = roll_time_series(\n",
    "        df_tsfresh,\n",
    "        column_id='listing_id',\n",
    "        column_sort='date',\n",
    "        max_timeshift=7,\n",
    "        rolling_direction=1\n",
    "    )\n",
    "    \n",
    "    # Define minimal but meaningful feature set\n",
    "    fc_parameters = {\n",
    "        \"mean\": None,\n",
    "        \"median\": None,\n",
    "        \"standard_deviation\": None,\n",
    "        \"variance\": None,\n",
    "        \"maximum\": None,\n",
    "        \"minimum\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        features_filtered = extract_features(\n",
    "            df_rolled,\n",
    "            column_id='listing_id',\n",
    "            column_sort='date',\n",
    "            column_value='price',\n",
    "            default_fc_parameters=fc_parameters,\n",
    "            n_jobs=0\n",
    "        )\n",
    "        \n",
    "        # Validate features\n",
    "        features_filtered = features_filtered.replace([np.inf, -np.inf], np.nan)\n",
    "        features_filtered = features_filtered.dropna(axis=1, how='all')\n",
    "        non_constant_cols = features_filtered.columns[features_filtered.nunique() > 1]\n",
    "        features_filtered = features_filtered[non_constant_cols]\n",
    "        \n",
    "        # Ensure index is listing_id\n",
    "        features_filtered.index = features_filtered.index.astype(int)\n",
    "        \n",
    "        print(f\"\\nExtracted {len(non_constant_cols)} valid tsfresh features\")\n",
    "        return features_filtered\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in tsfresh feature extraction: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_features(df, tsfresh_features):\n",
    "    \"\"\"Prepare final feature set.\"\"\"\n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    df['neighbourhood_cleansed_encoded'] = le.fit_transform(df['neighbourhood_cleansed'].fillna('Unknown'))\n",
    "    \n",
    "    # Basic features\n",
    "    basic_features = [\n",
    "        'day_of_week', 'month', 'is_weekend', 'season_sin', 'season_cos',\n",
    "         'neighbourhood_cleansed_encoded',\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    X_basic = df[basic_features]\n",
    "    \n",
    "    # Combine with tsfresh features if any were extracted\n",
    "    if not tsfresh_features.empty:\n",
    "        # Reset index of basic features to match with tsfresh features\n",
    "        X_basic = X_basic.reset_index(drop=True)\n",
    "        \n",
    "        # Ensure tsfresh features are properly aligned with the basic features\n",
    "        # by matching on listing_id\n",
    "        tsfresh_features = tsfresh_features.reindex(df['listing_id'].unique())\n",
    "        \n",
    "        # Merge tsfresh features with basic features\n",
    "        X_combined = pd.DataFrame()\n",
    "        for listing_id in df['listing_id'].unique():\n",
    "            # Get basic features for this listing\n",
    "            listing_basic = X_basic[df['listing_id'] == listing_id]\n",
    "            \n",
    "            # Get tsfresh features for this listing\n",
    "            listing_tsfresh = tsfresh_features.loc[listing_id:listing_id]\n",
    "            \n",
    "            # Repeat tsfresh features for each row of this listing\n",
    "            listing_tsfresh_repeated = pd.DataFrame(\n",
    "                np.repeat(listing_tsfresh.values, len(listing_basic), axis=0),\n",
    "                columns=listing_tsfresh.columns,\n",
    "                index=listing_basic.index\n",
    "            )\n",
    "            \n",
    "            # Combine basic and tsfresh features for this listing\n",
    "            listing_combined = pd.concat([listing_basic, listing_tsfresh_repeated], axis=1)\n",
    "            X_combined = pd.concat([X_combined, listing_combined])\n",
    "    else:\n",
    "        X_combined = X_basic\n",
    "    \n",
    "    return X_combined.fillna(0)\n",
    "\n",
    "def inspect_tsfresh_features(tsfresh_features, X_combined):\n",
    "    \"\"\"Enhanced inspection of tsfresh features.\"\"\"\n",
    "    print(\"\\n=== Tsfresh Feature Inspection ===\")\n",
    "    \n",
    "    if tsfresh_features.empty:\n",
    "        print(\"No tsfresh features were extracted!\")\n",
    "        return []\n",
    "    \n",
    "    # Print tsfresh features info\n",
    "    print(f\"\\nNumber of tsfresh features extracted: {tsfresh_features.shape[1]}\")\n",
    "    \n",
    "    # Show sample of features with their statistics\n",
    "    print(\"\\nSample of tsfresh features with statistics:\")\n",
    "    sample_features = tsfresh_features.sample(min(5, tsfresh_features.shape[1]), axis=1)\n",
    "    for col in sample_features.columns:\n",
    "        stats = sample_features[col].describe()\n",
    "        print(f\"\\nFeature: {col}\")\n",
    "        print(f\"Mean: {stats['mean']:.3f}\")\n",
    "        print(f\"Std: {stats['std']:.3f}\")\n",
    "        print(f\"Min: {stats['min']:.3f}\")\n",
    "        print(f\"Max: {stats['max']:.3f}\")\n",
    "    \n",
    "    # Check feature variance\n",
    "    low_variance_features = tsfresh_features.columns[tsfresh_features.std() < 1e-6]\n",
    "    print(f\"\\nFeatures with very low variance: {len(low_variance_features)}\")\n",
    "    \n",
    "    # Identify tsfresh features in final dataset\n",
    "    tsfresh_cols = [col for col in X_combined.columns if '__' in col]\n",
    "    \n",
    "    print(f\"\\nNumber of tsfresh features in final dataset: {len(tsfresh_cols)}\")\n",
    "    if tsfresh_cols:\n",
    "        print(\"\\nSample of tsfresh features in final dataset:\")\n",
    "        for col in tsfresh_cols[:5]:\n",
    "            stats = X_combined[col].describe()\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Mean: {stats['mean']:.3f}\")\n",
    "            print(f\"Std: {stats['std']:.3f}\")\n",
    "    \n",
    "    return tsfresh_cols\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    calendar_path = r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\paris_merged_calendar.csv'\n",
    "    listings_path = r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\2024-06-10\\listings.csv'\n",
    "    \n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_and_prepare_data(calendar_path, listings_path, n_listings=500)\n",
    "    df = create_temporal_features(df)\n",
    "    \n",
    "\n",
    "\n",
    "    # Reset index and convert prices\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['price_numeric'] = pd.to_numeric(df['price'].replace(r'[\\$,]', '', regex=True), errors='coerce')\n",
    "\n",
    "    # Calculate price cutoff and create mask\n",
    "    price_cutoff = df['price_numeric'].quantile(0.995)\n",
    "    mask = df['price_numeric'] <= price_cutoff\n",
    "\n",
    "    # Get list of valid listing IDs\n",
    "    valid_listings = df[mask]['listing_id'].unique()\n",
    "\n",
    "    # Remove listings above cutoff\n",
    "    df = df[df['listing_id'].isin(valid_listings)].copy()\n",
    "\n",
    "    # Final cleanup\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.dropna(subset=['price_numeric'])\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Extracting tsfresh features...\")\n",
    "    tsfresh_features = extract_tsfresh_features(df)\n",
    "    \n",
    "    print(\"Preparing final feature set...\")\n",
    "    X = prepare_features(df, tsfresh_features)\n",
    "    y = pd.to_numeric(df['price'].replace(r'[\\$,]', '', regex=True), errors='coerce')    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize and train XGBoost\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    y_pred = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "\n",
    "        # Prepare final feature set\n",
    "    print(\"Preparing final feature set...\")\n",
    "    X = prepare_features(df, tsfresh_features)\n",
    "    \n",
    "    # Inspect tsfresh features\n",
    "    tsfresh_cols = inspect_tsfresh_features(tsfresh_features, X)\n",
    "    \n",
    "    # Print feature composition\n",
    "    print(\"\\n=== Feature Set Composition ===\")\n",
    "    print(f\"Total features: {X.shape[1]}\")\n",
    "    print(f\"Basic features: {X.shape[1] - len(tsfresh_cols)}\")\n",
    "    print(f\"Tsfresh features: {len(tsfresh_cols)}\")\n",
    "    \n",
    "    # Optional: Save feature names for reference\n",
    "    feature_composition = pd.DataFrame({\n",
    "        'feature_name': X.columns,\n",
    "        'feature_type': ['tsfresh' if col in tsfresh_cols else 'basic' for col in X.columns]\n",
    "    })\n",
    "    print(\"\\nFeature composition summary:\")\n",
    "    print(feature_composition['feature_type'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
