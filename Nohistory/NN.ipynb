{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new listing price prediction pipeline...\n",
      "Loading data...\n",
      "Loading spatial features from cache: C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\cache\\spatial_features_train_781ee4073c.pkl\n",
      "Cache loaded successfully\n",
      "Loading spatial features from cache: C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\cache\\spatial_features_test_9f5884e967.pkl\n",
      "Cache loaded successfully\n",
      "Training data size: 1312312\n",
      "Validation data size: 328077\n",
      "Property features dimension: 22\n",
      "Amenities features dimension: 20\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/20505 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NewListingPricePredictor.forward() takes 3 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 971\u001b[0m\n\u001b[0;32m    968\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(output_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m model, predictions, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_new_listing_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce neighbor count to avoid issues with small datasets\u001b[39;49;00m\n\u001b[0;32m    976\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 680\u001b[0m, in \u001b[0;36mrun_new_listing_prediction\u001b[1;34m(train_path, test_path, output_path, use_validation, val_split, k_neighbors)\u001b[0m\n\u001b[0;32m    677\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m model, training_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_new_listing_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_neighbors\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path:\n",
      "Cell \u001b[1;32mIn[5], line 484\u001b[0m, in \u001b[0;36mtrain_new_listing_model\u001b[1;34m(train_data, validation_data, epochs, batch_size, learning_rate, k_neighbors)\u001b[0m\n\u001b[0;32m    481\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Forward pass without edge_index \u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamenity_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemporal_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighborhood_feat\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m    493\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: NewListingPricePredictor.forward() takes 3 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import BallTree\n",
    "from math import radians\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class NewListingPricePredictor(nn.Module):\n",
    "    \"\"\"Neural network model to predict prices for new listings without price history\"\"\"\n",
    "    def __init__(self, input_dim, amenities_dim=20, hidden_dim=128):\n",
    "        super(NewListingPricePredictor, self).__init__()\n",
    "        \n",
    "        # Save dimensions for debugging\n",
    "        self.input_dim = input_dim\n",
    "        self.amenities_dim = amenities_dim\n",
    "        \n",
    "        # Property feature embedding\n",
    "        self.property_embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Amenities-specific embedding\n",
    "        self.amenities_embedding = nn.Sequential(\n",
    "            nn.Linear(amenities_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers (simpler architecture)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, property_features, amenities_features, **kwargs):\n",
    "        # Simple forward pass without graph convolution for debugging\n",
    "        property_emb = self.property_embedding(property_features)\n",
    "        amenities_emb = self.amenities_embedding(amenities_features)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([property_emb, amenities_emb], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.predictor(combined).squeeze(-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def get_cache_path(data_path, prefix=\"spatial_features\"):\n",
    "    \"\"\"Generate a cache file path based on the input data path\"\"\"\n",
    "    # Create a hash of the data path to use in the cache filename\n",
    "    data_hash = hashlib.md5(data_path.encode()).hexdigest()[:10]\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    cache_dir = os.path.join(os.path.dirname(data_path), \"cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Return the cache file path\n",
    "    return os.path.join(cache_dir, f\"{prefix}_{data_hash}.pkl\")\n",
    "\n",
    "def create_enhanced_spatial_features(df, k_neighbors=10, chunk_size=1000, cache_path=None):\n",
    "    \"\"\"\n",
    "    Create enhanced spatial features for new listings prediction\n",
    "    \n",
    "    This function creates:\n",
    "    1. Distance to city center\n",
    "    2. North-south and east-west position\n",
    "    3. K-nearest neighbor statistics\n",
    "    4. Neighborhood aggregate statistics\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with listing data\n",
    "        k_neighbors: Number of neighbors to use for KNN features\n",
    "        chunk_size: Size of chunks for processing\n",
    "        cache_path: Path to cache file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with enhanced spatial features\n",
    "    \"\"\"\n",
    "    # Check if cache exists and load from it if available\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        print(f\"Loading spatial features from cache: {cache_path}\")\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                spatial_features_df = pickle.load(f)\n",
    "            \n",
    "            # Verify that the cache contains the expected number of rows\n",
    "            if len(spatial_features_df) == len(df):\n",
    "                print(\"Cache loaded successfully\")\n",
    "                return spatial_features_df\n",
    "            else:\n",
    "                print(\"Cache size mismatch. Recalculating features.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache: {e}. Recalculating features.\")\n",
    "    \n",
    "    print(\"Creating enhanced spatial features...\")\n",
    "    # Make a copy to avoid modifying the original\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # Paris coordinates\n",
    "    city_center_lat, city_center_lon = 48.8566, 2.3522\n",
    "    \n",
    "    # Calculate distance to city center and directional positions\n",
    "    enhanced_df['distance_to_center'] = enhanced_df.apply(\n",
    "        lambda row: calculate_distance(\n",
    "            row['latitude'], \n",
    "            row['longitude'], \n",
    "            city_center_lat, \n",
    "            city_center_lon\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enhanced_df['north_south'] = enhanced_df['latitude'] - city_center_lat\n",
    "    enhanced_df['east_west'] = enhanced_df['longitude'] - city_center_lon\n",
    "    \n",
    "    # Create BallTree for nearest neighbor calculations\n",
    "    coords = np.radians(enhanced_df[['latitude', 'longitude']].values)\n",
    "    tree = BallTree(coords, metric='haversine')\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    n_chunks = math.ceil(len(enhanced_df) / chunk_size)\n",
    "    chunks = np.array_split(enhanced_df, n_chunks)\n",
    "    \n",
    "    # Initialize arrays for KNN features\n",
    "    knn_price_mean = np.zeros(len(enhanced_df))\n",
    "    knn_price_std = np.zeros(len(enhanced_df))\n",
    "    knn_price_median = np.zeros(len(enhanced_df))\n",
    "    knn_price_min = np.zeros(len(enhanced_df))\n",
    "    knn_price_max = np.zeros(len(enhanced_df))\n",
    "    price_diff = np.zeros(len(enhanced_df))\n",
    "    \n",
    "    # Process each chunk\n",
    "    start_idx = 0\n",
    "    for chunk in tqdm(chunks, desc=\"Processing spatial chunks\"):\n",
    "        chunk_size = len(chunk)\n",
    "        chunk_coords = np.radians(chunk[['latitude', 'longitude']].values)\n",
    "        \n",
    "        # Find k+1 nearest neighbors (including self)\n",
    "        # Use min to avoid requesting more neighbors than we have data points\n",
    "        k_to_use = min(k_neighbors + 1, len(coords))\n",
    "        distances, indices = tree.query(chunk_coords, k=k_to_use)\n",
    "        \n",
    "        # Calculate neighbor statistics\n",
    "        for i in range(chunk_size):\n",
    "            # Skip self (index 0)\n",
    "            neighbor_indices = indices[i, 1:]\n",
    "            prices = enhanced_df.iloc[neighbor_indices]['price'].values\n",
    "            \n",
    "            if len(prices) > 0:\n",
    "                # Calculate statistics\n",
    "                knn_price_mean[start_idx + i] = np.mean(prices)\n",
    "                knn_price_std[start_idx + i] = np.std(prices)\n",
    "                knn_price_median[start_idx + i] = np.median(prices)\n",
    "                knn_price_min[start_idx + i] = np.min(prices)\n",
    "                knn_price_max[start_idx + i] = np.max(prices)\n",
    "                \n",
    "                if pd.notnull(chunk.iloc[i]['price']):\n",
    "                    price_diff[start_idx + i] = chunk.iloc[i]['price'] - np.mean(prices)\n",
    "                else:\n",
    "                    price_diff[start_idx + i] = 0\n",
    "            else:\n",
    "                # If no neighbors found, use zeros\n",
    "                knn_price_mean[start_idx + i] = 0\n",
    "                knn_price_std[start_idx + i] = 0\n",
    "                knn_price_median[start_idx + i] = 0\n",
    "                knn_price_min[start_idx + i] = 0\n",
    "                knn_price_max[start_idx + i] = 0\n",
    "                price_diff[start_idx + i] = 0\n",
    "        \n",
    "        start_idx += chunk_size\n",
    "    \n",
    "    # Add features to dataframe\n",
    "    enhanced_df['knn_price_mean'] = knn_price_mean\n",
    "    enhanced_df['knn_price_std'] = knn_price_std\n",
    "    enhanced_df['knn_price_median'] = knn_price_median\n",
    "    enhanced_df['knn_price_min'] = knn_price_min\n",
    "    enhanced_df['knn_price_max'] = knn_price_max\n",
    "    enhanced_df['price_diff_from_neighbors'] = price_diff\n",
    "    \n",
    "    # Add neighborhood aggregated statistics\n",
    "    if 'neighbourhood_cleansed_encoded' in enhanced_df.columns:\n",
    "        # Group by neighborhood and calculate statistics\n",
    "        neighborhood_stats = enhanced_df.groupby('neighbourhood_cleansed_encoded').agg({\n",
    "            'price': ['mean', 'std', 'median', 'min', 'max', 'count']\n",
    "        })\n",
    "        \n",
    "        # Flatten column names\n",
    "        neighborhood_stats.columns = ['_'.join(col).strip() for col in neighborhood_stats.columns.values]\n",
    "        neighborhood_stats = neighborhood_stats.reset_index()\n",
    "        \n",
    "        # Merge statistics back to the dataframe\n",
    "        enhanced_df = enhanced_df.merge(neighborhood_stats, on='neighbourhood_cleansed_encoded', how='left')\n",
    "    \n",
    "    # Standardize the new features\n",
    "    spatial_features = [\n",
    "        'distance_to_center', 'north_south', 'east_west',\n",
    "        'knn_price_mean', 'knn_price_std', 'knn_price_median', \n",
    "        'knn_price_min', 'knn_price_max', 'price_diff_from_neighbors'\n",
    "    ]\n",
    "    \n",
    "    for col in spatial_features:\n",
    "        mean_val = enhanced_df[col].mean()\n",
    "        std_val = enhanced_df[col].std()\n",
    "        if std_val > 0:\n",
    "            enhanced_df[col] = (enhanced_df[col] - mean_val) / std_val\n",
    "    \n",
    "    # Save to cache if path provided\n",
    "    if cache_path:\n",
    "        print(f\"Saving spatial features to cache: {cache_path}\")\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(enhanced_df, f)\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "def extract_amenity_features(df):\n",
    "    \"\"\"Extract amenity features into a separate tensor\"\"\"\n",
    "    amenity_cols = [col for col in df.columns if col.startswith('has_')]\n",
    "    return df[amenity_cols].values\n",
    "\n",
    "def extract_location_features(df):\n",
    "    \"\"\"Extract latitude and longitude into a separate tensor\"\"\"\n",
    "    return df[['latitude', 'longitude']].values\n",
    "\n",
    "def extract_temporal_features(df):\n",
    "    \"\"\"Extract temporal features into a separate tensor\"\"\"\n",
    "    temporal_cols = ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
    "    if all(col in df.columns for col in temporal_cols):\n",
    "        return df[temporal_cols].values\n",
    "    else:\n",
    "        # Create dummy temporal features if not available\n",
    "        print(\"Warning: Temporal features not found. Using dummy values.\")\n",
    "        return np.zeros((len(df), 5))\n",
    "\n",
    "def extract_neighborhood_stats(df):\n",
    "    \"\"\"Extract neighborhood statistics into a separate tensor\"\"\"\n",
    "    neighborhood_cols = [\n",
    "        'knn_price_mean', 'knn_price_std', 'knn_price_median', \n",
    "        'knn_price_min', 'knn_price_max'\n",
    "    ]\n",
    "    \n",
    "    if all(col in df.columns for col in neighborhood_cols):\n",
    "        return df[neighborhood_cols].values\n",
    "    else:\n",
    "        # Create dummy neighborhood stats if not available\n",
    "        print(\"Warning: Neighborhood statistics not found. Using dummy values.\")\n",
    "        return np.zeros((len(df), 5))\n",
    "\n",
    "\n",
    "def extract_property_features(df, amenity_cols=None, temporal_cols=None, neighborhood_cols=None, spatial_cols=None):\n",
    "    \"\"\"Extract core property features, excluding other feature categories\"\"\"\n",
    "    if amenity_cols is None:\n",
    "        amenity_cols = [col for col in df.columns if col.startswith('has_')]\n",
    "    \n",
    "    if temporal_cols is None:\n",
    "        temporal_cols = ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
    "    \n",
    "    if neighborhood_cols is None:\n",
    "        neighborhood_cols = [\n",
    "            'knn_price_mean', 'knn_price_std', 'knn_price_median', \n",
    "            'knn_price_min', 'knn_price_max'\n",
    "        ]\n",
    "    \n",
    "    if spatial_cols is None:\n",
    "        spatial_cols = ['latitude', 'longitude', 'distance_to_center', 'north_south', 'east_west', 'price_diff_from_neighbors']\n",
    "    \n",
    "    # Get all columns that aren't in the excluded categories\n",
    "    exclude_cols = amenity_cols + temporal_cols + neighborhood_cols + spatial_cols + ['listing_id', 'date', 'price']\n",
    "    property_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    return df[property_cols].values\n",
    "\n",
    "\n",
    "def create_batch_aware_edge_index(batch_data, k_neighbors=5, distance_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Create edge index for the current batch, ensuring all indices are valid\n",
    "    \n",
    "    Args:\n",
    "        batch_data: DataFrame containing the current batch\n",
    "        k_neighbors: Number of neighbors to connect each node to\n",
    "        distance_threshold: Maximum distance (in km) to consider for connections\n",
    "        \n",
    "    Returns:\n",
    "        PyTorch tensor with edge index\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_data)\n",
    "    \n",
    "    if batch_size <= 1:\n",
    "        # For single item or empty batch, return empty edge index\n",
    "        return torch.tensor([], dtype=torch.long).view(2, 0)\n",
    "    \n",
    "    # Extract coordinates\n",
    "    coords = batch_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Convert to radians for haversine distance\n",
    "    coords_rad = np.radians(coords)\n",
    "    \n",
    "    # Create BallTree for efficient nearest neighbor search\n",
    "    tree = BallTree(coords_rad, metric='haversine')\n",
    "    \n",
    "    # Limit k_neighbors to batch_size - 1 to avoid out of range indices\n",
    "    k_to_use = min(k_neighbors + 1, batch_size)\n",
    "    \n",
    "    # Find nearest neighbors for each point\n",
    "    distances, indices = tree.query(coords_rad, k=k_to_use)\n",
    "    \n",
    "    # Convert distances from radians to km\n",
    "    distances = distances * 6371.0  # Earth radius in km\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for i in range(batch_size):\n",
    "        # Skip the first neighbor (self)\n",
    "        for j in range(1, len(indices[i])):\n",
    "            neighbor_idx = indices[i, j]\n",
    "            distance = distances[i, j]\n",
    "            \n",
    "            # Only add edges within threshold distance\n",
    "            if distance <= distance_threshold:\n",
    "                edge_index.append([i, int(neighbor_idx)])\n",
    "                # Add reverse edge for undirected graph\n",
    "                edge_index.append([int(neighbor_idx), i])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    if edge_index:\n",
    "        edge_index = list(set(map(tuple, edge_index)))\n",
    "        edge_index = [list(edge) for edge in edge_index]\n",
    "        return torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    else:\n",
    "        # If no edges, return empty edge index\n",
    "        return torch.tensor([], dtype=torch.long).view(2, 0)\n",
    "\n",
    "class AirbnbNewListingDataset(Dataset):\n",
    "    \"\"\"Dataset class for new listing price prediction\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "        # Extract different feature sets\n",
    "        self.property_features = extract_property_features(data)\n",
    "        self.amenity_features = extract_amenity_features(data)\n",
    "        self.location_features = extract_location_features(data)\n",
    "        self.temporal_features = extract_temporal_features(data)\n",
    "        self.neighborhood_stats = extract_neighborhood_stats(data)\n",
    "        \n",
    "        # Target prices\n",
    "        self.prices = data['price'].values\n",
    "        \n",
    "        # Keep original dataframe for batch-aware edge index creation\n",
    "        self.original_df = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.property_features[idx]),\n",
    "            torch.FloatTensor(self.amenity_features[idx]),\n",
    "            torch.FloatTensor(self.location_features[idx]),\n",
    "            torch.FloatTensor(self.temporal_features[idx]),\n",
    "            torch.FloatTensor(self.neighborhood_stats[idx]),\n",
    "            torch.FloatTensor([self.prices[idx]])\n",
    "        )\n",
    "\n",
    "class BatchCollator:\n",
    "    \"\"\"Custom collator that also creates batch-aware edge index\"\"\"\n",
    "    def __init__(self, dataset, k_neighbors=5, distance_threshold=2.0):\n",
    "        self.dataset = dataset\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.distance_threshold = distance_threshold\n",
    "    \n",
    "    def __call__(self, batch_indices):\n",
    "        # Get batch data\n",
    "        batch_data = self.dataset.original_df.iloc[batch_indices].copy()\n",
    "        \n",
    "        # Create edge index for this specific batch\n",
    "        edge_index = create_batch_aware_edge_index(\n",
    "            batch_data, \n",
    "            k_neighbors=self.k_neighbors,\n",
    "            distance_threshold=self.distance_threshold\n",
    "        )\n",
    "        \n",
    "        # Get all items for the batch\n",
    "        property_features = torch.FloatTensor(self.dataset.property_features[batch_indices])\n",
    "        amenity_features = torch.FloatTensor(self.dataset.amenity_features[batch_indices])\n",
    "        location_features = torch.FloatTensor(self.dataset.location_features[batch_indices])\n",
    "        temporal_features = torch.FloatTensor(self.dataset.temporal_features[batch_indices])\n",
    "        neighborhood_stats = torch.FloatTensor(self.dataset.neighborhood_stats[batch_indices])\n",
    "        prices = torch.FloatTensor(self.dataset.prices[batch_indices])\n",
    "        \n",
    "        return property_features, amenity_features, location_features, temporal_features, neighborhood_stats, prices, edge_index\n",
    "\n",
    "def train_new_listing_model(train_data, validation_data=None, epochs=10, batch_size=64, learning_rate=0.001, k_neighbors=5):\n",
    "    \"\"\"Train the new listing price prediction model with fixed batch handling\"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = AirbnbNewListingDataset(train_data)\n",
    "    \n",
    "    # Create standard DataLoader (without custom collator for now)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if validation_data is not None:\n",
    "        val_dataset = AirbnbNewListingDataset(validation_data)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Get dimension of property features\n",
    "    property_dim = train_dataset.property_features.shape[1]\n",
    "    amenities_dim = train_dataset.amenity_features.shape[1]\n",
    "    \n",
    "    print(f\"Property features dimension: {property_dim}\")\n",
    "    print(f\"Amenities features dimension: {amenities_dim}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cpu')  # Use CPU for initial debugging\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = NewListingPricePredictor(\n",
    "        input_dim=property_dim,\n",
    "        amenities_dim=amenities_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # NOTE: We're now only expecting 6 values from the DataLoader\n",
    "        for property_feat, amenity_feat, location_feat, temporal_feat, neighborhood_feat, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            # Move data to device\n",
    "            property_feat = property_feat.to(device)\n",
    "            amenity_feat = amenity_feat.to(device)\n",
    "            location_feat = location_feat.to(device)\n",
    "            temporal_feat = temporal_feat.to(device)\n",
    "            neighborhood_feat = neighborhood_feat.to(device)\n",
    "            target = target.to(device).squeeze()\n",
    "            \n",
    "            # Forward pass without edge_index \n",
    "            output = model(\n",
    "                property_feat, \n",
    "                amenity_feat, \n",
    "                location_feat, \n",
    "                temporal_feat, \n",
    "                neighborhood_feat\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_train_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        training_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if validation_data is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for property_feat, amenity_feat, location_feat, temporal_feat, neighborhood_feat, target, edge_index in val_loader:\n",
    "                    # Move data to device\n",
    "                    property_feat = property_feat.to(device)\n",
    "                    amenity_feat = amenity_feat.to(device)\n",
    "                    location_feat = location_feat.to(device)\n",
    "                    temporal_feat = temporal_feat.to(device)\n",
    "                    neighborhood_feat = neighborhood_feat.to(device)\n",
    "                    target = target.to(device).squeeze()\n",
    "                    edge_index = edge_index.to(device) if edge_index.numel() > 0 else None\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output = model(\n",
    "                        property_feat, \n",
    "                        amenity_feat, \n",
    "                        location_feat, \n",
    "                        temporal_feat, \n",
    "                        neighborhood_feat, \n",
    "                        edge_index\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batch_count += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}\")\n",
    "    \n",
    "    # Load best model if validation was used\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, {\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_new_listings(model, test_data, batch_size=64, k_neighbors=5):\n",
    "    \"\"\"Make predictions for new listings using batch-aware edge indices\"\"\"\n",
    "    # Create dataset and batch collator\n",
    "    test_dataset = AirbnbNewListingDataset(test_data)\n",
    "    test_collator = BatchCollator(test_dataset, k_neighbors=k_neighbors)\n",
    "    \n",
    "    # Create data loader\n",
    "    test_loader = DataLoader(\n",
    "        dataset=range(len(test_dataset)),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=test_collator\n",
    "    )\n",
    "    \n",
    "    # Move model to evaluation mode\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Make predictions\n",
    "    all_indices = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (property_feat, amenity_feat, location_feat, temporal_feat, neighborhood_feat, target, edge_index) in enumerate(test_loader):\n",
    "            # Track which indices we're processing\n",
    "            batch_indices = list(range(i * batch_size, min((i + 1) * batch_size, len(test_dataset))))\n",
    "            all_indices.extend(batch_indices)\n",
    "            \n",
    "            # Move data to device\n",
    "            property_feat = property_feat.to(device)\n",
    "            amenity_feat = amenity_feat.to(device)\n",
    "            location_feat = location_feat.to(device)\n",
    "            temporal_feat = temporal_feat.to(device)\n",
    "            neighborhood_feat = neighborhood_feat.to(device)\n",
    "            edge_index = edge_index.to(device) if edge_index.numel() > 0 else None\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(\n",
    "                property_feat, \n",
    "                amenity_feat, \n",
    "                location_feat, \n",
    "                temporal_feat, \n",
    "                neighborhood_feat,\n",
    "                edge_index\n",
    "            )\n",
    "            \n",
    "            # Store predictions\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "    \n",
    "    # Ensure predictions are in the correct order\n",
    "    predictions_with_indices = list(zip(all_indices, all_predictions))\n",
    "    predictions_with_indices.sort()  # Sort by index\n",
    "    sorted_predictions = [pred for _, pred in predictions_with_indices]\n",
    "    \n",
    "    # Get actual targets\n",
    "    all_targets = test_dataset.prices\n",
    "    \n",
    "    # Combine predictions and targets\n",
    "    predictions_df = test_data[['listing_id', 'date']].copy()\n",
    "    predictions_df['price'] = test_data['price']  # Actual price\n",
    "    predictions_df['predicted'] = sorted_predictions\n",
    "    predictions_df['error'] = predictions_df['price'] - predictions_df['predicted']\n",
    "    predictions_df['abs_error'] = np.abs(predictions_df['error'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets, sorted_predictions)),\n",
    "        'mae': mean_absolute_error(all_targets, sorted_predictions),\n",
    "        'r2': r2_score(all_targets, sorted_predictions)\n",
    "    }\n",
    "    \n",
    "    return predictions_df, metrics\n",
    "\n",
    "\n",
    "def run_new_listing_prediction(train_path, test_path, output_path=None, use_validation=True, val_split=0.2, k_neighbors=5):\n",
    "    \"\"\"Run the entire pipeline for new listing price prediction\"\"\"\n",
    "    print(\"Starting new listing price prediction pipeline...\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "    \n",
    "    # Create enhanced spatial features with caching\n",
    "    train_cache_path = get_cache_path(train_path, prefix=\"spatial_features_train\")\n",
    "    test_cache_path = get_cache_path(test_path, prefix=\"spatial_features_test\")\n",
    "    \n",
    "    train_data = create_enhanced_spatial_features(\n",
    "        train_data, \n",
    "        k_neighbors=k_neighbors,\n",
    "        cache_path=train_cache_path\n",
    "    )\n",
    "    \n",
    "    test_data = create_enhanced_spatial_features(\n",
    "        test_data, \n",
    "        k_neighbors=k_neighbors,\n",
    "        cache_path=test_cache_path\n",
    "    )\n",
    "    \n",
    "    # Split train data into train and validation if requested\n",
    "    if use_validation:\n",
    "        # Sort by date\n",
    "        train_data = train_data.sort_values('date')\n",
    "        \n",
    "        # Use the most recent data as validation\n",
    "        val_size = int(len(train_data) * val_split)\n",
    "        validation_data = train_data.tail(val_size)\n",
    "        train_data = train_data.head(len(train_data) - val_size)\n",
    "        print(f\"Training data size: {len(train_data)}\")\n",
    "        print(f\"Validation data size: {len(validation_data)}\")\n",
    "    else:\n",
    "        validation_data = None\n",
    "    \n",
    "    # Train model\n",
    "    model, training_history = train_new_listing_model(\n",
    "        train_data=train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        learning_rate=0.001,\n",
    "        k_neighbors=k_neighbors\n",
    "    )\n",
    "    \n",
    "    # Save the trained model\n",
    "    if output_path:\n",
    "        model_dir = os.path.dirname(output_path)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        model_path = os.path.join(model_dir, \"new_listing_model.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_df, metrics = predict_new_listings(\n",
    "        model=model, \n",
    "        test_data=test_data,\n",
    "        batch_size=64,\n",
    "        k_neighbors=k_neighbors\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n=== New Listing Price Prediction Results ===\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Calculate MAPE (avoiding division by zero)\n",
    "    mape = np.mean(np.abs(predictions_df['error'] / (predictions_df['price'] + 1e-8))) * 100\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    # Save results if output path is provided\n",
    "    if output_path:\n",
    "        predictions_df.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "        \n",
    "        # Save metrics to a separate file\n",
    "        metrics_path = output_path.replace('.csv', '_metrics.csv')\n",
    "        pd.DataFrame([{\n",
    "            'rmse': metrics['rmse'],\n",
    "            'mae': metrics['mae'],\n",
    "            'r2': metrics['r2'],\n",
    "            'mape': mape\n",
    "        }]).to_csv(metrics_path, index=False)\n",
    "        print(f\"Metrics saved to {metrics_path}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    if training_history['training_losses']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(training_history['training_losses'], label='Training Loss')\n",
    "        \n",
    "        if training_history['validation_losses']:\n",
    "            plt.plot(training_history['validation_losses'], label='Validation Loss')\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        if output_path:\n",
    "            history_plot_path = output_path.replace('.csv', '_training_history.png')\n",
    "            plt.savefig(history_plot_path)\n",
    "            print(f\"Training history plot saved to {history_plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Create visualization of actual vs predicted prices\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Sample a subset if the dataset is large\n",
    "    if len(predictions_df) > 1000:\n",
    "        plot_data = predictions_df.sample(1000)\n",
    "    else:\n",
    "        plot_data = predictions_df\n",
    "    \n",
    "    plt.scatter(plot_data['price'], plot_data['predicted'], alpha=0.5)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(plot_data['price'].min(), plot_data['predicted'].min())\n",
    "    max_val = max(plot_data['price'].max(), plot_data['predicted'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title('New Listing Price Prediction')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if output_path:\n",
    "        scatter_plot_path = output_path.replace('.csv', '_scatter_plot.png')\n",
    "        plt.savefig(scatter_plot_path)\n",
    "        print(f\"Scatter plot saved to {scatter_plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Create heatmap of spatial errors\n",
    "    if 'latitude' in test_data.columns and 'longitude' in test_data.columns:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Take a sample for better visualization\n",
    "        if len(predictions_df) > 2000:\n",
    "            plot_data = predictions_df.sample(2000)\n",
    "        else:\n",
    "            plot_data = predictions_df\n",
    "        \n",
    "        # Create a scatter plot with error as color\n",
    "        scatter = plt.scatter(\n",
    "            plot_data['longitude'], \n",
    "            plot_data['latitude'],\n",
    "            c=np.abs(plot_data['error']),\n",
    "            cmap='viridis',\n",
    "            alpha=0.7,\n",
    "            s=30\n",
    "        )\n",
    "        \n",
    "        plt.colorbar(scatter, label='Absolute Error')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.title('Spatial Distribution of Prediction Errors')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        if output_path:\n",
    "            heatmap_path = output_path.replace('.csv', '_error_heatmap.png')\n",
    "            plt.savefig(heatmap_path)\n",
    "            print(f\"Error heatmap saved to {heatmap_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Create feature importance plot using correlation analysis\n",
    "    feature_importance = analyze_feature_importance(predictions_df, test_data)\n",
    "    \n",
    "    if feature_importance is not None and len(feature_importance) > 0:\n",
    "        # Plot top 20 features or fewer if not available\n",
    "        n_features = min(20, len(feature_importance))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        sns.barplot(\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            data=feature_importance.head(n_features),\n",
    "            palette='viridis'\n",
    "        )\n",
    "        \n",
    "        plt.title('Feature Importance (Correlation with Target)')\n",
    "        plt.xlabel('Absolute Correlation with Price')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_path:\n",
    "            feature_plot_path = output_path.replace('.csv', '_feature_importance.png')\n",
    "            plt.savefig(feature_plot_path)\n",
    "            print(f\"Feature importance plot saved to {feature_plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return model, predictions_df, metrics\n",
    "\n",
    "\n",
    "def analyze_feature_importance(predictions_df, test_data):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using correlation with target value\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with predictions\n",
    "        test_data: Original test data with features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importance\n",
    "    \"\"\"\n",
    "    # Get listing IDs from predictions\n",
    "    listing_ids = predictions_df['listing_id'].values\n",
    "    \n",
    "    # Filter test data to match predictions\n",
    "    matched_test_data = test_data[test_data['listing_id'].isin(listing_ids)].copy()\n",
    "    \n",
    "    # Get features (excluding non-feature columns)\n",
    "    non_feature_cols = ['listing_id', 'date', 'price']\n",
    "    feature_cols = [col for col in matched_test_data.columns if col not in non_feature_cols]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"No features found for importance analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate correlation with price\n",
    "    correlations = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        try:\n",
    "            corr = np.abs(np.corrcoef(matched_test_data[col].values, matched_test_data['price'].values)[0, 1])\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append({\n",
    "                    'feature': col,\n",
    "                    'importance': corr\n",
    "                })\n",
    "        except:\n",
    "            # Skip features that cause errors\n",
    "            pass\n",
    "    \n",
    "    if not correlations:\n",
    "        print(\"No valid correlations found\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame and sort by importance\n",
    "    importance_df = pd.DataFrame(correlations)\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def compare_model_performance(model_results):\n",
    "    \"\"\"\n",
    "    Compare performance across multiple models\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary mapping model names to their results dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with comparison metrics\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    for model_name, results in model_results.items():\n",
    "        comparison.append({\n",
    "            'Model': model_name,\n",
    "            'RMSE': results['metrics']['rmse'],\n",
    "            'MAE': results['metrics']['mae'],\n",
    "            'R²': results['metrics']['r2'],\n",
    "            'MAPE (%)': results.get('mape', 0)\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a grouped bar chart\n",
    "    comparison_df_melted = pd.melt(\n",
    "        comparison_df, \n",
    "        id_vars=['Model'],\n",
    "        value_vars=['RMSE', 'MAE', 'MAPE (%)'],\n",
    "        var_name='Metric',\n",
    "        value_name='Value'\n",
    "    )\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=comparison_df_melted,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        hue='Metric'\n",
    "    )\n",
    "    \n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Error Value (lower is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create R² comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=comparison_df,\n",
    "        x='Model',\n",
    "        y='R²',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    \n",
    "    plt.title('Model R² Comparison (higher is better)')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train.csv\"\n",
    "    test_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_feb.csv\"\n",
    "    output_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Output\\NN\\new_listing_predictions.csv\"\n",
    "    \n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    model, predictions, metrics = run_new_listing_prediction(\n",
    "        train_path=train_path,\n",
    "        test_path=test_path,\n",
    "        output_path=output_path,\n",
    "        k_neighbors=5  # Reduce neighbor count to avoid issues with small datasets\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
