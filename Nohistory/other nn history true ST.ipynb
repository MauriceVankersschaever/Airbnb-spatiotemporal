{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Price transformation function\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        The dataframe containing price data\n",
    "    inverse : bool\n",
    "        If True, apply inverse transformation; otherwise apply log transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with transformed prices\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Create calculated features\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataframe to add features to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with new features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 3. Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RÂ²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 4. Create simple price histories\n",
    "def create_simple_price_histories(data, sequence_length=30):\n",
    "    \"\"\"Create raw price histories without complex feature engineering\"\"\"\n",
    "    histories = {}\n",
    "    \n",
    "    for listing_id in data['listing_id'].unique():\n",
    "        listing_data = data[data['listing_id'] == listing_id].sort_values('date')\n",
    "        \n",
    "        if len(listing_data) >= sequence_length:\n",
    "            # Just use the actual prices - minimal preprocessing\n",
    "            prices = listing_data['price'].values[-sequence_length:]\n",
    "            \n",
    "            # Optionally add a few key features like day of week, is_weekend\n",
    "            temporal_info = listing_data[['DTF_is_weekend', 'DTF_month']].values[-sequence_length:]\n",
    "            \n",
    "            # Combine price with minimal temporal features\n",
    "            history = np.column_stack([prices, temporal_info])\n",
    "            histories[listing_id] = torch.FloatTensor(history)\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# 5. Generate price history features\n",
    "def generate_price_history_features(train_data, test_data=None, use_train_for_test=True, window_sizes=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Generate price history features for training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data with 'listing_id', 'date', and 'price' columns\n",
    "    test_data : DataFrame, optional\n",
    "        Test data with 'listing_id', 'date', and 'price' columns\n",
    "    use_train_for_test : bool, optional\n",
    "        If True, use training data to compute price history features for test data\n",
    "    window_sizes : list, optional\n",
    "        List of window sizes (in days) for rolling statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_data_with_features, test_data_with_features) DataFrames with added price history features\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy() if test_data is not None else None\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(train_df['date']):\n",
    "        train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    \n",
    "    if test_df is not None and not pd.api.types.is_datetime64_any_dtype(test_df['date']):\n",
    "        test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    \n",
    "    # Sort by listing_id and date\n",
    "    train_df = train_df.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    # Prepare data for feature generation\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Create a combined dataframe for computing features\n",
    "        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "        combined_df = combined_df.sort_values(['listing_id', 'date'])\n",
    "        data_for_features = combined_df\n",
    "    else:\n",
    "        data_for_features = train_df\n",
    "    \n",
    "    # Initialize list of created features\n",
    "    created_features = []\n",
    "    \n",
    "    # Generate lag features\n",
    "    for window in window_sizes:\n",
    "        feature_name = f'price_lag_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].shift(window)\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Generate rolling statistics\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        feature_name = f'rolling_mean_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling max\n",
    "        feature_name = f'rolling_max_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).max()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling min\n",
    "        feature_name = f'rolling_min_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).min()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling std\n",
    "        feature_name = f'rolling_std_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Price volatility (max - min)\n",
    "        feature_name = f'price_range_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features[f'rolling_max_{window}d'] - data_for_features[f'rolling_min_{window}d']\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for feature in created_features:\n",
    "        if data_for_features[feature].isnull().any():\n",
    "            # Fill within each listing_id group\n",
    "            data_for_features[feature] = data_for_features.groupby('listing_id')[feature].transform(\n",
    "                lambda x: x.fillna(x.median() if x.notna().any() else 0)\n",
    "            )\n",
    "    \n",
    "    # Split combined data back to train and test if necessary\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Get indices of train and test rows\n",
    "        train_indices = data_for_features.index[:len(train_df)]\n",
    "        test_indices = data_for_features.index[len(train_df):]\n",
    "        \n",
    "        # Extract features for train and test\n",
    "        train_df = data_for_features.loc[train_indices].copy()\n",
    "        test_df = data_for_features.loc[test_indices].copy()\n",
    "        \n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        if test_df is not None:\n",
    "            # Generate features for test separately\n",
    "            test_df = test_df.sort_values(['listing_id', 'date'])\n",
    "            for feature in created_features:\n",
    "                if feature in data_for_features.columns:\n",
    "                    # Just create empty columns that will be populated later\n",
    "                    test_df[feature] = np.nan\n",
    "            \n",
    "            return data_for_features, test_df\n",
    "        else:\n",
    "            return data_for_features, None\n",
    "\n",
    "# 6. Extract basic features\n",
    "def extract_basic_features(data, feature_groups):\n",
    "    \"\"\"Extract basic features from the data\"\"\"\n",
    "    features = []\n",
    "    for group in ['spatial', 'property']:\n",
    "        if group in feature_groups:\n",
    "            features.extend(feature_groups[group])\n",
    "    \n",
    "    # Get only columns that exist in the data\n",
    "    valid_features = [f for f in features if f in data.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        # Return dummy features if no valid features found\n",
    "        return np.zeros((len(data), 1))\n",
    "    \n",
    "    # Extract and handle NaN values\n",
    "    feature_matrix = data[valid_features].values\n",
    "    feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "# 7. Extract temporal features\n",
    "def extract_temporal_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract temporal features\"\"\"\n",
    "    if 'temporal' not in feature_groups or not feature_groups['temporal']:\n",
    "        return torch.zeros(len(train_data) + len(test_data), 1).to(device)\n",
    "    \n",
    "    features = feature_groups['temporal']\n",
    "    valid_features = [f for f in features if f in train_data.columns and f in test_data.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        return torch.zeros(len(train_data) + len(test_data), 1).to(device)\n",
    "    \n",
    "    train_features = train_data[valid_features].values\n",
    "    test_features = test_data[valid_features].values\n",
    "    \n",
    "    combined = np.vstack([train_features, test_features])\n",
    "    combined = np.nan_to_num(combined, nan=0.0)\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# 8. Extract amenity features\n",
    "def extract_amenity_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract amenity features\"\"\"\n",
    "    if 'amenity' not in feature_groups or not feature_groups['amenity']:\n",
    "        return torch.zeros(len(train_data) + len(test_data), 1).to(device)\n",
    "    \n",
    "    features = feature_groups['amenity']\n",
    "    valid_features = [f for f in features if f in train_data.columns and f in test_data.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        return torch.zeros(len(train_data) + len(test_data), 1).to(device)\n",
    "    \n",
    "    train_features = train_data[valid_features].values\n",
    "    test_features = test_data[valid_features].values\n",
    "    \n",
    "    combined = np.vstack([train_features, test_features])\n",
    "    combined = np.nan_to_num(combined, nan=0.0)\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# 9. Extract price history features\n",
    "def extract_price_history_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract price history features\"\"\"\n",
    "    price_history_cols = [col for col in train_data.columns if col.startswith(('price_lag', 'rolling'))]\n",
    "    \n",
    "    if not price_history_cols:\n",
    "        return torch.zeros(len(train_data) + len(test_data), 1).to(device)\n",
    "    \n",
    "    train_features = train_data[price_history_cols].values\n",
    "    test_features = test_data[price_history_cols].values\n",
    "    \n",
    "    combined = np.vstack([train_features, test_features])\n",
    "    combined = np.nan_to_num(combined, nan=0.0)\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# 10. Process listing history\n",
    "def process_listing_history(listing_data, sequence_length=30):\n",
    "    \"\"\"Process the history for a single listing\"\"\"\n",
    "    if len(listing_data) < 2:\n",
    "        return torch.zeros((1, 1))\n",
    "    \n",
    "    # Sort by date\n",
    "    listing_data = listing_data.sort_values('date')\n",
    "    \n",
    "    # Extract price\n",
    "    prices = listing_data['price'].values[-sequence_length:]\n",
    "    \n",
    "    # Extract temporal features if available\n",
    "    temporal_features = []\n",
    "    if 'DTF_is_weekend' in listing_data.columns and 'DTF_month' in listing_data.columns:\n",
    "        temporal_info = listing_data[['DTF_is_weekend', 'DTF_month']].values[-sequence_length:]\n",
    "        temporal_features.append(temporal_info)\n",
    "    \n",
    "    # Combine features\n",
    "    history_features = [prices.reshape(-1, 1)]\n",
    "    history_features.extend(temporal_features)\n",
    "    \n",
    "    history = np.concatenate(history_features, axis=1) if temporal_features else prices.reshape(-1, 1)\n",
    "    \n",
    "    return torch.FloatTensor(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "Using simplified GNN with transformer-based temporal processing\n",
      "Sequence length: 10, Batch size: 16384, Hidden dim: 32\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 903142 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training Simplified GNN Model (Split 1) -----\n",
      "Preparing graph data...\n",
      "Creating price histories...\n",
      "Building spatial graph...\n",
      "Building enhanced spatial graph with 180551 test listings and 3 nearest neighbors...\n",
      "Created graph with 1083306 edges\n",
      "Input dimensions - Spatial: 8, Price history: 3\n",
      "Training with 45 mini-batches per epoch, batch size: 16384\n",
      "Epoch 1/30 - Loss: 0.1724, Val Loss: 0.0766, RMSE: 111.23, MAE: 49.32\n",
      "Epoch 2/30 - Loss: 0.0675, Val Loss: 0.0643, RMSE: 105.13, MAE: 45.43\n",
      "Epoch 3/30 - Loss: 0.0621, Val Loss: 0.0613, RMSE: 100.22, MAE: 43.36\n",
      "Epoch 4/30 - Loss: 0.0602, Val Loss: 0.0602, RMSE: 98.53, MAE: 42.98\n",
      "Epoch 5/30 - Loss: 0.0590, Val Loss: 0.0589, RMSE: 97.97, MAE: 42.33\n",
      "Epoch 6/30 - Loss: 0.0572, Val Loss: 0.0572, RMSE: 95.17, MAE: 41.76\n",
      "Epoch 7/30 - Loss: 0.0550, Val Loss: 0.0548, RMSE: 94.99, MAE: 40.58\n",
      "Epoch 8/30 - Loss: 0.0523, Val Loss: 0.0545, RMSE: 94.76, MAE: 41.01\n",
      "Epoch 9/30 - Loss: 0.0578, Val Loss: 0.0518, RMSE: 89.74, MAE: 39.77\n",
      "Epoch 10/30 - Loss: 0.0495, Val Loss: 0.0499, RMSE: 90.90, MAE: 39.05\n",
      "Epoch 11/30 - Loss: 0.0483, Val Loss: 0.0486, RMSE: 89.83, MAE: 38.71\n",
      "Epoch 12/30 - Loss: 0.0474, Val Loss: 0.0477, RMSE: 88.09, MAE: 38.17\n",
      "Epoch 13/30 - Loss: 0.0469, Val Loss: 0.0477, RMSE: 88.66, MAE: 38.30\n",
      "Epoch 14/30 - Loss: 0.0466, Val Loss: 0.0473, RMSE: 87.71, MAE: 38.03\n",
      "Epoch 15/30 - Loss: 0.0464, Val Loss: 0.0478, RMSE: 88.70, MAE: 38.33\n",
      "Epoch 16/30 - Loss: 0.0463, Val Loss: 0.0485, RMSE: 89.53, MAE: 38.80\n",
      "Epoch 17/30 - Loss: 0.0462, Val Loss: 0.0483, RMSE: 89.39, MAE: 38.66\n",
      "Epoch 18/30 - Loss: 0.0459, Val Loss: 0.0477, RMSE: 89.16, MAE: 38.43\n",
      "Epoch 19/30 - Loss: 0.0457, Val Loss: 0.0470, RMSE: 87.83, MAE: 37.88\n",
      "Epoch 20/30 - Loss: 0.0455, Val Loss: 0.0469, RMSE: 87.91, MAE: 37.83\n",
      "Epoch 21/30 - Loss: 0.0454, Val Loss: 0.0471, RMSE: 88.20, MAE: 37.98\n",
      "Epoch 22/30 - Loss: 0.0454, Val Loss: 0.0468, RMSE: 87.85, MAE: 37.78\n",
      "Epoch 23/30 - Loss: 0.0451, Val Loss: 0.0477, RMSE: 89.04, MAE: 38.37\n",
      "Epoch 24/30 - Loss: 0.0454, Val Loss: 0.0468, RMSE: 87.78, MAE: 37.79\n",
      "Epoch 25/30 - Loss: 0.0449, Val Loss: 0.0465, RMSE: 87.30, MAE: 37.60\n",
      "Epoch 26/30 - Loss: 0.0447, Val Loss: 0.0462, RMSE: 87.16, MAE: 37.49\n",
      "Epoch 27/30 - Loss: 0.0447, Val Loss: 0.0462, RMSE: 87.56, MAE: 37.54\n",
      "Epoch 28/30 - Loss: 0.0446, Val Loss: 0.0461, RMSE: 87.16, MAE: 37.45\n",
      "Epoch 29/30 - Loss: 0.0443, Val Loss: 0.0467, RMSE: 88.08, MAE: 37.90\n",
      "Epoch 30/30 - Loss: 0.0443, Val Loss: 0.0459, RMSE: 86.97, MAE: 37.41\n",
      "\n",
      "----- Evaluating Simplified GNN on Test Data (Split 1) -----\n",
      "Preparing graph data...\n",
      "Creating price histories...\n",
      "Building spatial graph...\n",
      "Building enhanced spatial graph with 11011 test listings and 3 nearest neighbors...\n",
      "Created graph with 66066 edges\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 159.62\n",
      "MAE: 100.86\n",
      "RÂ²: 0.0193\n",
      "MAPE: 64.39%\n",
      "Split 1 Results - RMSE: 159.6236, MAE: 100.8558, RÂ²: 0.0193\n",
      "Model for split 1 saved to ./output/memory_efficient_gnn\\simplified_gnn_model_split_1.pt\n",
      "\n",
      "===== Split 2/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 947179 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training Simplified GNN Model (Split 2) -----\n",
      "Preparing graph data...\n",
      "Creating price histories...\n",
      "Building spatial graph...\n",
      "Building enhanced spatial graph with 189364 test listings and 3 nearest neighbors...\n",
      "Created graph with 1136184 edges\n",
      "Input dimensions - Spatial: 8, Price history: 3\n",
      "Training with 47 mini-batches per epoch, batch size: 16384\n",
      "Epoch 1/30 - Loss: 0.1625, Val Loss: 0.0870, RMSE: 124.53, MAE: 54.03\n",
      "Epoch 2/30 - Loss: 0.0710, Val Loss: 0.0660, RMSE: 105.33, MAE: 46.05\n",
      "Epoch 3/30 - Loss: 0.0654, Val Loss: 0.0642, RMSE: 102.06, MAE: 44.94\n",
      "Epoch 4/30 - Loss: 0.0634, Val Loss: 0.0628, RMSE: 101.71, MAE: 43.93\n",
      "Epoch 5/30 - Loss: 0.0617, Val Loss: 0.0614, RMSE: 101.22, MAE: 43.40\n",
      "Epoch 6/30 - Loss: 0.0605, Val Loss: 0.0602, RMSE: 99.52, MAE: 43.06\n",
      "Epoch 7/30 - Loss: 0.0601, Val Loss: 0.0598, RMSE: 98.66, MAE: 42.98\n",
      "Epoch 8/30 - Loss: 0.0593, Val Loss: 0.0604, RMSE: 101.06, MAE: 43.07\n",
      "Epoch 9/30 - Loss: 0.0590, Val Loss: 0.0593, RMSE: 99.11, MAE: 42.51\n",
      "Epoch 10/30 - Loss: 0.0584, Val Loss: 0.0587, RMSE: 98.42, MAE: 42.39\n",
      "Epoch 11/30 - Loss: 0.0581, Val Loss: 0.0588, RMSE: 99.11, MAE: 42.48\n",
      "Epoch 12/30 - Loss: 0.0576, Val Loss: 0.0583, RMSE: 98.31, MAE: 42.32\n",
      "Epoch 13/30 - Loss: 0.0577, Val Loss: 0.0599, RMSE: 100.45, MAE: 42.99\n",
      "Epoch 14/30 - Loss: 0.0573, Val Loss: 0.0581, RMSE: 98.42, MAE: 42.20\n",
      "Epoch 15/30 - Loss: 0.0567, Val Loss: 0.0574, RMSE: 97.57, MAE: 42.00\n",
      "Epoch 16/30 - Loss: 0.0562, Val Loss: 0.0567, RMSE: 96.08, MAE: 41.68\n",
      "Epoch 17/30 - Loss: 0.0559, Val Loss: 0.0566, RMSE: 95.94, MAE: 41.54\n",
      "Epoch 18/30 - Loss: 0.0554, Val Loss: 0.0572, RMSE: 96.21, MAE: 41.54\n",
      "Epoch 19/30 - Loss: 0.0547, Val Loss: 0.0552, RMSE: 93.76, MAE: 40.66\n",
      "Epoch 20/30 - Loss: 0.0540, Val Loss: 0.0547, RMSE: 93.02, MAE: 40.48\n",
      "Epoch 21/30 - Loss: 0.0543, Val Loss: 0.0556, RMSE: 95.37, MAE: 40.63\n",
      "Epoch 22/30 - Loss: 0.0531, Val Loss: 0.0548, RMSE: 93.62, MAE: 40.12\n",
      "Epoch 23/30 - Loss: 0.0530, Val Loss: 0.0544, RMSE: 92.40, MAE: 40.28\n",
      "Epoch 24/30 - Loss: 0.0530, Val Loss: 0.0544, RMSE: 93.34, MAE: 40.15\n",
      "Epoch 25/30 - Loss: 0.0526, Val Loss: 0.0544, RMSE: 93.39, MAE: 40.08\n",
      "Epoch 26/30 - Loss: 0.0530, Val Loss: 0.0542, RMSE: 92.59, MAE: 40.04\n",
      "Epoch 27/30 - Loss: 0.0526, Val Loss: 0.0550, RMSE: 93.92, MAE: 40.20\n",
      "Epoch 28/30 - Loss: 0.0531, Val Loss: 0.0542, RMSE: 91.89, MAE: 40.20\n",
      "Epoch 29/30 - Loss: 0.0525, Val Loss: 0.0542, RMSE: 92.08, MAE: 40.14\n",
      "Epoch 30/30 - Loss: 0.0522, Val Loss: 0.0543, RMSE: 92.40, MAE: 39.96\n",
      "\n",
      "----- Evaluating Simplified GNN on Test Data (Split 2) -----\n",
      "Preparing graph data...\n",
      "Creating price histories...\n",
      "Building spatial graph...\n",
      "Building enhanced spatial graph with 11011 test listings and 3 nearest neighbors...\n",
      "Created graph with 66066 edges\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 155.00\n",
      "MAE: 104.89\n",
      "RÂ²: 0.0303\n",
      "MAPE: 74.84%\n",
      "Split 2 Results - RMSE: 154.9990, MAE: 104.8877, RÂ²: 0.0303\n",
      "Model for split 2 saved to ./output/memory_efficient_gnn\\simplified_gnn_model_split_2.pt\n",
      "\n",
      "===== Split 3/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-18\n",
      "Testing period: 2024-01-19 to 2024-01-25\n",
      "Train data: 991216 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training Simplified GNN Model (Split 3) -----\n",
      "Preparing graph data...\n",
      "Creating price histories...\n",
      "Building spatial graph...\n",
      "Building enhanced spatial graph with 198177 test listings and 3 nearest neighbors...\n",
      "Created graph with 1189062 edges\n",
      "Input dimensions - Spatial: 8, Price history: 3\n",
      "Training with 49 mini-batches per epoch, batch size: 16384\n",
      "Epoch 1/30 - Loss: 0.1463, Val Loss: 0.0733, RMSE: 112.23, MAE: 48.27\n",
      "Epoch 2/30 - Loss: 0.0668, Val Loss: 0.0661, RMSE: 100.84, MAE: 44.81\n",
      "Epoch 3/30 - Loss: 0.0640, Val Loss: 0.0656, RMSE: 101.95, MAE: 44.26\n",
      "Epoch 4/30 - Loss: 0.0627, Val Loss: 0.0657, RMSE: 102.46, MAE: 44.08\n",
      "Epoch 5/30 - Loss: 0.0620, Val Loss: 0.0659, RMSE: 102.20, MAE: 43.98\n",
      "Epoch 6/30 - Loss: 0.0613, Val Loss: 0.0659, RMSE: 102.45, MAE: 44.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1115\u001b[0m\n\u001b[0;32m   1111\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# Run with memory-efficient approach\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simplified_gnn_with_rolling_window_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use full dataset\u001b[39;49;00m\n\u001b[0;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced from 30\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Added batch size\u001b[39;49;00m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Reduced from 64\u001b[39;49;00m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory-efficient transformer-based GNN model training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[4], line 963\u001b[0m, in \u001b[0;36mrun_simplified_gnn_with_rolling_window_cv\u001b[1;34m(train_path, train_ids_path, test_ids_path, output_dir, window_size, n_splits, sample_size, sequence_length, batch_size, hidden_dim)\u001b[0m\n\u001b[0;32m    960\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m    962\u001b[0m \u001b[38;5;66;03m# Train the model with memory-efficient approach\u001b[39;00m\n\u001b[1;32m--> 963\u001b[0m model, scalers \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_simplified_gnn_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced from 50 \u001b[39;49;00m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----- Evaluating Simplified GNN on Test Data (Split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 604\u001b[0m, in \u001b[0;36mtrain_simplified_gnn_model\u001b[1;34m(train_data, val_data, feature_groups, device, hidden_dim, epochs, lr, sequence_length, batch_size, grad_accum_steps)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# Forward pass with mixed precision\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m# Model will handle moving tensors to the correct device\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Move y to the same device as the output\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     y \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(out\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 262\u001b[0m, in \u001b[0;36mSimplifiedListingGNN.forward\u001b[1;34m(self, data, batch_indices)\u001b[0m\n\u001b[0;32m    259\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(model_device)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Get filtered edges\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m batch_edge_index, batch_edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Move edges to the model's device\u001b[39;00m\n\u001b[0;32m    265\u001b[0m batch_edge_index \u001b[38;5;241m=\u001b[39m batch_edge_index\u001b[38;5;241m.\u001b[39mto(model_device)\n",
      "Cell \u001b[1;32mIn[4], line 346\u001b[0m, in \u001b[0;36mSimplifiedListingGNN.filter_edges\u001b[1;34m(self, edge_index, edge_attr, node_indices)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(edge_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    345\u001b[0m     src, dst \u001b[38;5;241m=\u001b[39m edge_index_np[\u001b[38;5;241m0\u001b[39m, i], edge_index_np[\u001b[38;5;241m1\u001b[39m, i]\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m node_indices_set \u001b[38;5;129;01mand\u001b[39;00m dst \u001b[38;5;129;01min\u001b[39;00m node_indices_set:\n\u001b[0;32m    347\u001b[0m         mask\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Create new edge_index and edge_attr\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keep utility functions unchanged\n",
    "# apply_price_transformation, create_calculated_features, evaluate_gnn_predictions, etc.\n",
    "# [...]\n",
    "\n",
    "# Modified: Memory efficient graph building\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=3, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    with reduced memory usage\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes in smaller batches\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    batch_size = 1000  # Process in batches to reduce memory\n",
    "    for batch_start in range(0, len(test_coords), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(test_coords))\n",
    "        \n",
    "        for test_idx in range(batch_start, batch_end):\n",
    "            test_feat = test_features[test_idx - batch_start]\n",
    "            \n",
    "            for neighbor_idx, distance in zip(indices[test_idx - batch_start], distances[test_idx - batch_start]):\n",
    "                # Calculate feature similarity\n",
    "                train_feat = train_features[neighbor_idx]\n",
    "                feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "                \n",
    "                if feat_norm_product > 1e-8:\n",
    "                    feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "                else:\n",
    "                    feat_sim = 0.0\n",
    "                \n",
    "                # Normalize distance for better numerical stability\n",
    "                geo_weight = 1.0 / (distance + 1e-6)\n",
    "                \n",
    "                # Combined weight\n",
    "                combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                \n",
    "                # Add edges\n",
    "                edge_index.append([test_idx + len(train_data), neighbor_idx])\n",
    "                edge_attr.append([combined_weight])\n",
    "                \n",
    "                # Add reverse edge\n",
    "                edge_index.append([neighbor_idx, test_idx + len(train_data)])\n",
    "                edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "# NEW: Memory-efficient TimeSeriesEncoder\n",
    "class TimeSeriesEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=2, num_layers=1, dropout=0.1):\n",
    "        super(TimeSeriesEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Simpler transformer with fewer parameters\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,  # Reduced from 4 to 2\n",
    "            dim_feedforward=hidden_dim*2,  # Reduced from 4x to 2x\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # More stable training\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if x.shape[0] == 0:  # Handle empty batch case\n",
    "            return torch.zeros((0, self.embedding.out_features), device=x.device)\n",
    "            \n",
    "        # x shape: [batch_size, seq_len, features]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Handle potential memory issues with very large batches\n",
    "        if x.shape[0] > 1000:  # Process in sub-batches if batch size is large\n",
    "            outputs = []\n",
    "            sub_batch_size = 1000\n",
    "            \n",
    "            for i in range(0, x.shape[0], sub_batch_size):\n",
    "                end_idx = min(i + sub_batch_size, x.shape[0])\n",
    "                sub_batch = x[i:end_idx]\n",
    "                sub_mask = src_key_padding_mask[i:end_idx] if src_key_padding_mask is not None else None\n",
    "                \n",
    "                sub_output = self.transformer(sub_batch, src_key_padding_mask=sub_mask)\n",
    "                outputs.append(sub_output)\n",
    "                \n",
    "            return torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            return self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "# NEW: Memory-efficient CrossAttention\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, spatial_dim, temporal_dim, heads=2, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        # Project to same dimension\n",
    "        self.project_spatial = nn.Linear(spatial_dim, spatial_dim) if spatial_dim != temporal_dim else nn.Identity()\n",
    "        self.project_temporal = nn.Linear(temporal_dim, spatial_dim) if temporal_dim != spatial_dim else nn.Identity()\n",
    "        \n",
    "        # Multi-head attention with fewer heads\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=spatial_dim,\n",
    "            num_heads=heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization and feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(spatial_dim)\n",
    "        self.norm2 = nn.LayerNorm(spatial_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, spatial_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(spatial_dim * 2, spatial_dim)\n",
    "        )\n",
    "        \n",
    "        # FIXED: Output projection with correct dimensions\n",
    "        # Use spatial_dim + temporal_dim to match concatenated dimensions\n",
    "        self.output_proj = nn.Linear(spatial_dim + temporal_dim, spatial_dim * 2)\n",
    "\n",
    "    def forward(self, spatial_features, temporal_context):\n",
    "        # Check for empty inputs\n",
    "        if spatial_features.shape[0] == 0:\n",
    "            output_dim = self.output_proj.out_features\n",
    "            return torch.zeros((0, output_dim), device=spatial_features.device)\n",
    "        \n",
    "        # Project features if needed\n",
    "        spatial_proj = self.project_spatial(spatial_features)\n",
    "        temporal_proj = self.project_temporal(temporal_context)\n",
    "        \n",
    "        # Reshape temporal context if needed\n",
    "        if temporal_proj.dim() == 2:\n",
    "            # Add sequence dimension (treat as single token)\n",
    "            temporal_proj = temporal_proj.unsqueeze(1)\n",
    "        \n",
    "        # Multi-head attention - spatial as query, temporal as key/value\n",
    "        spatial_proj_seq = spatial_proj.unsqueeze(1) if spatial_proj.dim() == 2 else spatial_proj\n",
    "        \n",
    "        # Process in smaller batches if input is large\n",
    "        if spatial_proj_seq.shape[0] > 1000:\n",
    "            outputs = []\n",
    "            sub_batch_size = 1000\n",
    "            \n",
    "            for i in range(0, spatial_proj_seq.shape[0], sub_batch_size):\n",
    "                end_idx = min(i + sub_batch_size, spatial_proj_seq.shape[0])\n",
    "                \n",
    "                # Extract sub-batches\n",
    "                sub_spatial = spatial_proj_seq[i:end_idx]\n",
    "                sub_temporal = temporal_proj[i:end_idx] if temporal_proj.shape[0] > 1 else temporal_proj\n",
    "                \n",
    "                # Process sub-batch\n",
    "                sub_attn_output, _ = self.multihead_attn(\n",
    "                    query=sub_spatial,\n",
    "                    key=sub_temporal,\n",
    "                    value=sub_temporal\n",
    "                )\n",
    "                outputs.append(sub_attn_output)\n",
    "                \n",
    "            attn_output = torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            attn_output, _ = self.multihead_attn(\n",
    "                query=spatial_proj_seq,\n",
    "                key=temporal_proj,\n",
    "                value=temporal_proj\n",
    "            )\n",
    "        \n",
    "        # Remove sequence dimension if needed\n",
    "        if spatial_features.dim() == 2:\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        spatial_features = self.norm1(spatial_proj + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN\n",
    "        ffn_output = self.ffn(spatial_features)\n",
    "        \n",
    "        # Second residual connection\n",
    "        spatial_features = self.norm2(spatial_features + self.dropout(ffn_output))\n",
    "        \n",
    "        # Concatenate with temporal\n",
    "        final_output = torch.cat([\n",
    "            spatial_features, \n",
    "            temporal_context.squeeze(1) if temporal_context.dim() == 3 else temporal_context\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Apply final projection\n",
    "        return self.output_proj(final_output)\n",
    "\n",
    "# Simplified and memory-efficient GNN\n",
    "class SimplifiedListingGNN(nn.Module):\n",
    "    def __init__(self, spatial_dim, temporal_dim, price_history_dim, hidden_dim=32, heads=2):\n",
    "        super(SimplifiedListingGNN, self).__init__()\n",
    "        \n",
    "        # Spatial component\n",
    "        self.gat = GATv2Conv(spatial_dim, hidden_dim, heads=heads, edge_dim=1)\n",
    "        \n",
    "        # Temporal encoder (transformer-based)\n",
    "        self.time_series_encoder = TimeSeriesEncoder(\n",
    "            input_dim=price_history_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=heads\n",
    "        )\n",
    "        \n",
    "        # Calculate proper dimensions\n",
    "        gat_output_dim = hidden_dim * heads  # 64 with default values\n",
    "        \n",
    "        # CrossAttention between spatial and temporal\n",
    "        self.cross_attention = CrossAttention(gat_output_dim, hidden_dim)\n",
    "        \n",
    "        # FIXED: Output projection needs to match CrossAttention output\n",
    "        # CrossAttention outputs spatial_dim*2 (64*2 = 128) features\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(gat_output_dim * 2, hidden_dim),  # 128 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, batch_indices=None):\n",
    "        # Get the device of the model\n",
    "        model_device = next(self.parameters()).device\n",
    "        \n",
    "        # If batch_indices is provided, use only those indices\n",
    "        if batch_indices is not None:\n",
    "            # CRITICAL FIX: Move indices to the same device as the data (CPU) for indexing\n",
    "            cpu_indices = batch_indices.cpu()\n",
    "            \n",
    "            # Use CPU indices to index CPU tensors\n",
    "            batch_x = data.x[cpu_indices]\n",
    "            \n",
    "            # After indexing, move results to the model's device\n",
    "            batch_x = batch_x.to(model_device)\n",
    "            \n",
    "            # Get filtered edges\n",
    "            batch_edge_index, batch_edge_attr = self.filter_edges(data.edge_index, data.edge_attr, cpu_indices)\n",
    "            \n",
    "            # Move edges to the model's device\n",
    "            batch_edge_index = batch_edge_index.to(model_device)\n",
    "            batch_edge_attr = batch_edge_attr.to(model_device)\n",
    "            \n",
    "            # Process spatial data with GAT\n",
    "            spatial_features = self.gat(batch_x, batch_edge_index, edge_attr=batch_edge_attr)\n",
    "            \n",
    "            # Get temporal data for batch - also use CPU indices for indexing\n",
    "            if hasattr(data, 'price_history'):\n",
    "                batch_history = data.price_history[cpu_indices].to(model_device)\n",
    "                batch_mask = data.price_history_mask[cpu_indices].to(model_device) if hasattr(data, 'price_history_mask') else None\n",
    "                \n",
    "                # Process temporal data with transformer\n",
    "                temporal_features = self.time_series_encoder(batch_history, src_key_padding_mask=batch_mask)\n",
    "                \n",
    "                # Get the last non-masked position or just the last one\n",
    "                if batch_mask is not None:\n",
    "                    # Find the last non-masked position\n",
    "                    non_mask_indices = (~batch_mask).sum(dim=1) - 1\n",
    "                    non_mask_indices = torch.clamp(non_mask_indices, min=0)\n",
    "                    \n",
    "                    # Get embeddings at these positions\n",
    "                    batch_indices_local = torch.arange(len(batch_indices), device=model_device)\n",
    "                    temporal_context = temporal_features[batch_indices_local, non_mask_indices]\n",
    "                else:\n",
    "                    # Use the last timestep\n",
    "                    temporal_context = temporal_features[:, -1]\n",
    "            else:\n",
    "                # Create dummy temporal context\n",
    "                temporal_context = torch.zeros((len(batch_indices), self.time_series_encoder.embedding.out_features), \n",
    "                                        device=model_device)\n",
    "        else:\n",
    "            # Process entire graph (original code)\n",
    "            # Process spatial data with GAT\n",
    "            spatial_features = self.gat(data.x, data.edge_index, edge_attr=data.edge_attr)\n",
    "            \n",
    "            # Check if price_history attribute exists\n",
    "            if hasattr(data, 'price_history'):\n",
    "                # Process temporal data with transformer\n",
    "                temporal_features = self.time_series_encoder(data.price_history, \n",
    "                                                          src_key_padding_mask=data.price_history_mask \n",
    "                                                          if hasattr(data, 'price_history_mask') else None)\n",
    "                \n",
    "                # Use the last non-masked timestep or just the last one\n",
    "                batch_size = data.x.shape[0]\n",
    "                \n",
    "                if hasattr(data, 'price_history_mask') and data.price_history_mask is not None:\n",
    "                    # Find the last non-masked position\n",
    "                    non_mask_indices = (~data.price_history_mask).sum(dim=1) - 1\n",
    "                    non_mask_indices = torch.clamp(non_mask_indices, min=0)\n",
    "                    \n",
    "                    # Get embeddings at these positions\n",
    "                    batch_indices = torch.arange(batch_size, device=data.x.device)\n",
    "                    temporal_context = temporal_features[batch_indices, non_mask_indices]\n",
    "                else:\n",
    "                    # Use the last timestep\n",
    "                    temporal_context = temporal_features[:, -1]\n",
    "            else:\n",
    "                # Create dummy temporal context if price_history is missing\n",
    "                temporal_context = torch.zeros((data.x.shape[0], self.time_series_encoder.embedding.out_features), \n",
    "                                          device=data.x.device)\n",
    "        \n",
    "        # Cross-attention between spatial and temporal\n",
    "        fused_features = self.cross_attention(spatial_features, temporal_context)\n",
    "        \n",
    "        # Prediction\n",
    "        return self.output_mlp(fused_features)\n",
    "    \n",
    "    def filter_edges(self, edge_index, edge_attr, node_indices):\n",
    "        \"\"\"Filter edges to keep only those connecting nodes in the batch\"\"\"\n",
    "        # Get the model's device\n",
    "        model_device = next(self.parameters()).device\n",
    "        \n",
    "        # Convert node_indices to CPU for set operations\n",
    "        node_indices_set = set(node_indices.cpu().numpy())\n",
    "        \n",
    "        # Find edges where both source and target are in node_indices\n",
    "        mask = []\n",
    "        edge_index_np = edge_index.cpu().numpy()\n",
    "        \n",
    "        for i in range(edge_index.shape[1]):\n",
    "            src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
    "            if src in node_indices_set and dst in node_indices_set:\n",
    "                mask.append(i)\n",
    "        \n",
    "        # Create new edge_index and edge_attr\n",
    "        if not mask:\n",
    "            # Return empty tensors on the model's device\n",
    "            return (torch.zeros((2, 0), dtype=edge_index.dtype, device=model_device),\n",
    "                torch.zeros((0, edge_attr.shape[1]), dtype=edge_attr.dtype, device=model_device))\n",
    "        \n",
    "        # Get the filtered edges\n",
    "        filtered_edge_index = edge_index[:, mask]\n",
    "        filtered_edge_attr = edge_attr[mask]\n",
    "        \n",
    "        # Create remapped edge_index\n",
    "        node_remap = {old_idx: new_idx for new_idx, old_idx in enumerate(node_indices.cpu().numpy())}\n",
    "        \n",
    "        # Create remapped edge_index\n",
    "        remapped_edge_index = torch.zeros_like(filtered_edge_index, device=model_device)\n",
    "        for i in range(filtered_edge_index.shape[1]):\n",
    "            remapped_edge_index[0, i] = node_remap[filtered_edge_index[0, i].item()]\n",
    "            remapped_edge_index[1, i] = node_remap[filtered_edge_index[1, i].item()]\n",
    "        \n",
    "        # Move to model device\n",
    "        return remapped_edge_index.to(model_device), filtered_edge_attr.to(model_device)\n",
    "\n",
    "# MODIFIED: Memory-efficient data preparation\n",
    "def prepare_simplified_graph_data(train_data, test_data, feature_groups, device, sequence_length=10):\n",
    "    \"\"\"Prepare graph data with direct access to raw time series, keeping data on CPU initially\"\"\"\n",
    "    print(\"Preparing graph data...\")\n",
    "    \n",
    "    # Create scalers for features\n",
    "    scalers = {}\n",
    "    \n",
    "    # Scale the target (price)\n",
    "    target_scaler = StandardScaler()\n",
    "    train_data['price'] = target_scaler.fit_transform(train_data[['price']])\n",
    "    if 'price' in test_data.columns:\n",
    "        test_data['price'] = target_scaler.transform(test_data[['price']])\n",
    "    scalers['target'] = target_scaler\n",
    "    \n",
    "    # Process basic features\n",
    "    train_features = extract_basic_features(train_data, feature_groups)\n",
    "    test_features = extract_basic_features(test_data, feature_groups)\n",
    "    \n",
    "    # Create simple price histories (but keep on CPU)\n",
    "    print(\"Creating price histories...\")\n",
    "    train_histories = create_simple_price_histories(train_data, sequence_length)\n",
    "    test_histories = create_simple_price_histories(test_data, sequence_length)\n",
    "    \n",
    "    # Get a sample history to determine dimensions\n",
    "    sample_history = None\n",
    "    for hist in train_histories.values():\n",
    "        if hist is not None and hist.shape[0] > 0:\n",
    "            sample_history = hist\n",
    "            break\n",
    "            \n",
    "    if sample_history is None:\n",
    "        for hist in test_histories.values():\n",
    "            if hist is not None and hist.shape[0] > 0:\n",
    "                sample_history = hist\n",
    "                break\n",
    "    \n",
    "    # Prepare batch price history tensor - but keep on CPU\n",
    "    all_listing_ids = np.concatenate([\n",
    "        train_data['listing_id'].values,\n",
    "        test_data['listing_id'].values\n",
    "    ])\n",
    "    \n",
    "    if sample_history is not None:\n",
    "        # Determine dimensions from sample\n",
    "        seq_len, feature_dim = sample_history.shape\n",
    "        batch_size = len(all_listing_ids)\n",
    "        \n",
    "        # Create batch tensor on CPU\n",
    "        batch_histories = torch.zeros((batch_size, seq_len, feature_dim))\n",
    "        \n",
    "        # Combine histories\n",
    "        all_histories = {**train_histories, **test_histories}\n",
    "        \n",
    "        # Fill batch tensor\n",
    "        for i, lid in enumerate(all_listing_ids):\n",
    "            if lid in all_histories and all_histories[lid] is not None:\n",
    "                history = all_histories[lid]\n",
    "                if history.shape[0] > 0:\n",
    "                    # Make sure dimensions match\n",
    "                    actual_seq_len = min(history.shape[0], seq_len)\n",
    "                    batch_histories[i, :actual_seq_len, :] = history[:actual_seq_len]\n",
    "    else:\n",
    "        # Fallback if no valid histories found\n",
    "        batch_histories = torch.zeros((len(all_listing_ids), 1, 1))\n",
    "    \n",
    "    # Build graph structure - this stays on CPU initially\n",
    "    print(\"Building spatial graph...\")\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(train_data, test_data, k=3)  # Reduced neighbors\n",
    "    \n",
    "    # Create masks and combine data\n",
    "    train_mask = torch.zeros(len(train_features) + len(test_features), dtype=torch.bool)\n",
    "    train_mask[:len(train_features)] = True\n",
    "    \n",
    "    val_mask = torch.zeros(len(train_features) + len(test_features), dtype=torch.bool)\n",
    "    val_mask[len(train_features):] = True\n",
    "    \n",
    "    # Combine features\n",
    "    all_features = np.vstack([train_features, test_features])\n",
    "    \n",
    "    # Get target values\n",
    "    train_y = train_data['price'].values\n",
    "    test_y = test_data['price'].values if 'price' in test_data.columns else np.zeros(len(test_features))\n",
    "    all_y = np.concatenate([train_y, test_y])\n",
    "    \n",
    "    # Create PyG Data object with price_history attribute - all on CPU\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(all_features),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.FloatTensor(all_y.reshape(-1, 1)),\n",
    "        train_mask=train_mask,\n",
    "        val_mask=val_mask,\n",
    "        listing_ids=torch.LongTensor(all_listing_ids),\n",
    "        price_history=batch_histories,\n",
    "        price_history_mask=(batch_histories.sum(dim=-1) == 0)  # Add mask for attention\n",
    "    )\n",
    "    \n",
    "    # Add other feature types - keep on CPU\n",
    "    data.temporal_x = extract_temporal_features(train_data, test_data, feature_groups, 'cpu')\n",
    "    data.amenity_x = extract_amenity_features(train_data, test_data, feature_groups, 'cpu')\n",
    "    data.price_history_x = extract_price_history_features(train_data, test_data, feature_groups, 'cpu')\n",
    "    \n",
    "    return data, scalers, train_histories\n",
    "\n",
    "# NEW: Function to create mini-batches from train indices\n",
    "# Fix: Update the create_batch_data function to properly handle device\n",
    "def create_batch_data(graph_data, batch_indices, device):\n",
    "    \"\"\"Create a sub-batch of data for specified indices and move to device\"\"\"\n",
    "    # Create indices tensor but keep on CPU initially for indexing\n",
    "    indices_tensor = torch.tensor(batch_indices)\n",
    "    \n",
    "    # Extract features for this batch using CPU indices\n",
    "    # (since graph_data tensors are on CPU)\n",
    "    batch_x = graph_data.x[indices_tensor]\n",
    "    batch_y = graph_data.y[indices_tensor]\n",
    "    batch_price_history = graph_data.price_history[indices_tensor]\n",
    "    batch_price_history_mask = graph_data.price_history_mask[indices_tensor]\n",
    "    \n",
    "    # After indexing is complete, THEN move everything to the target device\n",
    "    if device is not None:\n",
    "        indices_tensor = indices_tensor.to(device)\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_price_history = batch_price_history.to(device)\n",
    "        batch_price_history_mask = batch_price_history_mask.to(device)\n",
    "    \n",
    "    return {\n",
    "        'x': batch_x,\n",
    "        'y': batch_y,\n",
    "        'price_history': batch_price_history,\n",
    "        'price_history_mask': batch_price_history_mask,\n",
    "        'indices': indices_tensor\n",
    "    }\n",
    "\n",
    "# MODIFIED: Memory-efficient training function with mini-batches\n",
    "def train_simplified_gnn_model(train_data, val_data, feature_groups, device='cuda', \n",
    "                              hidden_dim=32, epochs=30, lr=0.001, sequence_length=10,\n",
    "                              batch_size=16384, grad_accum_steps=2):  # Added batch_size\n",
    "    \"\"\"Train the simplified GNN model with transformer-based temporal processing using mini-batches\"\"\"\n",
    "    # Prepare data - keep on CPU\n",
    "    graph_data, scalers, histories = prepare_simplified_graph_data(\n",
    "        train_data, val_data, feature_groups, 'cpu', sequence_length\n",
    "    )\n",
    "    \n",
    "    # Get input dimensions\n",
    "    sample_history = next(iter(histories.values())) if histories else None\n",
    "    price_history_dim = sample_history.shape[1] if sample_history is not None else 1\n",
    "    spatial_dim = graph_data.x.shape[1]\n",
    "    \n",
    "    print(f\"Input dimensions - Spatial: {spatial_dim}, Price history: {price_history_dim}\")\n",
    "    \n",
    "    # Initialize model with reduced dimensions\n",
    "    model = SimplifiedListingGNN(\n",
    "        spatial_dim=spatial_dim,\n",
    "        temporal_dim=hidden_dim,\n",
    "        price_history_dim=price_history_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        heads=2  # Reduced from 4\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Get train indices\n",
    "    train_indices = torch.where(graph_data.train_mask)[0].cpu().numpy()\n",
    "    val_indices = torch.where(graph_data.val_mask)[0].cpu().numpy()\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_train_batches = int(np.ceil(len(train_indices) / batch_size))\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    # Move small parts of the graph to device as needed\n",
    "    edge_index = graph_data.edge_index.to(device)\n",
    "    edge_attr = graph_data.edge_attr.to(device)\n",
    "    \n",
    "    print(f\"Training with {num_train_batches} mini-batches per epoch, batch size: {batch_size}\")\n",
    "    \n",
    "    # Wrap graph_data in a simple object with to(device) method for compatibility\n",
    "    # Fix the GraphData class to properly expose attributes\n",
    "    class GraphData:\n",
    "        def __init__(self, data, edge_index, edge_attr):\n",
    "            self.data = data\n",
    "            self.edge_index = edge_index\n",
    "            self.edge_attr = edge_attr\n",
    "            \n",
    "        def __getattr__(self, name):\n",
    "            # Forward attribute access to the data object\n",
    "            if hasattr(self.data, name):\n",
    "                return getattr(self.data, name)\n",
    "            raise AttributeError(f\"'GraphData' object has no attribute '{name}'\")\n",
    "            \n",
    "        def to(self, device):\n",
    "            return self\n",
    "    \n",
    "    graph_obj = GraphData(graph_data, edge_index, edge_attr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Shuffle train indices for each epoch\n",
    "        np.random.shuffle(train_indices)\n",
    "        \n",
    "        # Zero gradients once at the beginning\n",
    "        optimizer.zero_grad()\n",
    "        accumulated_batches = 0\n",
    "        \n",
    "        # Process in mini-batches\n",
    "        for batch_idx in range(num_train_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(train_indices))\n",
    "            batch_train_indices = train_indices[start_idx:end_idx]\n",
    "            \n",
    "            # Create batch data and move to device\n",
    "            batch_data = create_batch_data(graph_data, batch_train_indices, device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                # Model will handle moving tensors to the correct device\n",
    "                out = model(graph_obj, batch_data['indices'])\n",
    "                # Move y to the same device as the output\n",
    "                y = batch_data['y'].to(out.device)\n",
    "                # Scale loss by accumulation steps\n",
    "                loss = criterion(out, y) / grad_accum_steps\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Track total loss (multiply back by grad_accum_steps for reporting)\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "            \n",
    "            accumulated_batches += 1\n",
    "            \n",
    "            # Only optimize after several batches or at the end\n",
    "            if accumulated_batches == grad_accum_steps or batch_idx == num_train_batches - 1:\n",
    "                # Update weights now\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                accumulated_batches = 0\n",
    "            \n",
    "            # Free up memory\n",
    "            del batch_data\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = total_loss / num_train_batches\n",
    "        \n",
    "        # Validation - also in batches\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        # Calculate validation in smaller batches\n",
    "        val_batch_size = batch_size\n",
    "        num_val_batches = int(np.ceil(len(val_indices) / val_batch_size))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx in range(num_val_batches):\n",
    "                start_idx = batch_idx * val_batch_size\n",
    "                end_idx = min(start_idx + val_batch_size, len(val_indices))\n",
    "                batch_val_indices = val_indices[start_idx:end_idx]\n",
    "                \n",
    "                # Create batch data and move to device\n",
    "                batch_data = create_batch_data(graph_data, batch_val_indices, device)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast():\n",
    "                    val_out = model(graph_obj, batch_data['indices'].to(device))\n",
    "                    batch_val_loss = criterion(val_out, batch_data['y'])\n",
    "                \n",
    "                val_loss += batch_val_loss.item() * len(batch_val_indices)\n",
    "                \n",
    "                # Store predictions and targets\n",
    "                val_preds.append(val_out.cpu().numpy())\n",
    "                val_targets.append(batch_data['y'].cpu().numpy())\n",
    "                \n",
    "                # Free memory\n",
    "                del batch_data\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine validation results\n",
    "        val_preds = np.vstack(val_preds)\n",
    "        val_targets = np.vstack(val_targets)\n",
    "        \n",
    "        # Calculate validation loss and metrics\n",
    "        val_loss = val_loss / len(val_indices)\n",
    "        \n",
    "        # Convert predictions to original scale for metrics\n",
    "        val_pred_orig = np.expm1(scalers['target'].inverse_transform(val_preds))\n",
    "        val_true_orig = np.expm1(scalers['target'].inverse_transform(val_targets))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "        val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, scalers\n",
    "\n",
    "# MODIFIED: Memory-efficient prediction function\n",
    "def predict_with_simplified_gnn(model, test_data, feature_groups, scalers, train_data, device, sequence_length=10, batch_size=256):\n",
    "    \"\"\"Make predictions with the simplified GNN model using batching\"\"\"\n",
    "    # Prepare data\n",
    "    graph_data, _, _ = prepare_simplified_graph_data(\n",
    "        train_data, test_data, feature_groups, 'cpu', sequence_length\n",
    "    )\n",
    "    \n",
    "    # Get test indices\n",
    "    test_indices = torch.where(graph_data.val_mask)[0].cpu().numpy()\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = int(np.ceil(len(test_indices) / batch_size))\n",
    "    \n",
    "    # Prepare for predictions\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Move edge info to device\n",
    "    edge_index = graph_data.edge_index.to(device)\n",
    "    edge_attr = graph_data.edge_attr.to(device)\n",
    "    \n",
    "    # Create GraphData object\n",
    "    # Fix the GraphData class to properly expose attributes\n",
    "    class GraphData:\n",
    "        def __init__(self, data, edge_index, edge_attr):\n",
    "            self.data = data\n",
    "            self.edge_index = edge_index\n",
    "            self.edge_attr = edge_attr\n",
    "            \n",
    "        def __getattr__(self, name):\n",
    "            # Forward attribute access to the data object\n",
    "            if hasattr(self.data, name):\n",
    "                return getattr(self.data, name)\n",
    "            raise AttributeError(f\"'GraphData' object has no attribute '{name}'\")\n",
    "            \n",
    "        def to(self, device):\n",
    "            return self\n",
    "    \n",
    "    graph_obj = GraphData(graph_data, edge_index, edge_attr)\n",
    "    \n",
    "    # Make predictions in batches\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(test_indices))\n",
    "            batch_test_indices = test_indices[start_idx:end_idx]\n",
    "            \n",
    "            # Create batch data and move to device\n",
    "            batch_data = create_batch_data(graph_data, batch_test_indices, device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                batch_preds = model(graph_obj, batch_data['indices'].to(device))\n",
    "            \n",
    "            # Store predictions\n",
    "            all_predictions.append(batch_preds.cpu().numpy())\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_data\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine predictions\n",
    "    predictions = np.vstack(all_predictions)\n",
    "    \n",
    "    # Transform back to original scale\n",
    "    predictions_np = scalers['target'].inverse_transform(predictions)\n",
    "    \n",
    "    # Inverse log transformation\n",
    "    predictions_orig = np.expm1(predictions_np)\n",
    "    \n",
    "    return predictions_orig\n",
    "\n",
    "# MODIFIED: Run with smaller batches and reduced dimensions\n",
    "def run_simplified_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                            window_size=35, n_splits=5, sample_size=None, sequence_length=10,\n",
    "                                            batch_size=256, hidden_dim=32):\n",
    "    \"\"\"\n",
    "    Run simplified GNN model with rolling window cross-validation using transformer-based temporal processing\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups - using a dictionary structure for the simplified approach\n",
    "    feature_groups = {\n",
    "        'spatial': ['latitude', 'longitude'],\n",
    "        'property': ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count'],\n",
    "        'amenity': [col for col in train_data.columns if col.startswith('has_')],\n",
    "        'temporal': ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos'],\n",
    "    }\n",
    "    \n",
    "    # Add additional temporal features if available\n",
    "    additional_temporal = ['DTF_day', 'DTF_is_holiday', 'DTF_days_to_weekend', \n",
    "                         'DTF_days_since_start', 'DTF_days_to_end_month']\n",
    "    for feat in additional_temporal:\n",
    "        if feat in train_data.columns:\n",
    "            feature_groups['temporal'].append(feat)\n",
    "    \n",
    "    # Ensure all feature groups only contain columns that exist in the dataset\n",
    "    for group in feature_groups:\n",
    "        feature_groups[group] = [f for f in feature_groups[group] if f in train_data.columns]\n",
    "    \n",
    "    # Get unique dates and create test periods\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    last_35_days = unique_dates[-window_size:]\n",
    "    \n",
    "    # Define explicit test periods\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * (window_size // n_splits)\n",
    "        end_idx = start_idx + (window_size // n_splits)\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set memory optimization configurations\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        # Set memory growth strategy\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Print model configuration\n",
    "    print(f\"Using simplified GNN with transformer-based temporal processing\")\n",
    "    print(f\"Sequence length: {sequence_length}, Batch size: {batch_size}, Hidden dim: {hidden_dim}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train simplified GNN model\n",
    "        try:\n",
    "            print(f\"\\n----- Training Simplified GNN Model (Split {i+1}) -----\")\n",
    "            \n",
    "            # Clear GPU memory before training\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train the model with memory-efficient approach\n",
    "            model, scalers = train_simplified_gnn_model(\n",
    "                train_subset, val_subset, feature_groups, \n",
    "                device=device, \n",
    "                hidden_dim=hidden_dim, \n",
    "                epochs=30,  # Reduced from 50 \n",
    "                lr=0.001, \n",
    "                sequence_length=sequence_length,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating Simplified GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_simplified_gnn(\n",
    "                model, split_test_data, feature_groups, scalers,\n",
    "                train_subset, device, sequence_length=sequence_length,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, RÂ²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'simplified_gnn_model_split_{i+1}.pt')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "            # Clear memory after each split\n",
    "            del model, scalers\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Process results - similar to the existing function\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids),\n",
    "        'config': {\n",
    "            'model_type': 'simplified_memory_efficient_gnn',\n",
    "            'sequence_length': sequence_length,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': hidden_dim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== MEMORY-EFFICIENT TRANSFORMER-GNN MODEL SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"RÂ²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\" \n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/memory_efficient_gnn\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Run with memory-efficient approach\n",
    "        results = run_simplified_gnn_with_rolling_window_cv(\n",
    "            train_path=train_path,\n",
    "            train_ids_path=train_ids_path,\n",
    "            test_ids_path=test_ids_path,\n",
    "            output_dir=output_dir,\n",
    "            window_size=35,\n",
    "            n_splits=5,\n",
    "            sample_size=None,  # Use full dataset\n",
    "            sequence_length=10,  # Reduced from 30\n",
    "            batch_size=16384,     # Added batch size\n",
    "            hidden_dim=32       # Reduced from 64\n",
    "        )\n",
    "        print(f\"Memory-efficient transformer-based GNN model training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running memory-efficient transformer-based GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
