{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Price transformation function\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        The dataframe containing price data\n",
    "    inverse : bool\n",
    "        If True, apply inverse transformation; otherwise apply log transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with transformed prices\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Create calculated features (reuse from your existing code)\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataframe to add features to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with new features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 3. Build enhanced spatial graph for GNN\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (optional, can help propagate information)\n",
    "    if len(train_coords) <= 5000:  # Only for smaller datasets to avoid memory issues\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "# 4. Enhanced GNN model\n",
    "class EnhancedSpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,  # New parameter for price history features\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(EnhancedSpatioTemporalGNN, self).__init__()\n",
    "        \n",
    "        # For multi-head attention, ensure hidden_dim is divisible by heads\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # Important: Make sure the output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads  # This ensures exact dimensions\n",
    "        \n",
    "        # Replace GCN with GAT for better spatial relationship modeling\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Add batch normalization for more stable training - USING EXACT OUTPUT DIMENSIONS\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Enhanced temporal processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Enhanced amenity processing with residual connection\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # New: Price history features processing\n",
    "        self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "        self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - updated for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object - updated to include price_history_x\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # First GAT layer with batch normalization and residual\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features with enhanced layers\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res  # Residual connection\n",
    "        \n",
    "        # Process amenity features with residual connection\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process price history features with residual connection\n",
    "        price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "        price_history_features = self.price_history_bn1(price_history_features)\n",
    "        price_history_features = self.dropout(price_history_features)\n",
    "        price_history_res = price_history_features\n",
    "        price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "        price_history_features = self.price_history_bn2(price_history_features)\n",
    "        price_history_features = price_history_features + price_history_res  # Residual connection\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type - now including price history\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "# 5. Function to prepare graph data\n",
    "def prepare_enhanced_graph(train_data, val_data, spatial_features, temporal_features, \n",
    "                          amenity_features, price_history_features, spatial_scaler, \n",
    "                          temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                          device, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings using enhanced features\n",
    "    \"\"\"\n",
    "    # Scale features and convert to float32\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features].values).astype(np.float32)\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features].values).astype(np.float32)\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features].values).astype(np.float32)\n",
    "    X_train_price_history = price_history_scaler.transform(train_data[price_history_features].values).astype(np.float32)\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features].values).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features].values).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features].values).astype(np.float32)\n",
    "    X_val_price_history = price_history_scaler.transform(val_data[price_history_features].values).astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 6. Function to train GNN model\n",
    "def train_gnn_model(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                   price_history_features, hidden_dim=64, epochs=50, lr=0.001, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GNN model with log-transformed prices and price history features\n",
    "    \"\"\"\n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()  # New scaler for price history\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedSpatioTemporalGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=len(price_history_features),\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# 7. Function to make predictions with GNN\n",
    "def predict_with_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                     price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                     price_history_scaler, target_scaler, train_data, device):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained GNN model\n",
    "    \"\"\"\n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, test_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "# 8. Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 9. Function to plot results\n",
    "def plot_gnn_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot GNN prediction results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(np.median(pct_errors), color='r', linestyle='--', \n",
    "              label=f'Median: {np.median(pct_errors):.2f}%')\n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'gnn_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'gnn_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 10. Main function to run the GNN model\n",
    "def run_strap_with_gnn(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run enhanced STRAP model with GNN, log-transformed prices, and price history features\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Split data into train and test based on listing IDs\n",
    "    train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "    test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "    \n",
    "    train_df = train_data[train_mask].copy()\n",
    "    test_df = train_data[test_mask].copy()\n",
    "    \n",
    "    print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "    print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # New: Price history features - price lags and rolling statistics\n",
    "    price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_df.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_df['dummy_amenity'] = 1\n",
    "        test_df['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_df['dummy_price_history'] = 1\n",
    "        test_df['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Apply log transformation to prices\n",
    "    train_df = apply_price_transformation(train_df)\n",
    "    test_df = apply_price_transformation(test_df)\n",
    "    \n",
    "    # Split train data into train and validation\n",
    "    unique_train_listings = train_df['listing_id'].unique()\n",
    "    train_listings, val_listings = train_test_split(\n",
    "        unique_train_listings, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "    val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "    \n",
    "    print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "    print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Train GNN model\n",
    "    print(\"\\n===== Training GNN Model =====\")\n",
    "    gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "        train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device\n",
    "    )\n",
    "    \n",
    "    # Return model and scalers to be used for predictions\n",
    "    return gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# Add this function to implement rolling window CV for the GNN model\n",
    "def run_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                   window_size=35, n_splits=5, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    \n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # New: Price history features - price lags and rolling statistics\n",
    "    price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "        'price_range_7d', 'price_range_14d', 'price_range_30d'\n",
    "        ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_data.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_data['dummy_amenity'] = 1\n",
    "        #test_data['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        #test_df['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Get unique dates and ensure they're properly sorted\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    \n",
    "    # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "    last_35_days = unique_dates[-35:]\n",
    "    \n",
    "    # Define explicit test periods - each 7 days\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * 7\n",
    "        end_idx = start_idx + 7\n",
    "        # Make sure we don't go beyond the available data\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    # Adjust n_splits if we couldn't create enough test periods\n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train GNN model for this split\n",
    "        try:\n",
    "            print(f\"\\n----- Training GNN Model (Split {i+1}) -----\")\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_gnn(\n",
    "                gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                price_history_scaler, target_scaler, train_subset, device\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                torch.save(gnn_model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids)\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'gnn_rolling_window_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'gnn_rolling_window_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'gnn_cv_summary.txt'), 'w') as f:\n",
    "            f.write(f\"GNN Rolling Window CV Model Summary\\n\")\n",
    "            f.write(f\"=================================\\n\\n\")\n",
    "            f.write(f\"Window size: {window_size} days\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics:\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_gnn_rolling_window_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Function to plot rolling window results\n",
    "def plot_gnn_rolling_window_results(evaluation_results):\n",
    "    \"\"\"Plot the results from GNN rolling window cross-validation\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Add title\n",
    "    fig.suptitle('GNN Model Evaluation with Rolling Window CV', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Cross-validation splits performance\n",
    "    splits_x = splits['split']\n",
    "    metrics_to_plot = ['rmse', 'mae']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        sns.lineplot(\n",
    "            x=splits_x,\n",
    "            y=splits[metric],\n",
    "            marker='o',\n",
    "            label=metric.upper(),\n",
    "            ax=axes[0, 1]\n",
    "        )\n",
    "    \n",
    "    axes[0, 1].set_title('Performance Across CV Splits')\n",
    "    axes[0, 1].set_xlabel('CV Split')\n",
    "    axes[0, 1].set_ylabel('Error Metric')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted (colored by split)\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['price'],\n",
    "        all_results['predicted'],\n",
    "        c=all_results['split'],\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    min_val = min(all_results['price'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['price'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by CV Split)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('CV Split')\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: Performance by listing ID count\n",
    "    # Group by listing ID and calculate average absolute error for each listing\n",
    "    listing_errors = all_results.groupby('listing_id')['abs_error'].mean().reset_index()\n",
    "    listing_errors = listing_errors.sort_values('abs_error')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(listing_errors)), listing_errors['abs_error'], alpha=0.6)\n",
    "    plt.axhline(y=listing_errors['abs_error'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {listing_errors[\"abs_error\"].mean():.2f}')\n",
    "    plt.title('Average Absolute Error by Listing')\n",
    "    plt.xlabel('Listing Index (sorted by error)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Generating price history features...\n",
      "Using 2 spatial features, 5 temporal features, 27 amenity features, and 18 price history features\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "Temporal modeling: Enabled - LSTM\n",
      "Sequence length: 7\n",
      "Price history features: Enabled\n",
      "Using training data for test history: Yes\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 903142 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "Regenerating price history features for test data using training data...\n",
      "\n",
      "----- Training GNN Model (Split 1) -----\n",
      "Using LSTM for temporal modeling with sequence length 7\n",
      "Building enhanced spatial graph with 180551 test listings and 10 nearest neighbors...\n",
      "Created graph with 3611020 edges\n",
      "Created enhanced graph with 903142 nodes and 3611020 edges\n",
      "Train nodes: 722591, Val nodes: 180551\n",
      "Temporal features dim: torch.Size([903142, 7]), Price history features dim: torch.Size([903142, 21])\n",
      "Error in RNN processing: CUDA out of memory. Tried to allocate 26.55 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.04 GiB is allocated by PyTorch, and 672.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 271\u001b[0m, in \u001b[0;36mTemporalEnhancedGNN.process_price_history\u001b[1;34m(self, x, batch_size)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 271\u001b[0m     out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprice_history_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# GRU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.60 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.30 GiB is allocated by PyTorch, and 403.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1138\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_rolling_window\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# Run with rolling window cross-validation\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gnn_with_rolling_window_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 5 weeks\u001b[39;49;00m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use full dataset, set to a number for testing\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_temporal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemporal_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemporal_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m            \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_price_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_price_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_train_for_test_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_train_for_test_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1152\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN model training with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporal_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m temporal modeling completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;66;03m# Run standard GNN model training\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 888\u001b[0m, in \u001b[0;36mrun_gnn_with_rolling_window_cv\u001b[1;34m(train_path, train_ids_path, test_ids_path, output_dir, window_size, n_splits, sample_size, use_temporal, temporal_model, sequence_length, use_price_history, use_train_for_test_history)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----- Training GNN Model (Split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m     gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gnn_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamenity_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprice_history_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemporal_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_temporal\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----- Evaluating GNN on Test Data (Split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 556\u001b[0m, in \u001b[0;36mtrain_gnn_model\u001b[1;34m(train_data, val_data, spatial_features, temporal_features, amenity_features, price_history_features, hidden_dim, epochs, lr, device, batch_size, temporal_model, sequence_length, use_temporal)\u001b[0m\n\u001b[0;32m    553\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# Get outputs for training nodes only\u001b[39;00m\n\u001b[0;32m    559\u001b[0m train_out \u001b[38;5;241m=\u001b[39m out[graph_data\u001b[38;5;241m.\u001b[39mtrain_mask]\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 346\u001b[0m, in \u001b[0;36mTemporalEnhancedGNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    343\u001b[0m temporal_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_temporal_data(temporal_x, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Process price history features\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m price_history_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_price_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_history_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# Dynamic feature fusion with learned weights\u001b[39;00m\n\u001b[0;32m    349\u001b[0m normalized_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 279\u001b[0m, in \u001b[0;36mTemporalEnhancedGNN.process_price_history\u001b[1;34m(self, x, batch_size)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in price history RNN processing: \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43me\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# Fallback to feed-forward if reshape fails\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_history_layer1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_price_history_features(train_data, test_data=None, use_train_for_test=True, window_sizes=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Generate price history features for training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data with 'listing_id', 'date', and 'price' columns\n",
    "    test_data : DataFrame, optional\n",
    "        Test data with 'listing_id', 'date', and 'price' columns\n",
    "    use_train_for_test : bool, optional\n",
    "        If True, use training data to compute price history features for test data\n",
    "    window_sizes : list, optional\n",
    "        List of window sizes (in days) for rolling statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_data_with_features, test_data_with_features) DataFrames with added price history features\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy() if test_data is not None else None\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(train_df['date']):\n",
    "        train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    \n",
    "    if test_df is not None and not pd.api.types.is_datetime64_any_dtype(test_df['date']):\n",
    "        test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    \n",
    "    # Sort by listing_id and date\n",
    "    train_df = train_df.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    # Prepare data for feature generation\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Create a combined dataframe for computing features\n",
    "        # We'll compute features on combined data but return them separately\n",
    "        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "        combined_df = combined_df.sort_values(['listing_id', 'date'])\n",
    "        data_for_features = combined_df\n",
    "    else:\n",
    "        data_for_features = train_df\n",
    "    \n",
    "    # Initialize list of created features\n",
    "    created_features = []\n",
    "    \n",
    "    # Generate lag features\n",
    "    for window in window_sizes:\n",
    "        feature_name = f'price_lag_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].shift(window)\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Generate rolling statistics\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        feature_name = f'rolling_mean_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling max\n",
    "        feature_name = f'rolling_max_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).max()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling min\n",
    "        feature_name = f'rolling_min_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).min()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling std\n",
    "        feature_name = f'rolling_std_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Price volatility (max - min)\n",
    "        feature_name = f'price_range_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features[f'rolling_max_{window}d'] - data_for_features[f'rolling_min_{window}d']\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for feature in created_features:\n",
    "        if data_for_features[feature].isnull().any():\n",
    "            # Fill within each listing_id group\n",
    "            data_for_features[feature] = data_for_features.groupby('listing_id')[feature].transform(\n",
    "                lambda x: x.fillna(x.median() if x.notna().any() else 0)\n",
    "            )\n",
    "    \n",
    "    # Split combined data back to train and test if necessary\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Get indices of train and test rows\n",
    "        train_indices = data_for_features.index[:len(train_df)]\n",
    "        test_indices = data_for_features.index[len(train_df):]\n",
    "        \n",
    "        # Extract features for train and test\n",
    "        train_df = data_for_features.loc[train_indices].copy()\n",
    "        test_df = data_for_features.loc[test_indices].copy()\n",
    "        \n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        if test_df is not None:\n",
    "            # Generate features for test separately\n",
    "            test_df = test_df.sort_values(['listing_id', 'date'])\n",
    "            for feature in created_features:\n",
    "                if feature in data_for_features.columns:\n",
    "                    # Just create empty columns that will be populated later\n",
    "                    test_df[feature] = np.nan\n",
    "            \n",
    "            return data_for_features, test_df\n",
    "        else:\n",
    "            return data_for_features, None\n",
    "\n",
    "class TemporalEnhancedGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,\n",
    "                 hidden_dim=64,\n",
    "                 temporal_model='lstm',  # Options: 'lstm', 'gru', 'none'\n",
    "                 sequence_length=7,      # For RNN/LSTM/GRU\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(TemporalEnhancedGNN, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        self.temporal_model = temporal_model\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Ensure output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        \n",
    "        # Spatial component: GAT layers\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Amenity processing\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Temporal processing - with recurrent layers\n",
    "        if temporal_model.lower() == 'lstm':\n",
    "            # LSTM model for temporal features\n",
    "            self.temporal_rnn = nn.LSTM(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        elif temporal_model.lower() == 'gru':\n",
    "            # GRU model for temporal features\n",
    "            self.temporal_rnn = nn.GRU(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Default to feed-forward layers\n",
    "            self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "            self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Price history processing - with recurrent layers\n",
    "        if temporal_model.lower() in ['lstm', 'gru']:\n",
    "            # Recurrent model for price history\n",
    "            rnn_class = nn.LSTM if temporal_model.lower() == 'lstm' else nn.GRU\n",
    "            self.price_history_rnn = rnn_class(\n",
    "                input_size=price_history_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Feed-forward layers for price history\n",
    "            self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "            self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def process_temporal_data(self, x, batch_size, model_type):\n",
    "        \"\"\"Process temporal data using the specified model type (LSTM, GRU, or feed-forward)\"\"\"\n",
    "        if model_type.lower() in ['lstm', 'gru']:\n",
    "            # Reshape for RNN: [batch_size, seq_len, features_per_step]\n",
    "            try:\n",
    "                features_per_step = x.shape[1] // self.sequence_length\n",
    "                x_reshaped = x.view(batch_size, self.sequence_length, features_per_step)\n",
    "                \n",
    "                # Process through RNN\n",
    "                if model_type.lower() == 'lstm':\n",
    "                    out, (h_n, c_n) = self.temporal_rnn(x_reshaped)\n",
    "                else:  # GRU\n",
    "                    out, h_n = self.temporal_rnn(x_reshaped)\n",
    "                \n",
    "                # Use the final hidden state (concat forward and backward for bidirectional)\n",
    "                output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Use last layer's hidden state\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error in RNN processing: {e}\")\n",
    "                # Fallback to feed-forward if reshape fails\n",
    "                return self.process_temporal_data(x, batch_size, 'none')\n",
    "        else:\n",
    "            # Standard feed-forward for 'none' model\n",
    "            if hasattr(self, 'temporal_layer1'):\n",
    "                out = F.elu(self.temporal_layer1(x))\n",
    "                out = self.temporal_bn1(out)\n",
    "                out = self.dropout(out)\n",
    "                out_res = out\n",
    "                out = F.elu(self.temporal_layer2(out))\n",
    "                out = self.temporal_bn2(out)\n",
    "                out = out + out_res  # Residual connection\n",
    "                return out\n",
    "            else:\n",
    "                # If temporal layers don't exist, create a dummy output of the right shape\n",
    "                return torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "    \n",
    "    def process_price_history(self, x, batch_size):\n",
    "        \"\"\"Process price history data using the configured model type\"\"\"\n",
    "        if self.temporal_model.lower() in ['lstm', 'gru']:\n",
    "            # Use recurrent model for price history\n",
    "            try:\n",
    "                features_per_step = x.shape[1] // self.sequence_length\n",
    "                x_reshaped = x.view(batch_size, self.sequence_length, features_per_step)\n",
    "                \n",
    "                # Process through RNN\n",
    "                if self.temporal_model.lower() == 'lstm':\n",
    "                    out, (h_n, c_n) = self.price_history_rnn(x_reshaped)\n",
    "                else:  # GRU\n",
    "                    out, h_n = self.price_history_rnn(x_reshaped)\n",
    "                \n",
    "                # Use the final hidden state (concat forward and backward for bidirectional)\n",
    "                output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Use last layer's hidden state\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error in price history RNN processing: {e}\")\n",
    "                # Fallback to feed-forward if reshape fails\n",
    "                if hasattr(self, 'price_history_layer1'):\n",
    "                    out = F.elu(self.price_history_layer1(x))\n",
    "                    out = self.price_history_bn1(out)\n",
    "                    out = self.dropout(out)\n",
    "                    out_res = out\n",
    "                    out = F.elu(self.price_history_layer2(out))\n",
    "                    out = self.price_history_bn2(out)\n",
    "                    out = out + out_res  # Residual connection\n",
    "                    return out\n",
    "                else:\n",
    "                    # If price history layers don't exist, create a dummy output\n",
    "                    return torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "        else:\n",
    "            # Feed-forward processing\n",
    "            out = F.elu(self.price_history_layer1(x))\n",
    "            out = self.price_history_bn1(out)\n",
    "            out = self.dropout(out)\n",
    "            out_res = out\n",
    "            out = F.elu(self.price_history_layer2(out))\n",
    "            out = self.price_history_bn2(out)\n",
    "            out = out + out_res  # Residual connection\n",
    "            return out\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # Get batch size for reshaping temporal data\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process spatial features with GAT\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer with residual connection\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process amenity features\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process temporal features with the selected model\n",
    "        temporal_features = self.process_temporal_data(temporal_x, batch_size, self.temporal_model)\n",
    "        \n",
    "        # Process price history features\n",
    "        price_history_features = self.process_price_history(price_history_x, batch_size)\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "def prepare_enhanced_graph_for_temporal(train_data, val_data, spatial_features, temporal_features, \n",
    "                                       amenity_features, price_history_features, spatial_scaler, \n",
    "                                       temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                                       device, k=10, feature_weight=0.3, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings, with temporal features organized for RNN processing\n",
    "    \"\"\"\n",
    "    # Scale features and convert to float32\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features].values).astype(np.float32)\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features].values).astype(np.float32)\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features].values).astype(np.float32)\n",
    "    X_train_price_history = price_history_scaler.transform(train_data[price_history_features].values).astype(np.float32)\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features].values).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features].values).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features].values).astype(np.float32)\n",
    "    X_val_price_history = price_history_scaler.transform(val_data[price_history_features].values).astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Ensure temporal features are padded to be divisible by sequence_length\n",
    "    # This is needed for proper reshaping in the RNN processing\n",
    "    def pad_features(features, seq_length):\n",
    "        \"\"\"Pad features to be divisible by sequence_length\"\"\"\n",
    "        feature_dim = features.shape[1]\n",
    "        if feature_dim % seq_length != 0:\n",
    "            padding_size = seq_length - (feature_dim % seq_length)\n",
    "            padding = np.zeros((features.shape[0], padding_size), dtype=np.float32)\n",
    "            return np.hstack([features, padding])\n",
    "        return features\n",
    "    \n",
    "    # Pad temporal and price history features if using recurrent networks\n",
    "    X_train_temporal = pad_features(X_train_temporal, sequence_length)\n",
    "    X_val_temporal = pad_features(X_val_temporal, sequence_length)\n",
    "    X_train_price_history = pad_features(X_train_price_history, sequence_length)\n",
    "    X_val_price_history = pad_features(X_val_price_history, sequence_length)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    print(f\"Temporal features dim: {data.temporal_x.shape}, Price history features dim: {data.price_history_x.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_gnn_model(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                   price_history_features, hidden_dim=64, epochs=50, lr=0.001, device='cuda', \n",
    "                   batch_size=64, temporal_model='lstm', sequence_length=7, use_temporal=True):\n",
    "    \"\"\"\n",
    "    Train GNN model with enhanced temporal modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data\n",
    "    val_data : DataFrame\n",
    "        Validation data\n",
    "    spatial_features : list\n",
    "        List of spatial feature column names\n",
    "    temporal_features : list\n",
    "        List of temporal feature column names\n",
    "    amenity_features : list\n",
    "        List of amenity feature column names\n",
    "    price_history_features : list\n",
    "        List of price history feature column names\n",
    "    hidden_dim : int, optional\n",
    "        Hidden dimension size for neural network layers\n",
    "    epochs : int, optional\n",
    "        Number of training epochs\n",
    "    lr : float, optional\n",
    "        Learning rate\n",
    "    device : str, optional\n",
    "        Device to use for training ('cuda' or 'cpu')\n",
    "    batch_size : int, optional\n",
    "        Batch size for training\n",
    "    temporal_model : str, optional\n",
    "        Type of temporal model to use ('lstm', 'gru', or 'none')\n",
    "    sequence_length : int, optional\n",
    "        Sequence length for recurrent models\n",
    "    use_temporal : bool, optional\n",
    "        Whether to use temporal modeling or fallback to feed-forward\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler)\n",
    "    \"\"\"\n",
    "    # Override temporal_model if use_temporal is False\n",
    "    if not use_temporal:\n",
    "        temporal_model = 'none'\n",
    "        print(\"Temporal modeling disabled, using feed-forward layers\")\n",
    "    else:\n",
    "        print(f\"Using {temporal_model.upper()} for temporal modeling with sequence length {sequence_length}\")\n",
    "    \n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data\n",
    "    graph_data = prepare_enhanced_graph_for_temporal(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TemporalEnhancedGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=graph_data.temporal_x.shape[1],  # Use actual padded size\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=graph_data.price_history_x.shape[1],  # Use actual padded size\n",
    "        hidden_dim=hidden_dim,\n",
    "        temporal_model=temporal_model,\n",
    "        sequence_length=sequence_length,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "def run_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                  window_size=35, n_splits=5, sample_size=None, \n",
    "                                  use_temporal=True, temporal_model='lstm', sequence_length=7,\n",
    "                                  use_price_history=True, use_train_for_test_history=True):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation and enhanced temporal modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    use_temporal : bool, optional\n",
    "        Whether to use enhanced temporal modeling (LSTM/GRU)\n",
    "    temporal_model : str, optional\n",
    "        Type of temporal model to use ('lstm', 'gru', or 'none')\n",
    "    sequence_length : int, optional\n",
    "        Sequence length for recurrent models\n",
    "    use_price_history : bool, optional\n",
    "        Whether to use price history features\n",
    "    use_train_for_test_history : bool, optional\n",
    "        Whether to use training data when creating price history for test data\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    \n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Generate price history features if enabled\n",
    "    if use_price_history:\n",
    "        print(\"Generating price history features...\")\n",
    "        train_data, _ = generate_price_history_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Add custom temporal features if available\n",
    "    additional_temporal = [\n",
    "        'DTF_day', 'DTF_is_holiday', 'DTF_days_to_weekend', \n",
    "        'DTF_days_since_start', 'DTF_days_to_end_month'\n",
    "    ]\n",
    "    \n",
    "    for feat in additional_temporal:\n",
    "        if feat in train_data.columns:\n",
    "            temporal_features.append(feat)\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # Price history features - only include if use_price_history is True\n",
    "    price_history_features = []\n",
    "    if use_price_history:\n",
    "        price_history_features = [\n",
    "            'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "            'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "            'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "            'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "            'price_range_7d', 'price_range_14d', 'price_range_30d',\n",
    "            'rolling_std_7d', 'rolling_std_14d', 'rolling_std_30d'\n",
    "        ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_data.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_data['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features and use_price_history:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    elif not use_price_history:\n",
    "        # Create a dummy feature when price history is disabled\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Get unique dates and ensure they're properly sorted\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    \n",
    "    # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "    last_35_days = unique_dates[-window_size:]\n",
    "    \n",
    "    # Define explicit test periods - each 7 days\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * (window_size // n_splits)\n",
    "        end_idx = start_idx + (window_size // n_splits)\n",
    "        # Make sure we don't go beyond the available data\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    # Adjust n_splits if we couldn't create enough test periods\n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print temporal modeling configuration\n",
    "    print(f\"Temporal modeling: {'Enabled - '+temporal_model.upper() if use_temporal else 'Disabled'}\")\n",
    "    if use_temporal:\n",
    "        print(f\"Sequence length: {sequence_length}\")\n",
    "    print(f\"Price history features: {'Enabled' if use_price_history else 'Disabled'}\")\n",
    "    if use_price_history:\n",
    "        print(f\"Using training data for test history: {'Yes' if use_train_for_test_history else 'No'}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Regenerate price history for test data using the train data if enabled\n",
    "        if use_price_history and use_train_for_test_history and len(split_test_data) > 0:\n",
    "            print(\"Regenerating price history features for test data using training data...\")\n",
    "            # Select relevant columns for price history calculation\n",
    "            relevant_cols = ['listing_id', 'date', 'price'] + [col for col in price_history_features if col in split_train_data.columns]\n",
    "            \n",
    "            # Get unique columns between train and test to avoid duplication\n",
    "            train_cols = set(split_train_data.columns)\n",
    "            test_cols = set(split_test_data.columns)\n",
    "            \n",
    "            # Only regenerate price history if necessary\n",
    "            if any(col for col in price_history_features if col not in split_test_data.columns):\n",
    "                # Generate features using combined data for lookback\n",
    "                train_subset = split_train_data[relevant_cols].copy()\n",
    "                test_subset = split_test_data[['listing_id', 'date', 'price']].copy()\n",
    "                _, test_with_history = generate_price_history_features(train_subset, test_subset, use_train_for_test=True)\n",
    "                \n",
    "                # Add generated history features to test data\n",
    "                for col in price_history_features:\n",
    "                    if col in test_with_history.columns:\n",
    "                        split_test_data[col] = test_with_history[col].values\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train GNN model for this split\n",
    "        try:\n",
    "            print(f\"\\n----- Training GNN Model (Split {i+1}) -----\")\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device,\n",
    "                temporal_model=temporal_model, sequence_length=sequence_length, use_temporal=use_temporal\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_gnn(\n",
    "                gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                price_history_scaler, target_scaler, train_subset, device,\n",
    "                temporal_model=temporal_model, sequence_length=sequence_length\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                torch.save(gnn_model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids),\n",
    "        'config': {\n",
    "            'use_temporal': use_temporal,\n",
    "            'temporal_model': temporal_model,\n",
    "            'sequence_length': sequence_length,\n",
    "            'use_price_history': use_price_history,\n",
    "            'use_train_for_test_history': use_train_for_test_history\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'gnn_rolling_window_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'gnn_rolling_window_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config_file = os.path.join(output_dir, 'temporal_config.json')\n",
    "        import json\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(evaluation_results['config'], f, indent=4)\n",
    "        print(f\"Configuration saved to {config_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'gnn_cv_summary.txt'), 'w') as f:\n",
    "            f.write(f\"GNN Rolling Window CV Model Summary\\n\")\n",
    "            f.write(f\"=================================\\n\\n\")\n",
    "            f.write(f\"Window size: {window_size} days\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Temporal modeling: {'Enabled - '+temporal_model.upper() if use_temporal else 'Disabled'}\\n\")\n",
    "            if use_temporal:\n",
    "                f.write(f\"Sequence length: {sequence_length}\\n\")\n",
    "            f.write(f\"Price history features: {'Enabled' if use_price_history else 'Disabled'}\\n\")\n",
    "            if use_price_history:\n",
    "                f.write(f\"Using training data for test history: {'Yes' if use_train_for_test_history else 'No'}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics:\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_gnn_rolling_window_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def predict_with_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                    price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                    price_history_scaler, target_scaler, train_data, device,\n",
    "                    temporal_model='lstm', sequence_length=7):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained temporal GNN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : TemporalEnhancedGNN\n",
    "        Trained GNN model\n",
    "    test_data : DataFrame\n",
    "        Test data to predict on\n",
    "    spatial_features, temporal_features, amenity_features, price_history_features : list\n",
    "        Lists of feature column names\n",
    "    spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler : StandardScaler\n",
    "        Fitted scaler objects\n",
    "    train_data : DataFrame\n",
    "        Training data (used for graph construction)\n",
    "    device : str\n",
    "        Device to use for prediction\n",
    "    temporal_model : str\n",
    "        Type of temporal model used ('lstm', 'gru', or 'none')\n",
    "    sequence_length : int\n",
    "        Sequence length used in the model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Predicted prices in original scale\n",
    "    \"\"\"\n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data = prepare_enhanced_graph_for_temporal(\n",
    "        train_data, test_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\"  # Your training data CSV\n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"  # Text file with training listing IDs\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"  # Text file with test listing IDs\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/temporal_gnn_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Configuration options\n",
    "    config = {\n",
    "        'use_rolling_window': True,  # Set to True to use rolling window CV\n",
    "        'use_temporal': True,        # Set to True to use temporal modeling (LSTM/GRU)\n",
    "        'temporal_model': 'lstm',    # Options: 'lstm', 'gru', 'none'\n",
    "        'sequence_length': 7,        # Sequence length for recurrent models\n",
    "        'use_price_history': True,   # Whether to use price history features\n",
    "        'use_train_for_test_history': True  # Whether to use training data for test history\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if config['use_rolling_window']:\n",
    "            # Run with rolling window cross-validation\n",
    "            results = run_gnn_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                sample_size=None,  # Use full dataset, set to a number for testing\n",
    "                use_temporal=config['use_temporal'],\n",
    "                temporal_model=config['temporal_model'],\n",
    "                sequence_length=config['sequence_length'],\n",
    "                use_price_history=config['use_price_history'],\n",
    "                use_train_for_test_history=config['use_train_for_test_history']\n",
    "            )\n",
    "            print(f\"GNN model training with {config['temporal_model'].upper()} temporal modeling completed successfully!\")\n",
    "        else:\n",
    "            # Run standard GNN model training\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device,\n",
    "                temporal_model=config['temporal_model'], \n",
    "                sequence_length=config['sequence_length'], \n",
    "                use_temporal=config['use_temporal']\n",
    "            )\n",
    "            print(f\"GNN model training with {config['temporal_model'].upper()} temporal modeling completed successfully!\")\n",
    "            \n",
    "            # Save model and scalers\n",
    "            torch.save(gnn_model.state_dict(), os.path.join(output_dir, \"temporal_gnn_model.pt\"))\n",
    "            torch.save({\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'amenity_scaler': amenity_scaler,\n",
    "                'price_history_scaler': price_history_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'config': config\n",
    "            }, os.path.join(output_dir, \"temporal_scalers.pt\"))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running temporal GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
