{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Price transformation function\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        The dataframe containing price data\n",
    "    inverse : bool\n",
    "        If True, apply inverse transformation; otherwise apply log transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with transformed prices\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Create calculated features (reuse from your existing code)\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataframe to add features to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with new features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 3. Build enhanced spatial graph for GNN\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (optional, can help propagate information)\n",
    "    if len(train_coords) <= 5000:  # Only for smaller datasets to avoid memory issues\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "# 4. Enhanced GNN model\n",
    "class EnhancedSpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,  # New parameter for price history features\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(EnhancedSpatioTemporalGNN, self).__init__()\n",
    "        \n",
    "        # For multi-head attention, ensure hidden_dim is divisible by heads\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # Important: Make sure the output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads  # This ensures exact dimensions\n",
    "        \n",
    "        # Replace GCN with GAT for better spatial relationship modeling\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Add batch normalization for more stable training - USING EXACT OUTPUT DIMENSIONS\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Enhanced temporal processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Enhanced amenity processing with residual connection\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # New: Price history features processing\n",
    "        self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "        self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - updated for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object - updated to include price_history_x\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # First GAT layer with batch normalization and residual\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features with enhanced layers\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res  # Residual connection\n",
    "        \n",
    "        # Process amenity features with residual connection\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process price history features with residual connection\n",
    "        price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "        price_history_features = self.price_history_bn1(price_history_features)\n",
    "        price_history_features = self.dropout(price_history_features)\n",
    "        price_history_res = price_history_features\n",
    "        price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "        price_history_features = self.price_history_bn2(price_history_features)\n",
    "        price_history_features = price_history_features + price_history_res  # Residual connection\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type - now including price history\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "# 5. Function to prepare graph data\n",
    "def prepare_enhanced_graph(train_data, val_data, spatial_features, temporal_features, \n",
    "                          amenity_features, price_history_features, spatial_scaler, \n",
    "                          temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                          device, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings using enhanced features\n",
    "    \"\"\"\n",
    "    # Scale features and convert to float32\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features].values).astype(np.float32)\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features].values).astype(np.float32)\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features].values).astype(np.float32)\n",
    "    X_train_price_history = price_history_scaler.transform(train_data[price_history_features].values).astype(np.float32)\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features].values).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features].values).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features].values).astype(np.float32)\n",
    "    X_val_price_history = price_history_scaler.transform(val_data[price_history_features].values).astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 6. Function to train GNN model\n",
    "def train_gnn_model(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                   price_history_features, hidden_dim=64, epochs=50, lr=0.001, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GNN model with log-transformed prices and price history features\n",
    "    \"\"\"\n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()  # New scaler for price history\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedSpatioTemporalGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=len(price_history_features),\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# 7. Function to make predictions with GNN\n",
    "def predict_with_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                     price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                     price_history_scaler, target_scaler, train_data, device):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained GNN model\n",
    "    \"\"\"\n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, test_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "# 8. Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 9. Function to plot results\n",
    "def plot_gnn_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot GNN prediction results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(np.median(pct_errors), color='r', linestyle='--', \n",
    "              label=f'Median: {np.median(pct_errors):.2f}%')\n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'gnn_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'gnn_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 10. Main function to run the GNN model\n",
    "def run_strap_with_gnn(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run enhanced STRAP model with GNN, log-transformed prices, and price history features\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Split data into train and test based on listing IDs\n",
    "    train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "    test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "    \n",
    "    train_df = train_data[train_mask].copy()\n",
    "    test_df = train_data[test_mask].copy()\n",
    "    \n",
    "    print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "    print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # New: Price history features - price lags and rolling statistics\n",
    "    price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_df.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_df['dummy_amenity'] = 1\n",
    "        test_df['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_df['dummy_price_history'] = 1\n",
    "        test_df['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Apply log transformation to prices\n",
    "    train_df = apply_price_transformation(train_df)\n",
    "    test_df = apply_price_transformation(test_df)\n",
    "    \n",
    "    # Split train data into train and validation\n",
    "    unique_train_listings = train_df['listing_id'].unique()\n",
    "    train_listings, val_listings = train_test_split(\n",
    "        unique_train_listings, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "    val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "    \n",
    "    print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "    print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Train GNN model\n",
    "    print(\"\\n===== Training GNN Model =====\")\n",
    "    gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "        train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device\n",
    "    )\n",
    "    \n",
    "    # Return model and scalers to be used for predictions\n",
    "    return gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# Add this function to implement rolling window CV for the GNN model\n",
    "def run_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                   window_size=35, n_splits=5, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    \n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # New: Price history features - price lags and rolling statistics\n",
    "    price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "        'price_range_7d', 'price_range_14d', 'price_range_30d'\n",
    "        ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_data.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_data['dummy_amenity'] = 1\n",
    "        #test_data['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        #test_df['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Get unique dates and ensure they're properly sorted\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    \n",
    "    # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "    last_35_days = unique_dates[-35:]\n",
    "    \n",
    "    # Define explicit test periods - each 7 days\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * 7\n",
    "        end_idx = start_idx + 7\n",
    "        # Make sure we don't go beyond the available data\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    # Adjust n_splits if we couldn't create enough test periods\n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train GNN model for this split\n",
    "        try:\n",
    "            print(f\"\\n----- Training GNN Model (Split {i+1}) -----\")\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_gnn(\n",
    "                gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                price_history_scaler, target_scaler, train_subset, device\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                torch.save(gnn_model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids)\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'gnn_rolling_window_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'gnn_rolling_window_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'gnn_cv_summary.txt'), 'w') as f:\n",
    "            f.write(f\"GNN Rolling Window CV Model Summary\\n\")\n",
    "            f.write(f\"=================================\\n\\n\")\n",
    "            f.write(f\"Window size: {window_size} days\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics:\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_gnn_rolling_window_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Function to plot rolling window results\n",
    "def plot_gnn_rolling_window_results(evaluation_results):\n",
    "    \"\"\"Plot the results from GNN rolling window cross-validation\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Add title\n",
    "    fig.suptitle('GNN Model Evaluation with Rolling Window CV', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Cross-validation splits performance\n",
    "    splits_x = splits['split']\n",
    "    metrics_to_plot = ['rmse', 'mae']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        sns.lineplot(\n",
    "            x=splits_x,\n",
    "            y=splits[metric],\n",
    "            marker='o',\n",
    "            label=metric.upper(),\n",
    "            ax=axes[0, 1]\n",
    "        )\n",
    "    \n",
    "    axes[0, 1].set_title('Performance Across CV Splits')\n",
    "    axes[0, 1].set_xlabel('CV Split')\n",
    "    axes[0, 1].set_ylabel('Error Metric')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted (colored by split)\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['price'],\n",
    "        all_results['predicted'],\n",
    "        c=all_results['split'],\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    min_val = min(all_results['price'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['price'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by CV Split)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('CV Split')\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: Performance by listing ID count\n",
    "    # Group by listing ID and calculate average absolute error for each listing\n",
    "    listing_errors = all_results.groupby('listing_id')['abs_error'].mean().reset_index()\n",
    "    listing_errors = listing_errors.sort_values('abs_error')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(listing_errors)), listing_errors['abs_error'], alpha=0.6)\n",
    "    plt.axhline(y=listing_errors['abs_error'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {listing_errors[\"abs_error\"].mean():.2f}')\n",
    "    plt.title('Average Absolute Error by Listing')\n",
    "    plt.xlabel('Listing Index (sorted by error)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_listing_histories(train_data, test_data=None, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Create history sequences for each listing suitable for LSTM/GRU processing\n",
    "    \"\"\"\n",
    "    histories = {}\n",
    "    \n",
    "    # Process train listings\n",
    "    for listing_id in train_data['listing_id'].unique():\n",
    "        listing_data = train_data[train_data['listing_id'] == listing_id].sort_values('date')\n",
    "        \n",
    "        if len(listing_data) >= sequence_length:\n",
    "            # Extract price history features\n",
    "            price_cols = [col for col in listing_data.columns if col.startswith(('price_lag_', 'rolling_'))]\n",
    "            if price_cols:\n",
    "                features = listing_data[price_cols].values.astype(np.float32)\n",
    "                \n",
    "                # Create sequences\n",
    "                sequences = []\n",
    "                for i in range(len(listing_data) - sequence_length + 1):\n",
    "                    sequences.append(features[i:i+sequence_length])\n",
    "                \n",
    "                if sequences:\n",
    "                    histories[listing_id] = torch.FloatTensor(sequences[-1:])  # Just use most recent sequence\n",
    "    \n",
    "    # Process test listings if provided\n",
    "    if test_data is not None:\n",
    "        for listing_id in test_data['listing_id'].unique():\n",
    "            if listing_id in histories:\n",
    "                continue  # Already processed in train\n",
    "                \n",
    "            listing_data = test_data[test_data['listing_id'] == listing_id].sort_values('date')\n",
    "            \n",
    "            if len(listing_data) >= sequence_length:\n",
    "                # Extract price history features\n",
    "                price_cols = [col for col in listing_data.columns if col.startswith(('price_lag_', 'rolling_'))]\n",
    "                if price_cols:\n",
    "                    features = listing_data[price_cols].values.astype(np.float32)\n",
    "                    \n",
    "                    # Create sequences\n",
    "                    sequences = []\n",
    "                    for i in range(len(listing_data) - sequence_length + 1):\n",
    "                        sequences.append(features[i:i+sequence_length])\n",
    "                    \n",
    "                    if sequences:\n",
    "                        histories[listing_id] = torch.FloatTensor(sequences[-1:])  # Just use most recent sequence\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def generate_price_history_features_by_listing(train_data, test_data=None, window_sizes=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Generate price history features at the listing level rather than row level\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data with 'listing_id', 'date', and 'price' columns\n",
    "    test_data : DataFrame, optional\n",
    "        Test data with 'listing_id', 'date', and 'price' columns\n",
    "    window_sizes : list, optional\n",
    "        List of window sizes (in days) for rolling statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_data_with_features, test_data_with_features) DataFrames with added price history features\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy() if test_data is not None else None\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(train_df['date']):\n",
    "        train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    \n",
    "    if test_df is not None and not pd.api.types.is_datetime64_any_dtype(test_df['date']):\n",
    "        test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    \n",
    "    # Process listings one by one to save memory\n",
    "    all_train_listings = train_df['listing_id'].unique()\n",
    "    all_test_listings = test_df['listing_id'].unique() if test_df is not None else []\n",
    "    \n",
    "    # Create empty DataFrames to hold results\n",
    "    train_features = []\n",
    "    test_features = []\n",
    "    \n",
    "    print(f\"Processing history features for {len(all_train_listings)} train listings and \"\n",
    "          f\"{len(all_test_listings)} test listings\")\n",
    "    \n",
    "    # Process train listings\n",
    "    for i, listing_id in enumerate(all_train_listings):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing train listing {i}/{len(all_train_listings)}\")\n",
    "            \n",
    "        # Get data for this listing only\n",
    "        listing_data = train_df[train_df['listing_id'] == listing_id].sort_values('date')\n",
    "        \n",
    "        # Generate features for this listing\n",
    "        features = generate_listing_history_features(listing_data, window_sizes)\n",
    "        train_features.append(features)\n",
    "    \n",
    "    # Process test listings\n",
    "    if test_df is not None:\n",
    "        for i, listing_id in enumerate(all_test_listings):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing test listing {i}/{len(all_test_listings)}\")\n",
    "                \n",
    "            # Get previous data for this listing if available in train\n",
    "            train_history = train_df[train_df['listing_id'] == listing_id].sort_values('date')\n",
    "            test_history = test_df[test_df['listing_id'] == listing_id].sort_values('date')\n",
    "            \n",
    "            # Combine train history with test data for this listing if available\n",
    "            if len(train_history) > 0:\n",
    "                combined_history = pd.concat([train_history, test_history], ignore_index=True).sort_values('date')\n",
    "                features = generate_listing_history_features(combined_history, window_sizes)\n",
    "                # Keep only test period rows\n",
    "                features = features[features['date'].isin(test_history['date'])]\n",
    "            else:\n",
    "                # No train history available\n",
    "                features = generate_listing_history_features(test_history, window_sizes)\n",
    "            \n",
    "            test_features.append(features)\n",
    "    \n",
    "    # Combine features\n",
    "    train_result = pd.concat(train_features, ignore_index=True) if train_features else pd.DataFrame()\n",
    "    test_result = pd.concat(test_features, ignore_index=True) if test_features else pd.DataFrame()\n",
    "    \n",
    "    return train_result, test_result\n",
    "\n",
    "def generate_listing_history_features(listing_data, window_sizes):\n",
    "    \"\"\"Helper function to generate features for a single listing\"\"\"\n",
    "    # Sort by date\n",
    "    listing_data = listing_data.sort_values('date')\n",
    "    \n",
    "    # Initialize list of created features and dataframe to store results\n",
    "    result = listing_data.copy()\n",
    "    created_features = []\n",
    "    \n",
    "    # Generate lag features\n",
    "    for window in window_sizes:\n",
    "        feature_name = f'price_lag_{window}d'\n",
    "        result[feature_name] = result['price'].shift(window)\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Generate rolling statistics\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        feature_name = f'rolling_mean_{window}d'\n",
    "        result[feature_name] = result['price'].rolling(window, min_periods=1).mean()\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling max\n",
    "        feature_name = f'rolling_max_{window}d'\n",
    "        result[feature_name] = result['price'].rolling(window, min_periods=1).max()\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling min\n",
    "        feature_name = f'rolling_min_{window}d'\n",
    "        result[feature_name] = result['price'].rolling(window, min_periods=1).min()\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling std\n",
    "        feature_name = f'rolling_std_{window}d'\n",
    "        result[feature_name] = result['price'].rolling(window, min_periods=1).std()\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Price volatility (max - min)\n",
    "        feature_name = f'price_range_{window}d'\n",
    "        result[feature_name] = result[f'rolling_max_{window}d'] - result[f'rolling_min_{window}d']\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for feature in created_features:\n",
    "        if result[feature].isnull().any():\n",
    "            # Fill with the first non-null value or median\n",
    "            result[feature] = result[feature].fillna(result[feature].median() if result[feature].notna().any() else 0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_price_history_features(train_data, test_data=None, use_train_for_test=True, window_sizes=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Generate price history features for training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data with 'listing_id', 'date', and 'price' columns\n",
    "    test_data : DataFrame, optional\n",
    "        Test data with 'listing_id', 'date', and 'price' columns\n",
    "    use_train_for_test : bool, optional\n",
    "        If True, use training data to compute price history features for test data\n",
    "    window_sizes : list, optional\n",
    "        List of window sizes (in days) for rolling statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_data_with_features, test_data_with_features) DataFrames with added price history features\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy() if test_data is not None else None\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(train_df['date']):\n",
    "        train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    \n",
    "    if test_df is not None and not pd.api.types.is_datetime64_any_dtype(test_df['date']):\n",
    "        test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    \n",
    "    # Sort by listing_id and date\n",
    "    train_df = train_df.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    # Prepare data for feature generation\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Create a combined dataframe for computing features\n",
    "        # We'll compute features on combined data but return them separately\n",
    "        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "        combined_df = combined_df.sort_values(['listing_id', 'date'])\n",
    "        data_for_features = combined_df\n",
    "    else:\n",
    "        data_for_features = train_df\n",
    "    \n",
    "    # Initialize list of created features\n",
    "    created_features = []\n",
    "    \n",
    "    # Generate lag features\n",
    "    for window in window_sizes:\n",
    "        feature_name = f'price_lag_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].shift(window)\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Generate rolling statistics\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        feature_name = f'rolling_mean_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling max\n",
    "        feature_name = f'rolling_max_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).max()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling min\n",
    "        feature_name = f'rolling_min_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).min()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Rolling std\n",
    "        feature_name = f'rolling_std_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features.groupby('listing_id')['price'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std()\n",
    "        )\n",
    "        created_features.append(feature_name)\n",
    "        \n",
    "        # Price volatility (max - min)\n",
    "        feature_name = f'price_range_{window}d'\n",
    "        data_for_features[feature_name] = data_for_features[f'rolling_max_{window}d'] - data_for_features[f'rolling_min_{window}d']\n",
    "        created_features.append(feature_name)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for feature in created_features:\n",
    "        if data_for_features[feature].isnull().any():\n",
    "            # Fill within each listing_id group\n",
    "            data_for_features[feature] = data_for_features.groupby('listing_id')[feature].transform(\n",
    "                lambda x: x.fillna(x.median() if x.notna().any() else 0)\n",
    "            )\n",
    "    \n",
    "    # Split combined data back to train and test if necessary\n",
    "    if test_df is not None and use_train_for_test:\n",
    "        # Get indices of train and test rows\n",
    "        train_indices = data_for_features.index[:len(train_df)]\n",
    "        test_indices = data_for_features.index[len(train_df):]\n",
    "        \n",
    "        # Extract features for train and test\n",
    "        train_df = data_for_features.loc[train_indices].copy()\n",
    "        test_df = data_for_features.loc[test_indices].copy()\n",
    "        \n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        if test_df is not None:\n",
    "            # Generate features for test separately\n",
    "            test_df = test_df.sort_values(['listing_id', 'date'])\n",
    "            for feature in created_features:\n",
    "                if feature in data_for_features.columns:\n",
    "                    # Just create empty columns that will be populated later\n",
    "                    test_df[feature] = np.nan\n",
    "            \n",
    "            return data_for_features, test_df\n",
    "        else:\n",
    "            return data_for_features, None\n",
    "\n",
    "class TemporalEnhancedGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,\n",
    "                 hidden_dim=64,\n",
    "                 temporal_model='lstm',  # Options: 'lstm', 'gru', 'none'\n",
    "                 sequence_length=7,      # For RNN/LSTM/GRU\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(TemporalEnhancedGNN, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        self.temporal_model = temporal_model\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Ensure output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        \n",
    "        # Spatial component: GAT layers\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Amenity processing\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Temporal processing - with recurrent layers\n",
    "        if temporal_model.lower() == 'lstm':\n",
    "            # LSTM model for temporal features\n",
    "            self.temporal_rnn = nn.LSTM(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        elif temporal_model.lower() == 'gru':\n",
    "            # GRU model for temporal features\n",
    "            self.temporal_rnn = nn.GRU(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Default to feed-forward layers\n",
    "            self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "            self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Price history processing - with recurrent layers\n",
    "        if temporal_model.lower() in ['lstm', 'gru']:\n",
    "            # Recurrent model for price history\n",
    "            rnn_class = nn.LSTM if temporal_model.lower() == 'lstm' else nn.GRU\n",
    "            self.price_history_rnn = rnn_class(\n",
    "                input_size=price_history_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Feed-forward layers for price history\n",
    "            self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "            self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def process_temporal_data(self, x, batch_size, model_type):\n",
    "        \"\"\"Process temporal data using the specified model type (LSTM, GRU, or feed-forward)\"\"\"\n",
    "        if model_type.lower() in ['lstm', 'gru']:\n",
    "            # Reshape for RNN: [batch_size, seq_len, features_per_step]\n",
    "            try:\n",
    "                features_per_step = x.shape[1] // self.sequence_length\n",
    "                x_reshaped = x.view(batch_size, self.sequence_length, features_per_step)\n",
    "                \n",
    "                # Process through RNN\n",
    "                if model_type.lower() == 'lstm':\n",
    "                    out, (h_n, c_n) = self.temporal_rnn(x_reshaped)\n",
    "                else:  # GRU\n",
    "                    out, h_n = self.temporal_rnn(x_reshaped)\n",
    "                \n",
    "                # Use the final hidden state (concat forward and backward for bidirectional)\n",
    "                output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Use last layer's hidden state\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error in RNN processing: {e}\")\n",
    "                # Fallback to feed-forward if reshape fails\n",
    "                return self.process_temporal_data(x, batch_size, 'none')\n",
    "        else:\n",
    "            # Standard feed-forward for 'none' model\n",
    "            if hasattr(self, 'temporal_layer1'):\n",
    "                out = F.elu(self.temporal_layer1(x))\n",
    "                out = self.temporal_bn1(out)\n",
    "                out = self.dropout(out)\n",
    "                out_res = out\n",
    "                out = F.elu(self.temporal_layer2(out))\n",
    "                out = self.temporal_bn2(out)\n",
    "                out = out + out_res  # Residual connection\n",
    "                return out\n",
    "            else:\n",
    "                # If temporal layers don't exist, create a dummy output of the right shape\n",
    "                return torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "    \n",
    "    def process_price_history(self, x, batch_size):\n",
    "        \"\"\"Process price history data using the configured model type\"\"\"\n",
    "        if self.temporal_model.lower() in ['lstm', 'gru']:\n",
    "            # Use recurrent model for price history\n",
    "            try:\n",
    "                features_per_step = x.shape[1] // self.sequence_length\n",
    "                x_reshaped = x.view(batch_size, self.sequence_length, features_per_step)\n",
    "                \n",
    "                # Process through RNN\n",
    "                if self.temporal_model.lower() == 'lstm':\n",
    "                    out, (h_n, c_n) = self.price_history_rnn(x_reshaped)\n",
    "                else:  # GRU\n",
    "                    out, h_n = self.price_history_rnn(x_reshaped)\n",
    "                \n",
    "                # Use the final hidden state (concat forward and backward for bidirectional)\n",
    "                output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Use last layer's hidden state\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error in price history RNN processing: {e}\")\n",
    "                # Fallback to feed-forward if reshape fails\n",
    "                if hasattr(self, 'price_history_layer1'):\n",
    "                    out = F.elu(self.price_history_layer1(x))\n",
    "                    out = self.price_history_bn1(out)\n",
    "                    out = self.dropout(out)\n",
    "                    out_res = out\n",
    "                    out = F.elu(self.price_history_layer2(out))\n",
    "                    out = self.price_history_bn2(out)\n",
    "                    out = out + out_res  # Residual connection\n",
    "                    return out\n",
    "                else:\n",
    "                    # If price history layers don't exist, create a dummy output\n",
    "                    return torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "        else:\n",
    "            # Feed-forward processing\n",
    "            out = F.elu(self.price_history_layer1(x))\n",
    "            out = self.price_history_bn1(out)\n",
    "            out = self.dropout(out)\n",
    "            out_res = out\n",
    "            out = F.elu(self.price_history_layer2(out))\n",
    "            out = self.price_history_bn2(out)\n",
    "            out = out + out_res  # Residual connection\n",
    "            return out\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # Get batch size for reshaping temporal data\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process spatial features with GAT\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer with residual connection\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process amenity features\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process temporal features with the selected model\n",
    "        temporal_features = self.process_temporal_data(temporal_x, batch_size, self.temporal_model)\n",
    "        \n",
    "        # Process price history features\n",
    "        price_history_features = self.process_price_history(price_history_x, batch_size)\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "class ListingLevelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(ListingLevelLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: [batch_size, seq_len, input_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, hidden)\n",
    "        \n",
    "        # Use last hidden state from both directions\n",
    "        last_hidden = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        \n",
    "        # Output price prediction\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "class ListingTemporalGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,\n",
    "                 hidden_dim=64,\n",
    "                 temporal_model='lstm',\n",
    "                 sequence_length=7,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(ListingTemporalGNN, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        self.temporal_model = temporal_model\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Ensure output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        \n",
    "        # Spatial component: GAT layers\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Amenity processing\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Temporal processing - with recurrent layers\n",
    "        if temporal_model.lower() == 'lstm':\n",
    "            # LSTM model for temporal features\n",
    "            self.temporal_rnn = nn.LSTM(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        elif temporal_model.lower() == 'gru':\n",
    "            # GRU model for temporal features\n",
    "            self.temporal_rnn = nn.GRU(\n",
    "                input_size=temporal_features_dim // sequence_length,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Default to feed-forward layers\n",
    "            self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "            self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Price history processing - with dedicated RNN\n",
    "        if temporal_model.lower() in ['lstm', 'gru']:\n",
    "            # Use listing-level RNN for price history\n",
    "            rnn_class = nn.LSTM if temporal_model.lower() == 'lstm' else nn.GRU\n",
    "            # Simpler RNN for listing histories - expects one feature per time step\n",
    "            self.price_history_rnn = rnn_class(\n",
    "                input_size=price_history_features_dim,\n",
    "                hidden_size=hidden_dim // 2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        else:\n",
    "            # Feed-forward layers for price history\n",
    "            self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "            self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def process_temporal_data(self, x, batch_size, model_type):\n",
    "        \"\"\"Process temporal data using the specified model type (LSTM, GRU, or feed-forward)\"\"\"\n",
    "        if model_type.lower() in ['lstm', 'gru']:\n",
    "            # Reshape for RNN: [batch_size, seq_len, features_per_step]\n",
    "            try:\n",
    "                features_per_step = x.shape[1] // self.sequence_length\n",
    "                x_reshaped = x.view(batch_size, self.sequence_length, features_per_step)\n",
    "                \n",
    "                # Process through RNN\n",
    "                if model_type.lower() == 'lstm':\n",
    "                    out, (h_n, c_n) = self.temporal_rnn(x_reshaped)\n",
    "                else:  # GRU\n",
    "                    out, h_n = self.temporal_rnn(x_reshaped)\n",
    "                \n",
    "                # Use the final hidden state (concat forward and backward for bidirectional)\n",
    "                output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Use last layer's hidden state\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error in RNN processing: {e}\")\n",
    "                # Fallback to feed-forward if reshape fails\n",
    "                return self.process_temporal_data(x, batch_size, 'none')\n",
    "        else:\n",
    "            # Standard feed-forward for 'none' model\n",
    "            if hasattr(self, 'temporal_layer1'):\n",
    "                out = F.elu(self.temporal_layer1(x))\n",
    "                out = self.temporal_bn1(out)\n",
    "                out = self.dropout(out)\n",
    "                out_res = out\n",
    "                out = F.elu(self.temporal_layer2(out))\n",
    "                out = self.temporal_bn2(out)\n",
    "                out = out + out_res  # Residual connection\n",
    "                return out\n",
    "            else:\n",
    "                # If temporal layers don't exist, create a dummy output of the right shape\n",
    "                return torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "    \n",
    "    def process_listing_histories(self, histories, listing_ids, batch_size):\n",
    "        \"\"\"Process price history using listing-level sequences\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # Prepare output tensor - one embedding per node in batch\n",
    "        history_embeddings = torch.zeros(batch_size, self.h_dim, device=device)\n",
    "        \n",
    "        # If not using temporal model or no histories available\n",
    "        if self.temporal_model.lower() == 'none' or not histories:\n",
    "            return history_embeddings\n",
    "        \n",
    "        # Extract sequences for listings in the current batch\n",
    "        batch_sequences = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, lid in enumerate(listing_ids):\n",
    "            if lid in histories:\n",
    "                batch_sequences.append(histories[lid])\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        # If no valid sequences in batch, return zero embeddings\n",
    "        if not batch_sequences:\n",
    "            return history_embeddings\n",
    "        \n",
    "        # Stack sequences for batch processing\n",
    "        stacked_sequences = torch.cat(batch_sequences, dim=0)\n",
    "        \n",
    "        # Process through RNN\n",
    "        if self.temporal_model.lower() == 'lstm':\n",
    "            _, (h_n, c_n) = self.price_history_rnn(stacked_sequences)\n",
    "        else:  # GRU\n",
    "            _, h_n = self.price_history_rnn(stacked_sequences)\n",
    "        \n",
    "        # Extract final embeddings (concatenate forward and backward for bidirectional)\n",
    "        embeddings = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        \n",
    "        # Place embeddings in the correct positions\n",
    "        for i, idx in enumerate(valid_indices):\n",
    "            history_embeddings[idx] = embeddings[i]\n",
    "        \n",
    "        return history_embeddings\n",
    "    \n",
    "    def forward(self, data, listing_histories=None):\n",
    "        # Unpack the PyG data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x, listing_ids = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x, data.listing_ids\n",
    "        )\n",
    "        \n",
    "        # Get batch size for reshaping temporal data\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process spatial features with GAT\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer with residual connection\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process amenity features\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process temporal features with the selected model\n",
    "        temporal_features = self.process_temporal_data(temporal_x, batch_size, self.temporal_model)\n",
    "        \n",
    "        # Process price history features - using listing level histories if available\n",
    "        if listing_histories is not None:\n",
    "            price_history_features = self.process_listing_histories(listing_histories, listing_ids, batch_size)\n",
    "        else:\n",
    "            # Fallback to old method\n",
    "            if hasattr(self, 'price_history_layer1'):\n",
    "                price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "                price_history_features = self.price_history_bn1(price_history_features)\n",
    "                price_history_features = self.dropout(price_history_features)\n",
    "                ph_res = price_history_features\n",
    "                price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "                price_history_features = self.price_history_bn2(price_history_features)\n",
    "                price_history_features = price_history_features + ph_res  # Residual connection\n",
    "            else:\n",
    "                # Dummy output if no price history layers\n",
    "                price_history_features = torch.zeros(batch_size, self.h_dim, device=x.device)\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "def prepare_enhanced_graph_for_temporal_listing_based(train_data, val_data, spatial_features, temporal_features, \n",
    "                                                    amenity_features, price_history_features, spatial_scaler, \n",
    "                                                    temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                                                    device, k=10, feature_weight=0.3, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings, with temporal features organized for RNN processing\n",
    "    using listing-based approach for efficiency\n",
    "    \"\"\"\n",
    "    # Scale features and convert to float32\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features].values).astype(np.float32)\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features].values).astype(np.float32)\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features].values).astype(np.float32)\n",
    "    X_train_price_history = price_history_scaler.transform(train_data[price_history_features].values).astype(np.float32)\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features].values).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features].values).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features].values).astype(np.float32)\n",
    "    X_val_price_history = price_history_scaler.transform(val_data[price_history_features].values).astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # We'll handle price history differently - at the listing level\n",
    "    # Create listing-level history sequences\n",
    "    train_histories = create_listing_histories(train_data, sequence_length=sequence_length)\n",
    "    val_histories = create_listing_histories(val_data, sequence_length=sequence_length)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Extract listing IDs in the same order as the features\n",
    "    combined_listing_ids = np.concatenate([\n",
    "        train_data['listing_id'].values,\n",
    "        val_data['listing_id'].values\n",
    "    ])\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "        listing_ids=combined_listing_ids  # Add listing IDs to access histories\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_histories = {**train_histories, **val_histories}\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    print(f\"Created history sequences for {len(combined_histories)} listings\")\n",
    "    \n",
    "    return data, combined_histories\n",
    "\n",
    "def train_gnn_model_listing_based(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                                 price_history_features, hidden_dim=64, epochs=50, lr=0.001, device='cuda', \n",
    "                                 batch_size=64, temporal_model='lstm', sequence_length=7, use_temporal=True):\n",
    "    \"\"\"\n",
    "    Train GNN model with enhanced temporal modeling using listing-level history processing\n",
    "    \"\"\"\n",
    "    # Override temporal_model if use_temporal is False\n",
    "    if not use_temporal:\n",
    "        temporal_model = 'none'\n",
    "        print(\"Temporal modeling disabled, using feed-forward layers\")\n",
    "    else:\n",
    "        print(f\"Using {temporal_model.upper()} for temporal modeling with sequence length {sequence_length}\")\n",
    "    \n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data with listing-level history processing\n",
    "    graph_data, listing_histories = prepare_enhanced_graph_for_temporal_listing_based(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # Initialize model - using the new ListingTemporalGNN\n",
    "    model = ListingTemporalGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=graph_data.temporal_x.shape[1],\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=len(price_history_features),\n",
    "        hidden_dim=hidden_dim,\n",
    "        temporal_model=temporal_model,\n",
    "        sequence_length=sequence_length,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with listing histories\n",
    "        out = model(graph_data, listing_histories)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass with listing histories\n",
    "            val_out = model(graph_data, listing_histories)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, listing_histories\n",
    "\n",
    "def run_gnn_with_rolling_window_cv_listing_based(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                                window_size=35, n_splits=5, sample_size=None, \n",
    "                                                use_temporal=True, temporal_model='lstm', sequence_length=7,\n",
    "                                                use_price_history=True, use_train_for_test_history=True):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation and enhanced temporal modeling using listing-based history\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    \n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Generate price history features if enabled - using the new listing-based approach\n",
    "    if use_price_history:\n",
    "        print(\"Generating price history features by listing...\")\n",
    "        train_data, _ = generate_price_history_features_by_listing(train_data, window_sizes=[7, 14, 30])\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups - just like in your original code\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    additional_temporal = [\n",
    "        'DTF_day', 'DTF_is_holiday', 'DTF_days_to_weekend', \n",
    "        'DTF_days_since_start', 'DTF_days_to_end_month'\n",
    "    ]\n",
    "    \n",
    "    for feat in additional_temporal:\n",
    "        if feat in train_data.columns:\n",
    "            temporal_features.append(feat)\n",
    "    \n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    price_history_features = []\n",
    "    if use_price_history:\n",
    "        price_history_features = [\n",
    "            'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "            'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "            'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "            'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "            'price_range_7d', 'price_range_14d', 'price_range_30d',\n",
    "            'rolling_std_7d', 'rolling_std_14d', 'rolling_std_30d'\n",
    "        ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_data.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_data['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features and use_price_history:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    elif not use_price_history:\n",
    "        # Create a dummy feature when price history is disabled\n",
    "        train_data['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Get unique dates and create test periods\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    last_35_days = unique_dates[-window_size:]\n",
    "    \n",
    "    # Define explicit test periods\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * (window_size // n_splits)\n",
    "        end_idx = start_idx + (window_size // n_splits)\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print temporal modeling configuration\n",
    "    print(f\"Temporal modeling: {'Enabled - '+temporal_model.upper() if use_temporal else 'Disabled'}\")\n",
    "    if use_temporal:\n",
    "        print(f\"Sequence length: {sequence_length}\")\n",
    "    print(f\"Price history features: {'Enabled' if use_price_history else 'Disabled'}\")\n",
    "    if use_price_history:\n",
    "        print(f\"Using training data for test history: {'Yes' if use_train_for_test_history else 'No'}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train GNN model using the listing-based approach\n",
    "        try:\n",
    "            print(f\"\\n----- Training GNN Model with Listing-Based History (Split {i+1}) -----\")\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, listing_histories = train_gnn_model_listing_based(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=50, lr=0.001, device=device,\n",
    "                temporal_model=temporal_model, sequence_length=sequence_length, use_temporal=use_temporal\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_gnn_listing_based(\n",
    "                gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                price_history_scaler, target_scaler, train_subset, device,\n",
    "                temporal_model=temporal_model, sequence_length=sequence_length,\n",
    "                listing_histories=listing_histories\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                torch.save(gnn_model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids),\n",
    "        'config': {\n",
    "            'use_temporal': use_temporal,\n",
    "            'temporal_model': temporal_model,\n",
    "            'sequence_length': sequence_length,\n",
    "            'use_price_history': use_price_history,\n",
    "            'use_train_for_test_history': use_train_for_test_history,\n",
    "            'listing_based_history': True  # Flag that we're using the new approach\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save results\n",
    "        results_file = os.path.join(output_dir, 'gnn_listing_based_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'gnn_listing_based_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config_file = os.path.join(output_dir, 'listing_based_config.json')\n",
    "        import json\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(evaluation_results['config'], f, indent=4)\n",
    "        print(f\"Configuration saved to {config_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'gnn_listing_based_summary.txt'), 'w') as f:\n",
    "            f.write(f\"GNN Listing-Based History Model Summary\\n\")\n",
    "            f.write(f\"=====================================\\n\\n\")\n",
    "            f.write(f\"Window size: {window_size} days\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Temporal modeling: {'Enabled - '+temporal_model.upper() if use_temporal else 'Disabled'}\\n\")\n",
    "            if use_temporal:\n",
    "                f.write(f\"Sequence length: {sequence_length}\\n\")\n",
    "            f.write(f\"Price history features: {'Enabled' if use_price_history else 'Disabled'}\\n\")\n",
    "            if use_price_history:\n",
    "                f.write(f\"Using training data for test history: {'Yes' if use_train_for_test_history else 'No'}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics:\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== GNN LISTING-BASED HISTORY MODEL SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations if plotting function available\n",
    "    if 'plot_gnn_rolling_window_results' in globals():\n",
    "        plot_gnn_rolling_window_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def predict_with_gnn_listing_based(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                                  price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                                  price_history_scaler, target_scaler, train_data, device,\n",
    "                                  temporal_model='lstm', sequence_length=7, listing_histories=None):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained temporal GNN model with listing-based history\n",
    "    \"\"\"\n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data, _ = prepare_enhanced_graph_for_temporal_listing_based(\n",
    "        train_data, test_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # Create test listing histories if not provided\n",
    "    if listing_histories is None:\n",
    "        listing_histories = create_listing_histories(\n",
    "            train_data, test_data, sequence_length=sequence_length\n",
    "        )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data, listing_histories)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "\n",
    "def prepare_listing_sequences(data, listing_ids, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Create sequential data specifically for LSTM/GRU at the listing level\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : DataFrame\n",
    "        Data with 'listing_id', 'date', and price features\n",
    "    listing_ids : list\n",
    "        List of listing IDs to process\n",
    "    sequence_length : int\n",
    "        Length of sequences to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with listing_id as key and sequence tensor as value\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    \n",
    "    for listing_id in listing_ids:\n",
    "        # Get data for this listing\n",
    "        listing_data = data[data['listing_id'] == listing_id].sort_values('date')\n",
    "        \n",
    "        if len(listing_data) < 2:  # Skip listings with insufficient data\n",
    "            continue\n",
    "            \n",
    "        # Extract features (adjust column names as needed)\n",
    "        price_history_cols = [col for col in listing_data.columns if col.startswith(('price_lag', 'rolling'))]\n",
    "        features = listing_data[price_history_cols].values\n",
    "        \n",
    "        # Create sequences\n",
    "        X_sequences = []\n",
    "        for i in range(len(listing_data) - sequence_length + 1):\n",
    "            X_sequences.append(features[i:i+sequence_length])\n",
    "        \n",
    "        if not X_sequences:  # Skip if no sequences could be created\n",
    "            continue\n",
    "            \n",
    "        # Convert to tensor\n",
    "        sequences[listing_id] = torch.FloatTensor(X_sequences)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def prepare_efficient_graph_data(train_data, test_data, features_config, device):\n",
    "    \"\"\"Process listings in a memory-efficient way\"\"\"\n",
    "    \n",
    "    # Extract unique listings\n",
    "    train_listings = train_data['listing_id'].unique()\n",
    "    test_listings = test_data['listing_id'].unique()\n",
    "    \n",
    "    print(f\"Processing {len(train_listings)} train listings and {len(test_listings)} test listings\")\n",
    "    \n",
    "    # Process history features by listing (only once)\n",
    "    listing_histories = {}\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(train_listings), batch_size):\n",
    "        batch_ids = train_listings[i:i+batch_size]\n",
    "        batch_data = train_data[train_data['listing_id'].isin(batch_ids)]\n",
    "        \n",
    "        for listing_id in batch_ids:\n",
    "            listing_history = batch_data[batch_data['listing_id'] == listing_id].sort_values('date')\n",
    "            if len(listing_history) > 0:\n",
    "                listing_histories[listing_id] = process_listing_history(listing_history)\n",
    "    \n",
    "    # Same for test listings\n",
    "    for i in range(0, len(test_listings), batch_size):\n",
    "        batch_ids = test_listings[i:i+batch_size]\n",
    "        batch_data = test_data[test_data['listing_id'].isin(batch_ids)]\n",
    "        \n",
    "        for listing_id in batch_ids:\n",
    "            listing_history = batch_data[batch_data['listing_id'] == listing_id].sort_values('date')\n",
    "            if len(listing_history) > 0:\n",
    "                listing_histories[listing_id] = process_listing_history(listing_history)\n",
    "    \n",
    "    # Now create graph structure only with actual nodes and edges needed\n",
    "    # (implementation details depend on your specific needs)\n",
    "    \n",
    "    return graph_data, listing_histories\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"path/to/your/train_data.csv\" \n",
    "    train_ids_path = r\"path/to/your/train_ids.txt\"\n",
    "    test_ids_path = r\"path/to/your/test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/listing_level_temporal_gnn\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Configuration options\n",
    "    config = {\n",
    "        'use_rolling_window': True,\n",
    "        'use_temporal': True,\n",
    "        'temporal_model': 'lstm',\n",
    "        'sequence_length': 7,\n",
    "        'use_price_history': True,\n",
    "        'use_train_for_test_history': True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run with the new listing-based approach\n",
    "        results = run_gnn_with_rolling_window_cv_listing_based(\n",
    "            train_path=train_path,\n",
    "            train_ids_path=train_ids_path,\n",
    "            test_ids_path=test_ids_path,\n",
    "            output_dir=output_dir,\n",
    "            window_size=35,\n",
    "            n_splits=5,\n",
    "            sample_size=None,  # Use full dataset, or set to a smaller number for testing\n",
    "            use_temporal=config['use_temporal'],\n",
    "            temporal_model=config['temporal_model'],\n",
    "            sequence_length=config['sequence_length'],\n",
    "            use_price_history=config['use_price_history'],\n",
    "            use_train_for_test_history=config['use_train_for_test_history']\n",
    "        )\n",
    "        print(f\"Listing-level GNN model training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running listing-level temporal GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
