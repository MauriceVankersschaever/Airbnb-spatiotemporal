{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----- Core Functions from paste2 (Fast Implementation) -----\n",
    "\n",
    "# 1. Price transformation function (from paste2)\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Create calculated features (from paste2)\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 3. Build enhanced spatial graph for GNN (from paste2, but with k=5)\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=5, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "# 8. Function to evaluate predictions (from paste2)\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ----- Components from paste1 (Transformer-based architecture) -----\n",
    "\n",
    "# TimeSeriesEncoder from paste1 (simplified version without sub-batching)\n",
    "class TimeSeriesEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=2, num_layers=1, dropout=0.1):\n",
    "        super(TimeSeriesEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim*2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if x.shape[0] == 0:  # Handle empty batch case\n",
    "            return torch.zeros((0, self.embedding.out_features), device=x.device)\n",
    "            \n",
    "        # x shape: [batch_size, seq_len, features]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Process entire batch at once\n",
    "        return self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "# CrossAttention from paste1 (simplified version without sub-batching)\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, spatial_dim, temporal_dim, heads=2, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        # Project to same dimension\n",
    "        self.project_spatial = nn.Linear(spatial_dim, spatial_dim) if spatial_dim != temporal_dim else nn.Identity()\n",
    "        self.project_temporal = nn.Linear(temporal_dim, spatial_dim) if temporal_dim != spatial_dim else nn.Identity()\n",
    "        \n",
    "        # Multi-head attention with fewer heads\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=spatial_dim,\n",
    "            num_heads=heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization and feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(spatial_dim)\n",
    "        self.norm2 = nn.LayerNorm(spatial_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, spatial_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(spatial_dim * 2, spatial_dim)\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(spatial_dim + temporal_dim, spatial_dim * 2)\n",
    "\n",
    "    def forward(self, spatial_features, temporal_context):\n",
    "        # Check for empty inputs\n",
    "        if spatial_features.shape[0] == 0:\n",
    "            output_dim = self.output_proj.out_features\n",
    "            return torch.zeros((0, output_dim), device=spatial_features.device)\n",
    "        \n",
    "        # Project features if needed\n",
    "        spatial_proj = self.project_spatial(spatial_features)\n",
    "        temporal_proj = self.project_temporal(temporal_context)\n",
    "        \n",
    "        # Reshape temporal context if needed\n",
    "        if temporal_proj.dim() == 2:\n",
    "            # Add sequence dimension (treat as single token)\n",
    "            temporal_proj = temporal_proj.unsqueeze(1)\n",
    "        \n",
    "        # Multi-head attention - spatial as query, temporal as key/value\n",
    "        spatial_proj_seq = spatial_proj.unsqueeze(1) if spatial_proj.dim() == 2 else spatial_proj\n",
    "        \n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            query=spatial_proj_seq,\n",
    "            key=temporal_proj,\n",
    "            value=temporal_proj\n",
    "        )\n",
    "        \n",
    "        # Remove sequence dimension if needed\n",
    "        if spatial_features.dim() == 2:\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        spatial_features = self.norm1(spatial_proj + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN\n",
    "        ffn_output = self.ffn(spatial_features)\n",
    "        \n",
    "        # Second residual connection\n",
    "        spatial_features = self.norm2(spatial_features + self.dropout(ffn_output))\n",
    "        \n",
    "        # Concatenate with temporal\n",
    "        final_output = torch.cat([\n",
    "            spatial_features, \n",
    "            temporal_context.squeeze(1) if temporal_context.dim() == 3 else temporal_context\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Apply final projection\n",
    "        return self.output_proj(final_output)\n",
    "\n",
    "# ListingGNN (hybrid of paste1 architecture with paste2 efficiency)\n",
    "class ListingGNN(nn.Module):\n",
    "    def __init__(self, spatial_dim, temporal_dim, price_history_dim, hidden_dim=64, heads=4):\n",
    "        super(ListingGNN, self).__init__()\n",
    "        \n",
    "        # Spatial component with GATv2Conv (from paste2)\n",
    "        self.gat = GATv2Conv(spatial_dim, hidden_dim // heads, heads=heads, edge_dim=1)\n",
    "        \n",
    "        # Temporal encoder (transformer-based from paste1)\n",
    "        self.time_series_encoder = TimeSeriesEncoder(\n",
    "            input_dim=price_history_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=heads,\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        # Calculate proper dimensions\n",
    "        gat_output_dim = hidden_dim  # hidden_dim // heads * heads = hidden_dim\n",
    "        \n",
    "        # CrossAttention between spatial and temporal (from paste1)\n",
    "        self.cross_attention = CrossAttention(gat_output_dim, hidden_dim, heads=heads)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(gat_output_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Process spatial data with GAT\n",
    "        spatial_features = self.gat(data.x, data.edge_index, edge_attr=data.edge_attr)\n",
    "        \n",
    "        # Process temporal data with transformer\n",
    "        if hasattr(data, 'price_history'):\n",
    "            temporal_features = self.time_series_encoder(\n",
    "                data.price_history, \n",
    "                src_key_padding_mask=data.price_history_mask if hasattr(data, 'price_history_mask') else None\n",
    "            )\n",
    "            \n",
    "            # Get the last non-masked timestep or just the last one\n",
    "            if hasattr(data, 'price_history_mask') and data.price_history_mask is not None:\n",
    "                # Find the last non-masked position\n",
    "                non_mask_indices = (~data.price_history_mask).sum(dim=1) - 1\n",
    "                non_mask_indices = torch.clamp(non_mask_indices, min=0)\n",
    "                \n",
    "                # Get embeddings at these positions\n",
    "                batch_indices = torch.arange(data.x.shape[0], device=data.x.device)\n",
    "                temporal_context = temporal_features[batch_indices, non_mask_indices]\n",
    "            else:\n",
    "                # Use the last timestep\n",
    "                temporal_context = temporal_features[:, -1]\n",
    "        else:\n",
    "            # Create dummy temporal context if price_history is missing\n",
    "            temporal_context = torch.zeros((data.x.shape[0], self.time_series_encoder.embedding.out_features), \n",
    "                                      device=data.x.device)\n",
    "        \n",
    "        # Cross-attention between spatial and temporal\n",
    "        fused_features = self.cross_attention(spatial_features, temporal_context)\n",
    "        \n",
    "        # Prediction\n",
    "        return self.output_mlp(fused_features)\n",
    "\n",
    "# ----- Feature Extraction and Data Preparation (optimized) -----\n",
    "\n",
    "# Extract basic features (modified from paste1)\n",
    "def extract_basic_features(data, feature_groups):\n",
    "    \"\"\"Extract and combine basic features\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Spatial features (must include latitude/longitude)\n",
    "    if 'spatial' in feature_groups:\n",
    "        spatial_features = data[feature_groups['spatial']].copy()\n",
    "        features.append(spatial_features)\n",
    "    \n",
    "    # Property features\n",
    "    if 'property' in feature_groups:\n",
    "        property_features = data[feature_groups['property']].copy()\n",
    "        features.append(property_features)\n",
    "    \n",
    "    # Convert to numpy and fill NaN\n",
    "    result = pd.concat(features, axis=1).values\n",
    "    result = np.nan_to_num(result, 0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create simple price histories\n",
    "def create_simple_price_histories(data, sequence_length=10):\n",
    "    \"\"\"Create price histories for each listing\"\"\"\n",
    "    price_histories = {}\n",
    "    \n",
    "    # Group by listing_id\n",
    "    for listing_id, group in data.groupby('listing_id'):\n",
    "        if len(group) >= 2:  # Need at least 2 rows to have a history\n",
    "            # Sort by date\n",
    "            sorted_group = group.sort_values('date')\n",
    "            \n",
    "            # Get the price values\n",
    "            prices = sorted_group['price'].values\n",
    "            \n",
    "            # Create features: [price(t), price(t)-price(t-1), rolling_mean if available]\n",
    "            features = []\n",
    "            features.append(prices.reshape(-1, 1))  # Current price\n",
    "            \n",
    "            # Price diff\n",
    "            diff = np.diff(prices, prepend=prices[0]).reshape(-1, 1)\n",
    "            features.append(diff)\n",
    "            \n",
    "            # Add rolling means if available\n",
    "            for window in [7, 14, 30]:\n",
    "                col = f'rolling_mean_{window}d'\n",
    "                if col in sorted_group.columns:\n",
    "                    rolling_mean = sorted_group[col].values.reshape(-1, 1)\n",
    "                    features.append(rolling_mean)\n",
    "            \n",
    "            # Combine features\n",
    "            feature_array = np.hstack(features)\n",
    "            \n",
    "            # Create history with correct sequence length\n",
    "            if len(feature_array) <= sequence_length:\n",
    "                # Pad with zeros at the beginning\n",
    "                padded = np.zeros((sequence_length, feature_array.shape[1]))\n",
    "                padded[-len(feature_array):] = feature_array\n",
    "                price_histories[listing_id] = torch.FloatTensor(padded)\n",
    "            else:\n",
    "                # Take the last sequence_length entries\n",
    "                price_histories[listing_id] = torch.FloatTensor(feature_array[-sequence_length:])\n",
    "        else:\n",
    "            # Create empty history for listings with insufficient data\n",
    "            price_histories[listing_id] = None\n",
    "    \n",
    "    return price_histories\n",
    "\n",
    "# Extract temporal features\n",
    "def extract_temporal_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract temporal features\"\"\"\n",
    "    if 'temporal' not in feature_groups or not feature_groups['temporal']:\n",
    "        # Return dummy tensor if no temporal features\n",
    "        return torch.zeros((len(train_data) + len(test_data), 1), device=device)\n",
    "    \n",
    "    # Extract temporal features from both datasets\n",
    "    train_temporal = train_data[feature_groups['temporal']].copy().values\n",
    "    test_temporal = test_data[feature_groups['temporal']].copy().values\n",
    "    \n",
    "    # Combine and convert to tensor\n",
    "    combined = np.vstack([train_temporal, test_temporal])\n",
    "    combined = np.nan_to_num(combined, 0)\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# Extract amenity features\n",
    "def extract_amenity_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract amenity features\"\"\"\n",
    "    if 'amenity' not in feature_groups or not feature_groups['amenity']:\n",
    "        # Return dummy tensor if no amenity features\n",
    "        return torch.zeros((len(train_data) + len(test_data), 1), device=device)\n",
    "    \n",
    "    # Extract amenity features from both datasets\n",
    "    train_amenity = train_data[feature_groups['amenity']].copy().values\n",
    "    test_amenity = test_data[feature_groups['amenity']].copy().values\n",
    "    \n",
    "    # Combine and convert to tensor\n",
    "    combined = np.vstack([train_amenity, test_amenity])\n",
    "    combined = np.nan_to_num(combined, 0)\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# Extract price history features\n",
    "def extract_price_history_features(train_data, test_data, feature_groups, device):\n",
    "    \"\"\"Extract price history features\"\"\"\n",
    "    # Define price history features if not in feature_groups\n",
    "    if 'price_history' not in feature_groups or not feature_groups['price_history']:\n",
    "        price_history_features = [\n",
    "            'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "            'rolling_mean_7d', 'rolling_mean_14d', 'rolling_mean_30d'\n",
    "        ]\n",
    "        available_features = [f for f in price_history_features if f in train_data.columns]\n",
    "    else:\n",
    "        available_features = feature_groups['price_history']\n",
    "    \n",
    "    if not available_features:\n",
    "        # Return dummy tensor if no price history features\n",
    "        return torch.zeros((len(train_data) + len(test_data), 1), device=device)\n",
    "    \n",
    "    # Extract price history features from both datasets\n",
    "    train_price_history = train_data[available_features].copy().fillna(0).values\n",
    "    test_price_history = test_data[available_features].copy().fillna(0).values\n",
    "    \n",
    "    # Combine and convert to tensor\n",
    "    combined = np.vstack([train_price_history, test_price_history])\n",
    "    \n",
    "    return torch.FloatTensor(combined).to(device)\n",
    "\n",
    "# Prepare graph data (hybrid of paste1 and paste2)\n",
    "def prepare_graph_data(train_data, test_data, feature_groups, device, sequence_length=10):\n",
    "    \"\"\"Prepare graph data combining approaches from paste1 and paste2\"\"\"\n",
    "    print(\"Preparing graph data...\")\n",
    "    \n",
    "    # Create scalers for features\n",
    "    scalers = {}\n",
    "    \n",
    "    # Scale the target (price)\n",
    "    target_scaler = StandardScaler()\n",
    "    train_data['price'] = target_scaler.fit_transform(train_data[['price']])\n",
    "    if 'price' in test_data.columns:\n",
    "        test_data['price'] = target_scaler.transform(test_data[['price']])\n",
    "    scalers['target'] = target_scaler\n",
    "    \n",
    "    # Process basic features\n",
    "    spatial_features = feature_groups.get('spatial', [])\n",
    "    property_features = feature_groups.get('property', [])\n",
    "    \n",
    "    # Combine spatial and property features for the main node features\n",
    "    basic_features = spatial_features + property_features\n",
    "    \n",
    "    if not basic_features:\n",
    "        raise ValueError(\"No spatial or property features provided!\")\n",
    "    \n",
    "    train_features = train_data[basic_features].copy().fillna(0).values\n",
    "    test_features = test_data[basic_features].copy().fillna(0).values\n",
    "    \n",
    "    # Create price histories\n",
    "    print(\"Creating price histories...\")\n",
    "    train_histories = create_simple_price_histories(train_data, sequence_length)\n",
    "    test_histories = create_simple_price_histories(test_data, sequence_length)\n",
    "    \n",
    "    # Determine dimensions from sample history\n",
    "    sample_history = None\n",
    "    for hist in train_histories.values():\n",
    "        if hist is not None and hist.shape[0] > 0:\n",
    "            sample_history = hist\n",
    "            break\n",
    "            \n",
    "    if sample_history is None:\n",
    "        for hist in test_histories.values():\n",
    "            if hist is not None and hist.shape[0] > 0:\n",
    "                sample_history = hist\n",
    "                break\n",
    "    \n",
    "    # Prepare batch price history tensor\n",
    "    all_listing_ids = np.concatenate([\n",
    "        train_data['listing_id'].values,\n",
    "        test_data['listing_id'].values\n",
    "    ])\n",
    "    \n",
    "    if sample_history is not None:\n",
    "        # Determine dimensions from sample\n",
    "        seq_len, feature_dim = sample_history.shape\n",
    "        batch_size = len(all_listing_ids)\n",
    "        \n",
    "        # Create batch tensor\n",
    "        batch_histories = torch.zeros((batch_size, seq_len, feature_dim))\n",
    "        \n",
    "        # Combine histories\n",
    "        all_histories = {**train_histories, **test_histories}\n",
    "        \n",
    "        # Fill batch tensor\n",
    "        for i, lid in enumerate(all_listing_ids):\n",
    "            if lid in all_histories and all_histories[lid] is not None:\n",
    "                history = all_histories[lid]\n",
    "                if history.shape[0] > 0:\n",
    "                    # Make sure dimensions match\n",
    "                    actual_seq_len = min(history.shape[0], seq_len)\n",
    "                    batch_histories[i, :actual_seq_len, :] = history[:actual_seq_len]\n",
    "    else:\n",
    "        # Fallback if no valid histories found\n",
    "        batch_histories = torch.zeros((len(all_listing_ids), 1, 1))\n",
    "    \n",
    "    # Build graph structure\n",
    "    print(\"Building spatial graph...\")\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[spatial_features], test_data[spatial_features], k=5\n",
    "    )\n",
    "    \n",
    "    # Create masks and combine data\n",
    "    train_mask = torch.zeros(len(train_features) + len(test_features), dtype=torch.bool)\n",
    "    train_mask[:len(train_features)] = True\n",
    "    \n",
    "    val_mask = torch.zeros(len(train_features) + len(test_features), dtype=torch.bool)\n",
    "    val_mask[len(train_features):] = True\n",
    "    \n",
    "    # Get target values\n",
    "    train_y = train_data['price'].values\n",
    "    test_y = test_data['price'].values if 'price' in test_data.columns else np.zeros(len(test_features))\n",
    "    all_y = np.concatenate([train_y, test_y])\n",
    "    \n",
    "    # Combine features\n",
    "    all_features = np.vstack([train_features, test_features])\n",
    "    \n",
    "    # Create PyG Data object and move everything to device\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(all_features).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        y=torch.FloatTensor(all_y.reshape(-1, 1)).to(device),\n",
    "        train_mask=train_mask.to(device),\n",
    "        val_mask=val_mask.to(device),\n",
    "        listing_ids=torch.LongTensor(all_listing_ids).to(device),\n",
    "        price_history=batch_histories.to(device),\n",
    "        price_history_mask=(batch_histories.sum(dim=-1) == 0).to(device)\n",
    "    )\n",
    "    \n",
    "    # Extract and add other feature types\n",
    "    data.temporal_x = extract_temporal_features(train_data, test_data, feature_groups, device)\n",
    "    data.amenity_x = extract_amenity_features(train_data, test_data, feature_groups, device)\n",
    "    data.price_history_x = extract_price_history_features(train_data, test_data, feature_groups, device)\n",
    "    \n",
    "    return data, scalers\n",
    "\n",
    "# ----- Optimized Training Function -----\n",
    "\n",
    "def train_model(train_data, val_data, feature_groups, device='cuda', hidden_dim=64, \n",
    "               epochs=50, lr=0.001, sequence_length=10):\n",
    "    \"\"\"Train the optimized hybrid GNN model\"\"\"\n",
    "    # Prepare data\n",
    "    graph_data, scalers = prepare_graph_data(\n",
    "        train_data, val_data, feature_groups, device, sequence_length\n",
    "    )\n",
    "    \n",
    "    # Get input dimensions\n",
    "    spatial_dim = graph_data.x.shape[1]\n",
    "    price_history_dim = graph_data.price_history.shape[2] if hasattr(graph_data, 'price_history') else 1\n",
    "    \n",
    "    print(f\"Input dimensions - Spatial: {spatial_dim}, Price history: {price_history_dim}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ListingGNN(\n",
    "        spatial_dim=spatial_dim,\n",
    "        temporal_dim=hidden_dim,\n",
    "        price_history_dim=price_history_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            # Process entire graph at once\n",
    "            train_out = model(graph_data)[graph_data.train_mask]\n",
    "            train_y = graph_data.y[graph_data.train_mask]\n",
    "            loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                val_out = model(graph_data)[graph_data.val_mask]\n",
    "                val_y = graph_data.y[graph_data.val_mask]\n",
    "                val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions to original scale for metrics\n",
    "            val_pred_orig = np.expm1(scalers['target'].inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(scalers['target'].inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, scalers\n",
    "\n",
    "# Prediction function\n",
    "def predict_with_model(model, test_data, train_data, feature_groups, scalers, device, sequence_length=10):\n",
    "    \"\"\"Make predictions with the trained hybrid model\"\"\"\n",
    "    # Prepare graph data\n",
    "    graph_data, _ = prepare_graph_data(\n",
    "        train_data, test_data, feature_groups, device, sequence_length\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = scalers['target'].inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "    \n",
    "    return predictions_orig\n",
    "\n",
    "\n",
    "# Main function to run the model with rolling window CV\n",
    "def run_hybrid_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                       window_size=35, n_splits=5, sample_size=None, sequence_length=10,\n",
    "                                       hidden_dim=64):\n",
    "    \"\"\"\n",
    "    Run hybrid GNN model with rolling window cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    sequence_length : int, optional\n",
    "        Number of previous time steps to use in price history\n",
    "    hidden_dim : int, optional\n",
    "        Hidden dimension size for neural network\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups using a dictionary structure (for flexibility)\n",
    "    feature_groups = {\n",
    "        'spatial': ['latitude', 'longitude'],\n",
    "        'property': ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio'],\n",
    "        'amenity': [col for col in train_data.columns if col.startswith('has_')],\n",
    "        'temporal': [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ],\n",
    "        'price_history': [\n",
    "            'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "            'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "            'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "            'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "            'price_range_7d', 'price_range_14d', 'price_range_30d'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add additional temporal features if available\n",
    "    additional_temporal = ['DTF_day', 'DTF_is_holiday', 'DTF_days_to_weekend', \n",
    "                         'DTF_days_since_start', 'DTF_days_to_end_month']\n",
    "    for feat in additional_temporal:\n",
    "        if feat in train_data.columns:\n",
    "            feature_groups['temporal'].append(feat)\n",
    "    \n",
    "    # Ensure all feature groups only contain columns that exist in the dataset\n",
    "    for group in feature_groups:\n",
    "        feature_groups[group] = [f for f in feature_groups[group] if f in train_data.columns]\n",
    "    \n",
    "    # Get unique dates and create test periods\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    last_35_days = unique_dates[-window_size:]\n",
    "    \n",
    "    # Define explicit test periods\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * (window_size // n_splits)\n",
    "        end_idx = start_idx + (window_size // n_splits)\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print model configuration\n",
    "    print(f\"Using hybrid GNN with transformer-based temporal processing\")\n",
    "    print(f\"Sequence length: {sequence_length}, Hidden dim: {hidden_dim}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train hybrid GNN model\n",
    "        try:\n",
    "            print(f\"\\n----- Training Hybrid GNN Model (Split {i+1}) -----\")\n",
    "            \n",
    "            # Clear GPU memory before training\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train the model\n",
    "            model, scalers = train_model(\n",
    "                train_subset, val_subset, feature_groups, \n",
    "                device=device, \n",
    "                hidden_dim=hidden_dim, \n",
    "                epochs=30,  # Reduced from 50 \n",
    "                lr=0.001, \n",
    "                sequence_length=sequence_length\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating Hybrid GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_model(\n",
    "                model, split_test_data, train_subset, feature_groups, scalers,\n",
    "                device, sequence_length=sequence_length\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'hybrid_gnn_model_split_{i+1}.pt')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "            # Clear memory after each split\n",
    "            del model, scalers\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Process results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids),\n",
    "        'config': {\n",
    "            'model_type': 'hybrid_transformer_gnn',\n",
    "            'sequence_length': sequence_length,\n",
    "            'hidden_dim': hidden_dim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== HYBRID TRANSFORMER-GNN MODEL SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'hybrid_gnn_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'hybrid_gnn_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Plot and save visualizations\n",
    "        plot_gnn_rolling_window_results(evaluation_results, output_dir)\n",
    "        plot_listing_predictions(all_results, num_listings=10, output_dir=output_dir)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Function to plot rolling window results\n",
    "def plot_gnn_rolling_window_results(evaluation_results, output_dir=None):\n",
    "    \"\"\"Plot the results from GNN rolling window cross-validation\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Add title\n",
    "    fig.suptitle('Hybrid GNN Model Evaluation with Rolling Window CV', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Cross-validation splits performance\n",
    "    splits_x = splits['split']\n",
    "    metrics_to_plot = ['rmse', 'mae']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        sns.lineplot(\n",
    "            x=splits_x,\n",
    "            y=splits[metric],\n",
    "            marker='o',\n",
    "            label=metric.upper(),\n",
    "            ax=axes[0, 1]\n",
    "        )\n",
    "    \n",
    "    axes[0, 1].set_title('Performance Across CV Splits')\n",
    "    axes[0, 1].set_xlabel('CV Split')\n",
    "    axes[0, 1].set_ylabel('Error Metric')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted (colored by split)\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['price'],\n",
    "        all_results['predicted'],\n",
    "        c=all_results['split'],\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    min_val = min(all_results['price'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['price'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by CV Split)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('CV Split')\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the suptitle\n",
    "    \n",
    "    # Save if output directory provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'hybrid_gnn_results.png'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Function to plot individual listing predictions\n",
    "def plot_listing_predictions(all_results, num_listings=10, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted prices for a sample of individual listings\n",
    "    \"\"\"\n",
    "    # Get unique listing IDs\n",
    "    unique_listings = all_results['listing_id'].unique()\n",
    "    \n",
    "    # Select a sample of listings that have multiple data points\n",
    "    listing_counts = all_results.groupby('listing_id').size()\n",
    "    listings_with_multiple_points = listing_counts[listing_counts > 3].index.tolist()\n",
    "    \n",
    "    # If we don't have enough listings with multiple points, use what we have\n",
    "    if len(listings_with_multiple_points) < num_listings:\n",
    "        sample_listings = listings_with_multiple_points + list(unique_listings[:num_listings - len(listings_with_multiple_points)])\n",
    "        sample_listings = sample_listings[:num_listings]  # Ensure we don't exceed requested number\n",
    "    else:\n",
    "        # Randomly select listings\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        sample_listings = np.random.choice(listings_with_multiple_points, num_listings, replace=False)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(math.ceil(num_listings/2), 2, figsize=(15, 3*math.ceil(num_listings/2)))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, listing_id in enumerate(sample_listings):\n",
    "        if i >= len(axes):  # Safety check\n",
    "            break\n",
    "            \n",
    "        # Get data for this listing\n",
    "        listing_data = all_results[all_results['listing_id'] == listing_id].copy()\n",
    "        listing_data = listing_data.sort_values('date')\n",
    "        \n",
    "        # Plot actual and predicted prices\n",
    "        ax = axes[i]\n",
    "        ax.plot(pd.to_datetime(listing_data['date']), listing_data['price'], 'o-', label='Actual', color='blue')\n",
    "        ax.plot(pd.to_datetime(listing_data['date']), listing_data['predicted'], 's--', label='Predicted', color='red')\n",
    "        \n",
    "        # Calculate RMSE for this listing\n",
    "        rmse = np.sqrt(mean_squared_error(listing_data['price'], listing_data['predicted']))\n",
    "        \n",
    "        ax.set_title(f'Listing ID: {listing_id} (RMSE: {rmse:.2f})')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price')\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'listing_predictions.png'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = \"train_up3.csv\"\n",
    "    train_ids_path = \"train_ids.txt\"\n",
    "    test_ids_path = \"test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/hybrid_gnn\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Run with hybrid approach\n",
    "        results = run_hybrid_gnn_with_rolling_window_cv(\n",
    "            train_path=train_path,\n",
    "            train_ids_path=train_ids_path,\n",
    "            test_ids_path=test_ids_path,\n",
    "            output_dir=output_dir,\n",
    "            window_size=35,\n",
    "            n_splits=5,\n",
    "            sample_size=None,  # Use full dataset\n",
    "            sequence_length=10,\n",
    "            hidden_dim=64\n",
    "        )\n",
    "        print(f\"Hybrid transformer-based GNN model training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running hybrid transformer-based GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
