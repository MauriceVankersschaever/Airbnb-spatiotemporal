{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Limiting to 200 random listings for testing\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Using 2 spatial features, 5 temporal features, 27 amenity features, and 15 price history features\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 19980 rows, 140 unique listings\n",
      "Test data: 420 rows, 60 unique listings\n",
      "\n",
      "----- Training GNN Model with Reduced LSTM (Split 1) -----\n",
      "Building enhanced spatial graph with 3872 test listings and 10 nearest neighbors...\n",
      "Created graph with 77440 edges\n",
      "Created enhanced graph with 19980 nodes and 77440 edges\n",
      "Train nodes: 16108, Val nodes: 3872\n",
      "Epoch 1/50 - Loss: 0.4095, Val Loss: 0.3985, RMSE: 142.35, MAE: 98.68\n",
      "Epoch 2/50 - Loss: 0.2674, Val Loss: 0.3930, RMSE: 141.71, MAE: 97.87\n",
      "Epoch 3/50 - Loss: 0.2117, Val Loss: 0.3894, RMSE: 141.24, MAE: 97.44\n",
      "Epoch 4/50 - Loss: 0.1941, Val Loss: 0.3882, RMSE: 141.02, MAE: 97.43\n",
      "Epoch 5/50 - Loss: 0.1870, Val Loss: 0.3879, RMSE: 140.90, MAE: 97.58\n",
      "Epoch 6/50 - Loss: 0.1709, Val Loss: 0.3880, RMSE: 140.84, MAE: 97.74\n",
      "Epoch 7/50 - Loss: 0.1600, Val Loss: 0.3868, RMSE: 140.67, MAE: 97.65\n",
      "Epoch 8/50 - Loss: 0.1542, Val Loss: 0.3834, RMSE: 140.32, MAE: 97.17\n",
      "Epoch 9/50 - Loss: 0.1475, Val Loss: 0.3777, RMSE: 139.72, MAE: 96.28\n",
      "Epoch 10/50 - Loss: 0.1446, Val Loss: 0.3694, RMSE: 138.86, MAE: 95.05\n",
      "Epoch 11/50 - Loss: 0.1354, Val Loss: 0.3592, RMSE: 137.79, MAE: 93.57\n",
      "Epoch 12/50 - Loss: 0.1278, Val Loss: 0.3479, RMSE: 136.64, MAE: 91.97\n",
      "Epoch 13/50 - Loss: 0.1220, Val Loss: 0.3368, RMSE: 135.59, MAE: 90.47\n",
      "Epoch 14/50 - Loss: 0.1168, Val Loss: 0.3266, RMSE: 134.64, MAE: 89.16\n",
      "Epoch 15/50 - Loss: 0.1114, Val Loss: 0.3182, RMSE: 133.83, MAE: 88.13\n",
      "Epoch 16/50 - Loss: 0.1131, Val Loss: 0.3120, RMSE: 133.17, MAE: 87.45\n",
      "Epoch 17/50 - Loss: 0.1073, Val Loss: 0.3076, RMSE: 132.56, MAE: 87.03\n",
      "Epoch 18/50 - Loss: 0.1033, Val Loss: 0.3042, RMSE: 131.90, MAE: 86.70\n",
      "Epoch 19/50 - Loss: 0.1010, Val Loss: 0.3009, RMSE: 131.09, MAE: 86.32\n",
      "Epoch 20/50 - Loss: 0.0977, Val Loss: 0.2968, RMSE: 129.99, MAE: 85.75\n",
      "Epoch 21/50 - Loss: 0.0943, Val Loss: 0.2915, RMSE: 128.65, MAE: 84.93\n",
      "Epoch 22/50 - Loss: 0.0913, Val Loss: 0.2856, RMSE: 127.14, MAE: 83.93\n",
      "Epoch 23/50 - Loss: 0.0887, Val Loss: 0.2788, RMSE: 125.61, MAE: 82.77\n",
      "Epoch 24/50 - Loss: 0.0863, Val Loss: 0.2711, RMSE: 124.02, MAE: 81.40\n",
      "Epoch 25/50 - Loss: 0.0856, Val Loss: 0.2631, RMSE: 122.47, MAE: 79.95\n",
      "Epoch 26/50 - Loss: 0.0834, Val Loss: 0.2546, RMSE: 120.98, MAE: 78.45\n",
      "Epoch 27/50 - Loss: 0.0800, Val Loss: 0.2460, RMSE: 119.58, MAE: 76.99\n",
      "Epoch 28/50 - Loss: 0.0766, Val Loss: 0.2371, RMSE: 118.24, MAE: 75.53\n",
      "Epoch 29/50 - Loss: 0.0746, Val Loss: 0.2281, RMSE: 116.77, MAE: 74.06\n",
      "Epoch 30/50 - Loss: 0.0718, Val Loss: 0.2195, RMSE: 115.30, MAE: 72.67\n",
      "Epoch 31/50 - Loss: 0.0683, Val Loss: 0.2113, RMSE: 113.85, MAE: 71.37\n",
      "Epoch 32/50 - Loss: 0.0692, Val Loss: 0.2032, RMSE: 112.23, MAE: 70.03\n",
      "Epoch 33/50 - Loss: 0.0676, Val Loss: 0.1951, RMSE: 110.42, MAE: 68.61\n",
      "Epoch 34/50 - Loss: 0.0655, Val Loss: 0.1872, RMSE: 108.44, MAE: 67.15\n",
      "Epoch 35/50 - Loss: 0.0652, Val Loss: 0.1796, RMSE: 106.36, MAE: 65.65\n",
      "Epoch 36/50 - Loss: 0.0634, Val Loss: 0.1721, RMSE: 104.18, MAE: 64.10\n",
      "Epoch 37/50 - Loss: 0.0611, Val Loss: 0.1649, RMSE: 102.08, MAE: 62.61\n",
      "Epoch 38/50 - Loss: 0.0601, Val Loss: 0.1572, RMSE: 99.87, MAE: 61.04\n",
      "Epoch 39/50 - Loss: 0.0569, Val Loss: 0.1496, RMSE: 97.65, MAE: 59.45\n",
      "Epoch 40/50 - Loss: 0.0560, Val Loss: 0.1419, RMSE: 95.52, MAE: 57.90\n",
      "Epoch 41/50 - Loss: 0.0558, Val Loss: 0.1341, RMSE: 93.41, MAE: 56.32\n",
      "Epoch 42/50 - Loss: 0.0528, Val Loss: 0.1266, RMSE: 91.45, MAE: 54.82\n",
      "Epoch 43/50 - Loss: 0.0527, Val Loss: 0.1193, RMSE: 89.47, MAE: 53.38\n",
      "Epoch 44/50 - Loss: 0.0523, Val Loss: 0.1127, RMSE: 87.73, MAE: 52.13\n",
      "Epoch 45/50 - Loss: 0.0515, Val Loss: 0.1066, RMSE: 86.06, MAE: 50.95\n",
      "Epoch 46/50 - Loss: 0.0485, Val Loss: 0.1005, RMSE: 84.31, MAE: 49.72\n",
      "Epoch 47/50 - Loss: 0.0482, Val Loss: 0.0942, RMSE: 82.38, MAE: 48.34\n",
      "Epoch 48/50 - Loss: 0.0481, Val Loss: 0.0875, RMSE: 80.11, MAE: 46.75\n",
      "Epoch 49/50 - Loss: 0.0455, Val Loss: 0.0809, RMSE: 77.70, MAE: 45.11\n",
      "Epoch 50/50 - Loss: 0.0450, Val Loss: 0.0743, RMSE: 75.11, MAE: 43.30\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 1) -----\n",
      "Building enhanced spatial graph with 420 test listings and 10 nearest neighbors...\n",
      "Created graph with 8400 edges\n",
      "Created enhanced graph with 16528 nodes and 8400 edges\n",
      "Train nodes: 16108, Val nodes: 420\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 80.35\n",
      "MAE: 47.79\n",
      "R²: 0.6321\n",
      "MAPE: 24.62%\n",
      "Split 1 Results - RMSE: 80.3522, MAE: 47.7935, R²: 0.6321\n",
      "Model for split 1 saved to ./output/reduced_lstm_gnn_model\\gnn_model_split_1.pt\n",
      "\n",
      "===== Split 2/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 20960 rows, 140 unique listings\n",
      "Test data: 420 rows, 60 unique listings\n",
      "\n",
      "----- Training GNN Model with Reduced LSTM (Split 2) -----\n",
      "Building enhanced spatial graph with 4068 test listings and 10 nearest neighbors...\n",
      "Created graph with 81360 edges\n",
      "Created enhanced graph with 20960 nodes and 81360 edges\n",
      "Train nodes: 16892, Val nodes: 4068\n",
      "Epoch 1/50 - Loss: 0.5677, Val Loss: 0.4050, RMSE: 144.77, MAE: 99.68\n",
      "Epoch 2/50 - Loss: 0.3641, Val Loss: 0.3972, RMSE: 143.89, MAE: 98.43\n",
      "Epoch 3/50 - Loss: 0.2445, Val Loss: 0.3892, RMSE: 143.02, MAE: 97.36\n",
      "Epoch 4/50 - Loss: 0.1989, Val Loss: 0.3836, RMSE: 142.49, MAE: 96.85\n",
      "Epoch 5/50 - Loss: 0.1847, Val Loss: 0.3810, RMSE: 142.40, MAE: 96.86\n",
      "Epoch 6/50 - Loss: 0.1782, Val Loss: 0.3805, RMSE: 142.45, MAE: 97.04\n",
      "Epoch 7/50 - Loss: 0.1686, Val Loss: 0.3809, RMSE: 142.49, MAE: 97.18\n",
      "Epoch 8/50 - Loss: 0.1558, Val Loss: 0.3804, RMSE: 142.28, MAE: 97.03\n",
      "Epoch 9/50 - Loss: 0.1464, Val Loss: 0.3777, RMSE: 141.66, MAE: 96.44\n",
      "Epoch 10/50 - Loss: 0.1389, Val Loss: 0.3723, RMSE: 140.65, MAE: 95.39\n",
      "Epoch 11/50 - Loss: 0.1324, Val Loss: 0.3652, RMSE: 139.39, MAE: 94.06\n",
      "Epoch 12/50 - Loss: 0.1285, Val Loss: 0.3568, RMSE: 137.97, MAE: 92.56\n",
      "Epoch 13/50 - Loss: 0.1254, Val Loss: 0.3475, RMSE: 136.47, MAE: 91.05\n",
      "Epoch 14/50 - Loss: 0.1203, Val Loss: 0.3384, RMSE: 135.05, MAE: 89.65\n",
      "Epoch 15/50 - Loss: 0.1170, Val Loss: 0.3300, RMSE: 133.75, MAE: 88.48\n",
      "Epoch 16/50 - Loss: 0.1093, Val Loss: 0.3223, RMSE: 132.56, MAE: 87.50\n",
      "Epoch 17/50 - Loss: 0.1056, Val Loss: 0.3157, RMSE: 131.43, MAE: 86.72\n",
      "Epoch 18/50 - Loss: 0.1011, Val Loss: 0.3094, RMSE: 130.26, MAE: 85.99\n",
      "Epoch 19/50 - Loss: 0.0997, Val Loss: 0.3034, RMSE: 128.98, MAE: 85.21\n",
      "Epoch 20/50 - Loss: 0.0979, Val Loss: 0.2975, RMSE: 127.60, MAE: 84.32\n",
      "Epoch 21/50 - Loss: 0.0940, Val Loss: 0.2915, RMSE: 126.10, MAE: 83.29\n",
      "Epoch 22/50 - Loss: 0.0928, Val Loss: 0.2854, RMSE: 124.53, MAE: 82.15\n",
      "Epoch 23/50 - Loss: 0.0886, Val Loss: 0.2796, RMSE: 123.03, MAE: 81.00\n",
      "Epoch 24/50 - Loss: 0.0830, Val Loss: 0.2737, RMSE: 121.56, MAE: 79.81\n",
      "Epoch 25/50 - Loss: 0.0805, Val Loss: 0.2679, RMSE: 120.19, MAE: 78.62\n",
      "Epoch 26/50 - Loss: 0.0803, Val Loss: 0.2626, RMSE: 119.02, MAE: 77.55\n",
      "Epoch 27/50 - Loss: 0.0768, Val Loss: 0.2573, RMSE: 117.92, MAE: 76.51\n",
      "Epoch 28/50 - Loss: 0.0758, Val Loss: 0.2515, RMSE: 116.86, MAE: 75.43\n",
      "Epoch 29/50 - Loss: 0.0727, Val Loss: 0.2455, RMSE: 115.85, MAE: 74.36\n",
      "Epoch 30/50 - Loss: 0.0712, Val Loss: 0.2393, RMSE: 114.83, MAE: 73.25\n",
      "Epoch 31/50 - Loss: 0.0687, Val Loss: 0.2324, RMSE: 113.69, MAE: 72.06\n",
      "Epoch 32/50 - Loss: 0.0675, Val Loss: 0.2251, RMSE: 112.38, MAE: 70.82\n",
      "Epoch 33/50 - Loss: 0.0661, Val Loss: 0.2168, RMSE: 110.82, MAE: 69.41\n",
      "Epoch 34/50 - Loss: 0.0648, Val Loss: 0.2075, RMSE: 108.99, MAE: 67.83\n",
      "Epoch 35/50 - Loss: 0.0632, Val Loss: 0.1976, RMSE: 106.96, MAE: 66.16\n",
      "Epoch 36/50 - Loss: 0.0616, Val Loss: 0.1874, RMSE: 104.65, MAE: 64.37\n",
      "Epoch 37/50 - Loss: 0.0592, Val Loss: 0.1774, RMSE: 102.20, MAE: 62.60\n",
      "Epoch 38/50 - Loss: 0.0579, Val Loss: 0.1677, RMSE: 99.66, MAE: 60.88\n",
      "Epoch 39/50 - Loss: 0.0563, Val Loss: 0.1586, RMSE: 97.01, MAE: 59.23\n",
      "Epoch 40/50 - Loss: 0.0551, Val Loss: 0.1500, RMSE: 94.33, MAE: 57.59\n",
      "Epoch 41/50 - Loss: 0.0531, Val Loss: 0.1421, RMSE: 91.63, MAE: 56.02\n",
      "Epoch 42/50 - Loss: 0.0523, Val Loss: 0.1348, RMSE: 89.00, MAE: 54.47\n",
      "Epoch 43/50 - Loss: 0.0512, Val Loss: 0.1280, RMSE: 86.40, MAE: 52.92\n",
      "Epoch 44/50 - Loss: 0.0490, Val Loss: 0.1215, RMSE: 83.80, MAE: 51.35\n",
      "Epoch 45/50 - Loss: 0.0481, Val Loss: 0.1152, RMSE: 81.19, MAE: 49.72\n",
      "Epoch 46/50 - Loss: 0.0464, Val Loss: 0.1086, RMSE: 78.62, MAE: 48.01\n",
      "Epoch 47/50 - Loss: 0.0451, Val Loss: 0.1020, RMSE: 76.09, MAE: 46.25\n",
      "Epoch 48/50 - Loss: 0.0452, Val Loss: 0.0955, RMSE: 73.62, MAE: 44.51\n",
      "Epoch 49/50 - Loss: 0.0445, Val Loss: 0.0890, RMSE: 71.21, MAE: 42.76\n",
      "Epoch 50/50 - Loss: 0.0421, Val Loss: 0.0826, RMSE: 68.92, MAE: 41.06\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 2) -----\n",
      "Building enhanced spatial graph with 420 test listings and 10 nearest neighbors...\n",
      "Created graph with 8400 edges\n",
      "Created enhanced graph with 17312 nodes and 8400 edges\n",
      "Train nodes: 16892, Val nodes: 420\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 71.05\n",
      "MAE: 42.54\n",
      "R²: 0.7049\n",
      "MAPE: 22.83%\n",
      "Split 2 Results - RMSE: 71.0473, MAE: 42.5396, R²: 0.7049\n",
      "Model for split 2 saved to ./output/reduced_lstm_gnn_model\\gnn_model_split_2.pt\n",
      "\n",
      "===== Split 3/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-18\n",
      "Testing period: 2024-01-19 to 2024-01-25\n",
      "Train data: 21940 rows, 140 unique listings\n",
      "Test data: 420 rows, 60 unique listings\n",
      "\n",
      "----- Training GNN Model with Reduced LSTM (Split 3) -----\n",
      "Building enhanced spatial graph with 4264 test listings and 10 nearest neighbors...\n",
      "Created graph with 85280 edges\n",
      "Created enhanced graph with 21940 nodes and 85280 edges\n",
      "Train nodes: 17676, Val nodes: 4264\n",
      "Epoch 1/50 - Loss: 0.6139, Val Loss: 0.4052, RMSE: 147.61, MAE: 100.19\n",
      "Epoch 2/50 - Loss: 0.3718, Val Loss: 0.3978, RMSE: 146.55, MAE: 98.83\n",
      "Epoch 3/50 - Loss: 0.2752, Val Loss: 0.3926, RMSE: 145.87, MAE: 98.02\n",
      "Epoch 4/50 - Loss: 0.2423, Val Loss: 0.3903, RMSE: 145.60, MAE: 97.81\n",
      "Epoch 5/50 - Loss: 0.2151, Val Loss: 0.3901, RMSE: 145.42, MAE: 98.00\n",
      "Epoch 6/50 - Loss: 0.1978, Val Loss: 0.3900, RMSE: 145.08, MAE: 98.13\n",
      "Epoch 7/50 - Loss: 0.1895, Val Loss: 0.3883, RMSE: 144.44, MAE: 97.87\n",
      "Epoch 8/50 - Loss: 0.1773, Val Loss: 0.3847, RMSE: 143.57, MAE: 97.18\n",
      "Epoch 9/50 - Loss: 0.1680, Val Loss: 0.3795, RMSE: 142.59, MAE: 96.15\n",
      "Epoch 10/50 - Loss: 0.1569, Val Loss: 0.3730, RMSE: 141.56, MAE: 94.89\n",
      "Epoch 11/50 - Loss: 0.1496, Val Loss: 0.3661, RMSE: 140.62, MAE: 93.62\n",
      "Epoch 12/50 - Loss: 0.1500, Val Loss: 0.3584, RMSE: 139.66, MAE: 92.33\n",
      "Epoch 13/50 - Loss: 0.1420, Val Loss: 0.3503, RMSE: 138.62, MAE: 91.03\n",
      "Epoch 14/50 - Loss: 0.1372, Val Loss: 0.3422, RMSE: 137.52, MAE: 89.76\n",
      "Epoch 15/50 - Loss: 0.1307, Val Loss: 0.3341, RMSE: 136.31, MAE: 88.51\n",
      "Epoch 16/50 - Loss: 0.1245, Val Loss: 0.3268, RMSE: 135.05, MAE: 87.44\n",
      "Epoch 17/50 - Loss: 0.1230, Val Loss: 0.3203, RMSE: 133.73, MAE: 86.52\n",
      "Epoch 18/50 - Loss: 0.1179, Val Loss: 0.3147, RMSE: 132.42, MAE: 85.72\n",
      "Epoch 19/50 - Loss: 0.1166, Val Loss: 0.3096, RMSE: 131.00, MAE: 84.94\n",
      "Epoch 20/50 - Loss: 0.1112, Val Loss: 0.3050, RMSE: 129.58, MAE: 84.18\n",
      "Epoch 21/50 - Loss: 0.1099, Val Loss: 0.3003, RMSE: 128.16, MAE: 83.34\n",
      "Epoch 22/50 - Loss: 0.1061, Val Loss: 0.2958, RMSE: 126.78, MAE: 82.49\n",
      "Epoch 23/50 - Loss: 0.1017, Val Loss: 0.2910, RMSE: 125.54, MAE: 81.63\n",
      "Epoch 24/50 - Loss: 0.1007, Val Loss: 0.2858, RMSE: 124.40, MAE: 80.76\n",
      "Epoch 25/50 - Loss: 0.0975, Val Loss: 0.2799, RMSE: 123.33, MAE: 79.83\n",
      "Epoch 26/50 - Loss: 0.0970, Val Loss: 0.2734, RMSE: 122.31, MAE: 78.90\n",
      "Epoch 27/50 - Loss: 0.0934, Val Loss: 0.2664, RMSE: 121.29, MAE: 77.93\n",
      "Epoch 28/50 - Loss: 0.0899, Val Loss: 0.2590, RMSE: 120.27, MAE: 76.88\n",
      "Epoch 29/50 - Loss: 0.0870, Val Loss: 0.2510, RMSE: 119.19, MAE: 75.74\n",
      "Epoch 30/50 - Loss: 0.0848, Val Loss: 0.2430, RMSE: 118.02, MAE: 74.55\n",
      "Epoch 31/50 - Loss: 0.0838, Val Loss: 0.2351, RMSE: 116.76, MAE: 73.36\n",
      "Epoch 32/50 - Loss: 0.0807, Val Loss: 0.2276, RMSE: 115.38, MAE: 72.16\n",
      "Epoch 33/50 - Loss: 0.0779, Val Loss: 0.2196, RMSE: 113.74, MAE: 70.79\n",
      "Epoch 34/50 - Loss: 0.0769, Val Loss: 0.2121, RMSE: 112.00, MAE: 69.39\n",
      "Epoch 35/50 - Loss: 0.0743, Val Loss: 0.2048, RMSE: 110.17, MAE: 68.00\n",
      "Epoch 36/50 - Loss: 0.0723, Val Loss: 0.1976, RMSE: 108.26, MAE: 66.55\n",
      "Epoch 37/50 - Loss: 0.0689, Val Loss: 0.1904, RMSE: 106.34, MAE: 65.06\n",
      "Epoch 38/50 - Loss: 0.0694, Val Loss: 0.1832, RMSE: 104.46, MAE: 63.58\n",
      "Epoch 39/50 - Loss: 0.0671, Val Loss: 0.1758, RMSE: 102.52, MAE: 62.08\n",
      "Epoch 40/50 - Loss: 0.0672, Val Loss: 0.1691, RMSE: 100.77, MAE: 60.71\n",
      "Epoch 41/50 - Loss: 0.0640, Val Loss: 0.1630, RMSE: 99.16, MAE: 59.47\n",
      "Epoch 42/50 - Loss: 0.0621, Val Loss: 0.1565, RMSE: 97.55, MAE: 58.17\n",
      "Epoch 43/50 - Loss: 0.0609, Val Loss: 0.1501, RMSE: 96.05, MAE: 56.92\n",
      "Epoch 44/50 - Loss: 0.0591, Val Loss: 0.1436, RMSE: 94.45, MAE: 55.65\n",
      "Epoch 45/50 - Loss: 0.0586, Val Loss: 0.1366, RMSE: 92.71, MAE: 54.27\n",
      "Epoch 46/50 - Loss: 0.0565, Val Loss: 0.1291, RMSE: 90.82, MAE: 52.77\n",
      "Epoch 47/50 - Loss: 0.0550, Val Loss: 0.1215, RMSE: 88.76, MAE: 51.22\n",
      "Epoch 48/50 - Loss: 0.0539, Val Loss: 0.1143, RMSE: 86.51, MAE: 49.64\n",
      "Epoch 49/50 - Loss: 0.0521, Val Loss: 0.1073, RMSE: 84.15, MAE: 48.01\n",
      "Epoch 50/50 - Loss: 0.0518, Val Loss: 0.1008, RMSE: 81.67, MAE: 46.39\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 3) -----\n",
      "Building enhanced spatial graph with 420 test listings and 10 nearest neighbors...\n",
      "Created graph with 8400 edges\n",
      "Created enhanced graph with 18096 nodes and 8400 edges\n",
      "Train nodes: 17676, Val nodes: 420\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 77.86\n",
      "MAE: 45.25\n",
      "R²: 0.6482\n",
      "MAPE: 26.67%\n",
      "Split 3 Results - RMSE: 77.8584, MAE: 45.2492, R²: 0.6482\n",
      "Model for split 3 saved to ./output/reduced_lstm_gnn_model\\gnn_model_split_3.pt\n",
      "\n",
      "===== Split 4/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-25\n",
      "Testing period: 2024-01-26 to 2024-02-01\n",
      "Train data: 22920 rows, 140 unique listings\n",
      "Test data: 420 rows, 60 unique listings\n",
      "\n",
      "----- Training GNN Model with Reduced LSTM (Split 4) -----\n",
      "Building enhanced spatial graph with 4460 test listings and 10 nearest neighbors...\n",
      "Created graph with 89200 edges\n",
      "Created enhanced graph with 22920 nodes and 89200 edges\n",
      "Train nodes: 18460, Val nodes: 4460\n",
      "Epoch 1/50 - Loss: 0.6240, Val Loss: 0.4071, RMSE: 143.19, MAE: 99.46\n",
      "Epoch 2/50 - Loss: 0.3979, Val Loss: 0.4005, RMSE: 142.44, MAE: 98.41\n",
      "Epoch 3/50 - Loss: 0.2567, Val Loss: 0.3936, RMSE: 141.63, MAE: 97.39\n",
      "Epoch 4/50 - Loss: 0.2164, Val Loss: 0.3892, RMSE: 141.18, MAE: 96.92\n",
      "Epoch 5/50 - Loss: 0.2093, Val Loss: 0.3876, RMSE: 141.07, MAE: 96.96\n",
      "Epoch 6/50 - Loss: 0.1968, Val Loss: 0.3878, RMSE: 141.15, MAE: 97.27\n",
      "Epoch 7/50 - Loss: 0.1807, Val Loss: 0.3884, RMSE: 141.27, MAE: 97.59\n",
      "Epoch 8/50 - Loss: 0.1665, Val Loss: 0.3877, RMSE: 141.24, MAE: 97.67\n",
      "Epoch 9/50 - Loss: 0.1602, Val Loss: 0.3849, RMSE: 140.97, MAE: 97.37\n",
      "Epoch 10/50 - Loss: 0.1517, Val Loss: 0.3801, RMSE: 140.49, MAE: 96.75\n",
      "Epoch 11/50 - Loss: 0.1459, Val Loss: 0.3735, RMSE: 139.80, MAE: 95.87\n",
      "Epoch 12/50 - Loss: 0.1397, Val Loss: 0.3656, RMSE: 138.95, MAE: 94.80\n",
      "Epoch 13/50 - Loss: 0.1350, Val Loss: 0.3571, RMSE: 138.02, MAE: 93.64\n",
      "Epoch 14/50 - Loss: 0.1290, Val Loss: 0.3487, RMSE: 137.07, MAE: 92.53\n",
      "Epoch 15/50 - Loss: 0.1266, Val Loss: 0.3413, RMSE: 136.19, MAE: 91.58\n",
      "Epoch 16/50 - Loss: 0.1183, Val Loss: 0.3352, RMSE: 135.41, MAE: 90.84\n",
      "Epoch 17/50 - Loss: 0.1146, Val Loss: 0.3308, RMSE: 134.71, MAE: 90.34\n",
      "Epoch 18/50 - Loss: 0.1104, Val Loss: 0.3279, RMSE: 134.06, MAE: 90.03\n",
      "Epoch 19/50 - Loss: 0.1068, Val Loss: 0.3263, RMSE: 133.49, MAE: 89.89\n",
      "Epoch 20/50 - Loss: 0.1033, Val Loss: 0.3250, RMSE: 132.89, MAE: 89.76\n",
      "Epoch 21/50 - Loss: 0.0999, Val Loss: 0.3236, RMSE: 132.17, MAE: 89.54\n",
      "Epoch 22/50 - Loss: 0.0956, Val Loss: 0.3212, RMSE: 131.32, MAE: 89.12\n",
      "Epoch 23/50 - Loss: 0.0935, Val Loss: 0.3177, RMSE: 130.30, MAE: 88.48\n",
      "Epoch 24/50 - Loss: 0.0901, Val Loss: 0.3126, RMSE: 129.14, MAE: 87.58\n",
      "Epoch 25/50 - Loss: 0.0875, Val Loss: 0.3059, RMSE: 127.84, MAE: 86.45\n",
      "Epoch 26/50 - Loss: 0.0869, Val Loss: 0.2977, RMSE: 126.41, MAE: 85.13\n",
      "Epoch 27/50 - Loss: 0.0832, Val Loss: 0.2882, RMSE: 124.88, MAE: 83.64\n",
      "Epoch 28/50 - Loss: 0.0809, Val Loss: 0.2777, RMSE: 123.29, MAE: 82.06\n",
      "Epoch 29/50 - Loss: 0.0801, Val Loss: 0.2666, RMSE: 121.65, MAE: 80.39\n",
      "Epoch 30/50 - Loss: 0.0773, Val Loss: 0.2550, RMSE: 119.95, MAE: 78.68\n",
      "Epoch 31/50 - Loss: 0.0739, Val Loss: 0.2439, RMSE: 118.29, MAE: 77.04\n",
      "Epoch 32/50 - Loss: 0.0723, Val Loss: 0.2334, RMSE: 116.64, MAE: 75.49\n",
      "Epoch 33/50 - Loss: 0.0726, Val Loss: 0.2238, RMSE: 115.04, MAE: 74.07\n",
      "Epoch 34/50 - Loss: 0.0697, Val Loss: 0.2152, RMSE: 113.41, MAE: 72.78\n",
      "Epoch 35/50 - Loss: 0.0678, Val Loss: 0.2076, RMSE: 111.82, MAE: 71.62\n",
      "Epoch 36/50 - Loss: 0.0663, Val Loss: 0.2010, RMSE: 110.23, MAE: 70.59\n",
      "Epoch 37/50 - Loss: 0.0646, Val Loss: 0.1951, RMSE: 108.62, MAE: 69.63\n",
      "Epoch 38/50 - Loss: 0.0630, Val Loss: 0.1893, RMSE: 106.95, MAE: 68.65\n",
      "Epoch 39/50 - Loss: 0.0609, Val Loss: 0.1838, RMSE: 105.29, MAE: 67.69\n",
      "Epoch 40/50 - Loss: 0.0582, Val Loss: 0.1784, RMSE: 103.63, MAE: 66.72\n",
      "Epoch 41/50 - Loss: 0.0577, Val Loss: 0.1728, RMSE: 101.94, MAE: 65.69\n",
      "Epoch 42/50 - Loss: 0.0565, Val Loss: 0.1666, RMSE: 100.11, MAE: 64.54\n",
      "Epoch 43/50 - Loss: 0.0548, Val Loss: 0.1592, RMSE: 98.10, MAE: 63.16\n",
      "Epoch 44/50 - Loss: 0.0528, Val Loss: 0.1509, RMSE: 95.94, MAE: 61.59\n",
      "Epoch 45/50 - Loss: 0.0540, Val Loss: 0.1417, RMSE: 93.57, MAE: 59.84\n",
      "Epoch 46/50 - Loss: 0.0518, Val Loss: 0.1321, RMSE: 90.99, MAE: 57.90\n",
      "Epoch 47/50 - Loss: 0.0509, Val Loss: 0.1223, RMSE: 88.32, MAE: 55.87\n",
      "Epoch 48/50 - Loss: 0.0486, Val Loss: 0.1128, RMSE: 85.65, MAE: 53.85\n",
      "Epoch 49/50 - Loss: 0.0477, Val Loss: 0.1040, RMSE: 83.03, MAE: 51.87\n",
      "Epoch 50/50 - Loss: 0.0468, Val Loss: 0.0958, RMSE: 80.42, MAE: 49.91\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 4) -----\n",
      "Building enhanced spatial graph with 420 test listings and 10 nearest neighbors...\n",
      "Created graph with 8400 edges\n",
      "Created enhanced graph with 18880 nodes and 8400 edges\n",
      "Train nodes: 18460, Val nodes: 420\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 109.06\n",
      "MAE: 52.88\n",
      "R²: 0.5034\n",
      "MAPE: 26.04%\n",
      "Split 4 Results - RMSE: 109.0606, MAE: 52.8820, R²: 0.5034\n",
      "Model for split 4 saved to ./output/reduced_lstm_gnn_model\\gnn_model_split_4.pt\n",
      "\n",
      "===== Split 5/5 =====\n",
      "Training period: 2023-08-07 to 2024-02-01\n",
      "Testing period: 2024-02-02 to 2024-02-08\n",
      "Train data: 23900 rows, 140 unique listings\n",
      "Test data: 420 rows, 60 unique listings\n",
      "\n",
      "----- Training GNN Model with Reduced LSTM (Split 5) -----\n",
      "Building enhanced spatial graph with 4656 test listings and 10 nearest neighbors...\n",
      "Created graph with 93120 edges\n",
      "Created enhanced graph with 23900 nodes and 93120 edges\n",
      "Train nodes: 19244, Val nodes: 4656\n",
      "Epoch 1/50 - Loss: 0.6883, Val Loss: 0.4011, RMSE: 141.45, MAE: 97.96\n",
      "Epoch 2/50 - Loss: 0.4235, Val Loss: 0.3947, RMSE: 140.16, MAE: 96.66\n",
      "Epoch 3/50 - Loss: 0.2541, Val Loss: 0.3874, RMSE: 138.75, MAE: 95.19\n",
      "Epoch 4/50 - Loss: 0.2009, Val Loss: 0.3818, RMSE: 137.85, MAE: 94.18\n",
      "Epoch 5/50 - Loss: 0.2002, Val Loss: 0.3789, RMSE: 137.54, MAE: 93.81\n",
      "Epoch 6/50 - Loss: 0.1882, Val Loss: 0.3783, RMSE: 137.63, MAE: 93.90\n",
      "Epoch 7/50 - Loss: 0.1699, Val Loss: 0.3786, RMSE: 137.92, MAE: 94.23\n",
      "Epoch 8/50 - Loss: 0.1549, Val Loss: 0.3789, RMSE: 138.18, MAE: 94.56\n",
      "Epoch 9/50 - Loss: 0.1462, Val Loss: 0.3776, RMSE: 138.14, MAE: 94.63\n",
      "Epoch 10/50 - Loss: 0.1427, Val Loss: 0.3737, RMSE: 137.72, MAE: 94.26\n",
      "Epoch 11/50 - Loss: 0.1388, Val Loss: 0.3673, RMSE: 136.90, MAE: 93.45\n",
      "Epoch 12/50 - Loss: 0.1332, Val Loss: 0.3587, RMSE: 135.75, MAE: 92.26\n",
      "Epoch 13/50 - Loss: 0.1282, Val Loss: 0.3481, RMSE: 134.32, MAE: 90.75\n",
      "Epoch 14/50 - Loss: 0.1194, Val Loss: 0.3368, RMSE: 132.79, MAE: 89.12\n",
      "Epoch 15/50 - Loss: 0.1148, Val Loss: 0.3260, RMSE: 131.29, MAE: 87.50\n",
      "Epoch 16/50 - Loss: 0.1098, Val Loss: 0.3160, RMSE: 129.90, MAE: 85.98\n",
      "Epoch 17/50 - Loss: 0.1058, Val Loss: 0.3074, RMSE: 128.70, MAE: 84.65\n",
      "Epoch 18/50 - Loss: 0.1037, Val Loss: 0.3004, RMSE: 127.71, MAE: 83.58\n",
      "Epoch 19/50 - Loss: 0.1004, Val Loss: 0.2944, RMSE: 126.86, MAE: 82.67\n",
      "Epoch 20/50 - Loss: 0.0989, Val Loss: 0.2892, RMSE: 126.15, MAE: 81.89\n",
      "Epoch 21/50 - Loss: 0.0959, Val Loss: 0.2843, RMSE: 125.48, MAE: 81.17\n",
      "Epoch 22/50 - Loss: 0.0923, Val Loss: 0.2794, RMSE: 124.79, MAE: 80.49\n",
      "Epoch 23/50 - Loss: 0.0901, Val Loss: 0.2746, RMSE: 124.05, MAE: 79.81\n",
      "Epoch 24/50 - Loss: 0.0880, Val Loss: 0.2697, RMSE: 123.21, MAE: 79.08\n",
      "Epoch 25/50 - Loss: 0.0840, Val Loss: 0.2646, RMSE: 122.24, MAE: 78.31\n",
      "Epoch 26/50 - Loss: 0.0812, Val Loss: 0.2592, RMSE: 121.12, MAE: 77.46\n",
      "Epoch 27/50 - Loss: 0.0815, Val Loss: 0.2539, RMSE: 119.89, MAE: 76.58\n",
      "Epoch 28/50 - Loss: 0.0767, Val Loss: 0.2489, RMSE: 118.62, MAE: 75.69\n",
      "Epoch 29/50 - Loss: 0.0732, Val Loss: 0.2439, RMSE: 117.34, MAE: 74.78\n",
      "Epoch 30/50 - Loss: 0.0736, Val Loss: 0.2385, RMSE: 115.93, MAE: 73.83\n",
      "Epoch 31/50 - Loss: 0.0726, Val Loss: 0.2331, RMSE: 114.47, MAE: 72.84\n",
      "Epoch 32/50 - Loss: 0.0689, Val Loss: 0.2271, RMSE: 112.94, MAE: 71.79\n",
      "Epoch 33/50 - Loss: 0.0686, Val Loss: 0.2206, RMSE: 111.33, MAE: 70.66\n",
      "Epoch 34/50 - Loss: 0.0661, Val Loss: 0.2133, RMSE: 109.67, MAE: 69.42\n",
      "Epoch 35/50 - Loss: 0.0636, Val Loss: 0.2057, RMSE: 107.97, MAE: 68.12\n",
      "Epoch 36/50 - Loss: 0.0617, Val Loss: 0.1980, RMSE: 106.27, MAE: 66.79\n",
      "Epoch 37/50 - Loss: 0.0604, Val Loss: 0.1900, RMSE: 104.55, MAE: 65.37\n",
      "Epoch 38/50 - Loss: 0.0592, Val Loss: 0.1822, RMSE: 102.78, MAE: 63.95\n",
      "Epoch 39/50 - Loss: 0.0578, Val Loss: 0.1744, RMSE: 100.98, MAE: 62.48\n",
      "Epoch 40/50 - Loss: 0.0560, Val Loss: 0.1667, RMSE: 99.14, MAE: 60.96\n",
      "Epoch 41/50 - Loss: 0.0563, Val Loss: 0.1589, RMSE: 97.21, MAE: 59.42\n",
      "Epoch 42/50 - Loss: 0.0541, Val Loss: 0.1514, RMSE: 95.30, MAE: 57.91\n",
      "Epoch 43/50 - Loss: 0.0518, Val Loss: 0.1438, RMSE: 93.29, MAE: 56.36\n",
      "Epoch 44/50 - Loss: 0.0516, Val Loss: 0.1358, RMSE: 91.11, MAE: 54.71\n",
      "Epoch 45/50 - Loss: 0.0504, Val Loss: 0.1278, RMSE: 88.83, MAE: 53.02\n",
      "Epoch 46/50 - Loss: 0.0485, Val Loss: 0.1200, RMSE: 86.49, MAE: 51.33\n",
      "Epoch 47/50 - Loss: 0.0469, Val Loss: 0.1122, RMSE: 84.17, MAE: 49.64\n",
      "Epoch 48/50 - Loss: 0.0461, Val Loss: 0.1045, RMSE: 81.83, MAE: 47.97\n",
      "Epoch 49/50 - Loss: 0.0449, Val Loss: 0.0973, RMSE: 79.58, MAE: 46.38\n",
      "Epoch 50/50 - Loss: 0.0443, Val Loss: 0.0903, RMSE: 77.38, MAE: 44.84\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 5) -----\n",
      "Building enhanced spatial graph with 420 test listings and 10 nearest neighbors...\n",
      "Created graph with 8400 edges\n",
      "Created enhanced graph with 19664 nodes and 8400 edges\n",
      "Train nodes: 19244, Val nodes: 420\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 88.61\n",
      "MAE: 48.07\n",
      "R²: 0.6010\n",
      "MAPE: 23.30%\n",
      "Split 5 Results - RMSE: 88.6124, MAE: 48.0700, R²: 0.6010\n",
      "Model for split 5 saved to ./output/reduced_lstm_gnn_model\\gnn_model_split_5.pt\n",
      "Results saved to ./output/reduced_lstm_gnn_model\\gnn_rolling_window_results.csv\n",
      "Daily metrics saved to ./output/reduced_lstm_gnn_model\\gnn_rolling_window_metrics.csv\n",
      "\n",
      "===== GNN ROLLING WINDOW CV SUMMARY =====\n",
      "Using 140 listings for training and 60 listings for testing\n",
      "\n",
      "=== Overall Metrics ===\n",
      "RMSE: 86.3857\n",
      "MAE: 47.3069\n",
      "R²: 0.6095\n",
      "MAPE: 24.6919%\n",
      "\n",
      "=== Split Performance ===\n",
      " split       rmse       mae       r2  n_samples\n",
      "     0  80.352155 47.793529 0.632143        420\n",
      "     1  71.047298 42.539561 0.704938        420\n",
      "     2  77.858406 45.249246 0.648190        420\n",
      "     3 109.060607 52.882031 0.503428        420\n",
      "     4  88.612371 48.070004 0.600985        420\n",
      "GNN model with reduced LSTM training with rolling window CV completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import gc\n",
    "\n",
    "# Reduced dimensionality TimeSeriesEncoderLSTM\n",
    "class TimeSeriesEncoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers=1, dropout=0.0, bidirectional=True):\n",
    "        super(TimeSeriesEncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim  # Reduced internal dimension\n",
    "        self.output_dim = output_dim  # Final output dimension\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM layer with reduced hidden dimension\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,  # Single feature per timestep (price)\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Projection layer to map from LSTM output to desired output dimension\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.projection = nn.Linear(lstm_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch_size, seq_len] -> [batch_size, seq_len, 1]\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final hidden states from both directions\n",
    "            hidden_forward = hidden[-2, :, :]\n",
    "            hidden_backward = hidden[-1, :, :]\n",
    "            combined = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            # Use only the final hidden state\n",
    "            combined = hidden[-1, :, :]\n",
    "            \n",
    "        # Project to output dimension\n",
    "        projected = self.projection(combined)\n",
    "        return projected\n",
    "\n",
    "# Price transformation function\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create calculated features\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Build enhanced spatial graph for GNN\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (optional, for smaller datasets)\n",
    "    if len(train_coords) <= 5000:\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "def create_daily_price_history(df, seq_length=30):\n",
    "    \"\"\"\n",
    "    Create raw price history sequences for each listing\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe with reset index\n",
    "    result_df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Get all unique listings\n",
    "    listings = result_df['listing_id'].unique()\n",
    "    \n",
    "    # For each listing\n",
    "    for listing_id in listings:\n",
    "        # Get all dates for this listing, sorted\n",
    "        listing_data = result_df[result_df['listing_id'] == listing_id].sort_values('date')\n",
    "        \n",
    "        # For each row in this listing's data - use internal index instead of original DataFrame index\n",
    "        for idx, (_, row) in enumerate(listing_data.iterrows()):\n",
    "            current_date = row['date']\n",
    "            \n",
    "            # Get previous dates data for this listing\n",
    "            previous_dates = listing_data[listing_data['date'] < current_date].sort_values('date', ascending=False).head(seq_length)\n",
    "            \n",
    "            # Find the corresponding row index in result_df that matches this row\n",
    "            current_idx = listing_data.index[idx]\n",
    "            \n",
    "            # Create sequence features - ONLY the raw price\n",
    "            for j, prev_row in enumerate(previous_dates.iterrows()):\n",
    "                day_num = j + 1\n",
    "                prev_row_data = prev_row[1]  # Get the data from the tuple\n",
    "                \n",
    "                # Just store the raw price for each previous day\n",
    "                result_df.loc[current_idx, f'price_day_{day_num}'] = prev_row_data['price']\n",
    "            \n",
    "            # Fill missing days with the last known value or current price\n",
    "            for day in range(1, seq_length + 1):\n",
    "                col_name = f'price_day_{day}'\n",
    "                if col_name not in result_df.columns or pd.isnull(result_df.loc[current_idx, col_name]):\n",
    "                    # Use the last available price, or current price if none available\n",
    "                    if previous_dates.empty:\n",
    "                        result_df.loc[current_idx, col_name] = row['price']\n",
    "                    else:\n",
    "                        result_df.loc[current_idx, col_name] = previous_dates.iloc[-1]['price'] if len(previous_dates) > 0 else row['price']\n",
    "    \n",
    "    # Create the list of generated features - only price_day_X columns\n",
    "    sequence_features = [f'price_day_{day}' for day in range(1, seq_length + 1)]\n",
    "    \n",
    "    return result_df, sequence_features\n",
    "\n",
    "# Enhanced GNN model with reduced LSTM dimension\n",
    "class EnhancedSpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "             spatial_features_dim,\n",
    "             temporal_features_dim,\n",
    "             amenity_features_dim,\n",
    "             price_history_features_dim,\n",
    "             price_history_seq_len=30,  \n",
    "             hidden_dim=64,\n",
    "             lstm_hidden_dim=32,  # Reduced LSTM hidden dimension\n",
    "             lstm_layers=1,\n",
    "             lstm_bidirectional=True,\n",
    "             dropout=0.3,\n",
    "             heads=4,\n",
    "             edge_dim=1):\n",
    "        super(EnhancedSpatioTemporalGNN, self).__init__()\n",
    "        \n",
    "        # Store sequence length as instance attribute\n",
    "        self.seq_len = price_history_seq_len\n",
    "        self.use_lstm = price_history_seq_len > 1\n",
    "        \n",
    "        # For multi-head attention, ensure hidden_dim is divisible by heads\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # Important: Make sure the output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads  # This ensures exact dimensions\n",
    "        \n",
    "        # Replace GCN with GAT for better spatial relationship modeling\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Add batch normalization for more stable training\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Enhanced temporal processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Enhanced amenity processing with residual connection\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Always create fallback linear layers for price history\n",
    "        self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "        self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # New: LSTM for price history encoding with reduced dimensions\n",
    "        self.use_lstm = price_history_seq_len > 1\n",
    "        \n",
    "        if self.use_lstm:\n",
    "            # Use reduced LSTM hidden dimension but project to common hidden_dim\n",
    "            self.price_history_lstm = TimeSeriesEncoderLSTM(\n",
    "                hidden_dim=lstm_hidden_dim,  # Reduced internal LSTM dimension\n",
    "                output_dim=hidden_dim,       # Project back to common hidden_dim\n",
    "                num_layers=lstm_layers,\n",
    "                dropout=dropout,\n",
    "                bidirectional=lstm_bidirectional\n",
    "            )\n",
    "            self.lstm_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - updated for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # First GAT layer with batch normalization and residual\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features with enhanced layers\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res  # Residual connection\n",
    "        \n",
    "        # Process amenity features with residual connection\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process price history features\n",
    "        # Get batch size\n",
    "        batch_size = price_history_x.shape[0]\n",
    "        \n",
    "        # Decision path for processing price history\n",
    "        if self.use_lstm:\n",
    "            try:\n",
    "                # For raw time series, reshape directly to [batch_size, seq_len]\n",
    "                # Assuming the price_history_x contains seq_len sequential prices\n",
    "                price_history_seq = price_history_x.reshape(batch_size, self.seq_len)\n",
    "                \n",
    "                # Pass through LSTM encoder with reduced dimension\n",
    "                price_history_features = self.price_history_lstm(price_history_seq)\n",
    "                price_history_features = self.lstm_bn(price_history_features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in LSTM processing: {e}. Falling back to linear layers.\")\n",
    "                # Fallback to linear layers\n",
    "                price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "                price_history_features = self.price_history_bn1(price_history_features)\n",
    "                price_history_features = self.dropout(price_history_features)\n",
    "                price_history_res = price_history_features\n",
    "                price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "                price_history_features = self.price_history_bn2(price_history_features)\n",
    "                price_history_features = price_history_features + price_history_res  # Residual connection\n",
    "        else:\n",
    "            # Use linear layers\n",
    "            price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "            price_history_features = self.price_history_bn1(price_history_features)\n",
    "            price_history_features = self.dropout(price_history_features)\n",
    "            price_history_res = price_history_features\n",
    "            price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "            price_history_features = self.price_history_bn2(price_history_features)\n",
    "            price_history_features = price_history_features + price_history_res  # Residual connection\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type - now including price history\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "# Function to prepare graph data\n",
    "def prepare_enhanced_graph(train_data, val_data, spatial_features, temporal_features, \n",
    "                          amenity_features, price_history_features, spatial_scaler, \n",
    "                          temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                          device, k=10, feature_weight=0.3, seq_len=30):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings using enhanced features\n",
    "    with sequential price history data for LSTM\n",
    "    \"\"\"\n",
    "    # Scale the spatial features\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features]).astype(np.float32)\n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale the temporal features\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale the amenity features\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features]).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features]).astype(np.float32)\n",
    "    \n",
    "    # Transform the target variable\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Special handling for price history to create raw time series\n",
    "    X_train_price_history = np.zeros((len(train_data), seq_len), dtype=np.float32)\n",
    "    # Fix: Use enumerate instead of iterrows to get sequential indices\n",
    "    for idx, (_, row) in enumerate(train_data.iterrows()):\n",
    "        for day in range(1, seq_len + 1):\n",
    "            col_name = f'price_day_{day}'\n",
    "            if col_name in train_data.columns and not pd.isnull(row[col_name]):\n",
    "                X_train_price_history[idx, day-1] = row[col_name]\n",
    "            else:\n",
    "                # Fill with last known value or the current price\n",
    "                X_train_price_history[idx, day-1] = row['price']\n",
    "    \n",
    "    # Do the same for validation data\n",
    "    X_val_price_history = np.zeros((len(val_data), seq_len), dtype=np.float32)\n",
    "    # Fix: Use enumerate for validation data too\n",
    "    for idx, (_, row) in enumerate(val_data.iterrows()):\n",
    "        for day in range(1, seq_len + 1):\n",
    "            col_name = f'price_day_{day}'\n",
    "            if col_name in val_data.columns and not pd.isnull(row[col_name]):\n",
    "                X_val_price_history[idx, day-1] = row[col_name]\n",
    "            else:\n",
    "                X_val_price_history[idx, day-1] = row['price']\n",
    "    \n",
    "    # Scale the time series data\n",
    "    X_train_price_history = price_history_scaler.fit_transform(X_train_price_history)\n",
    "    X_val_price_history = price_history_scaler.transform(X_val_price_history)\n",
    "    \n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Store sequence length for LSTM\n",
    "    data.seq_len = seq_len\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Train GNN model function\n",
    "def train_gnn_model(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                   price_history_features, hidden_dim=64, lstm_hidden_dim=32, lstm_layers=1, \n",
    "                   price_history_seq_len=30, epochs=50, lr=0.001, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GNN model with log-transformed prices and price history features using LSTM with reduced dimension\n",
    "    \"\"\"\n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "\n",
    "    # Check if we have price_history_features\n",
    "    if not price_history_features or len(price_history_features) == 0:\n",
    "        print(\"Warning: No price history features provided. Creating dummy feature.\")\n",
    "        price_history_features = ['dummy_price_history']\n",
    "        train_data['dummy_price_history'] = 1.0\n",
    "        val_data['dummy_price_history'] = 1.0\n",
    "\n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data with sequence length parameter\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, seq_len=price_history_seq_len\n",
    "    )\n",
    "    \n",
    "    # Initialize model with reduced LSTM parameters\n",
    "    model = EnhancedSpatioTemporalGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=len(price_history_features),\n",
    "        price_history_seq_len=price_history_seq_len,\n",
    "        hidden_dim=hidden_dim,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,  # Reduced LSTM dimension\n",
    "        lstm_layers=lstm_layers,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    # Store history for plotting\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Store history\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory management\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, history\n",
    "\n",
    "# Function to make predictions with the model\n",
    "def predict_with_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                     price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                     price_history_scaler, target_scaler, train_data, device, \n",
    "                     price_history_seq_len=30):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained GNN model with LSTM support\n",
    "    \"\"\"\n",
    "    # Create a copy of test_data to avoid modifying the original\n",
    "    test_data_copy = test_data.copy()\n",
    "    \n",
    "    # Prepare sequential features for test data if using LSTM\n",
    "    if price_history_seq_len > 1:\n",
    "        test_data_copy, _ = create_daily_price_history(test_data_copy, seq_length=price_history_seq_len)\n",
    "    \n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, test_data_copy, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device, seq_len=price_history_seq_len\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Enhanced run_gnn_with_rolling_window_cv function \n",
    "def run_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                   window_size=35, n_splits=5, sample_size=None,\n",
    "                                   price_history_seq_len=30, lstm_hidden_dim=32, lstm_layers=1):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation and reduced LSTM dimensions\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "\n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "        # Drop legacy price columns if they exist\n",
    "        price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "        \n",
    "        for col in price_cols_to_remove:\n",
    "            if col in train_data.columns:\n",
    "                print(f\"Dropping {col} column from the dataset\")\n",
    "                train_data = train_data.drop(col, axis=1)\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "        start_date = pd.to_datetime('2023-07-08')\n",
    "        end_date = pd.to_datetime('2024-02-08')\n",
    "        train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "        \n",
    "        # Apply log transformation to price\n",
    "        train_data = apply_price_transformation(train_data)\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Define feature groups based on your dataset columns\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Temporal features - using your DTF prefixed features\n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "        amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "        basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "        available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "        amenity_features.extend(available_basic_features)\n",
    "        \n",
    "        # New: Price history features - price lags and rolling statistics\n",
    "        price_history_features = [\n",
    "            'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "            'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "            'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "            'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d',\n",
    "            'price_range_7d', 'price_range_14d', 'price_range_30d'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "        amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "        price_history_features = [f for f in price_history_features if f in train_data.columns]\n",
    "        \n",
    "        # If any feature group is empty, create dummy features\n",
    "        if not amenity_features:\n",
    "            print(\"No amenity features found, creating dummy feature\")\n",
    "            train_data['dummy_amenity'] = 1\n",
    "            amenity_features = ['dummy_amenity']\n",
    "        \n",
    "        if not price_history_features:\n",
    "            print(\"No price history features found, creating dummy feature\")\n",
    "            train_data['dummy_price_history'] = 1\n",
    "            price_history_features = ['dummy_price_history']\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "        \n",
    "        # Get unique dates and ensure they're properly sorted\n",
    "        unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "        \n",
    "        # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "        last_35_days = unique_dates[-window_size:]\n",
    "        \n",
    "        # Define explicit test periods - each 7 days\n",
    "        test_periods = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = i * (window_size // n_splits)\n",
    "            end_idx = start_idx + (window_size // n_splits)\n",
    "            # Make sure we don't go beyond the available data\n",
    "            if end_idx <= len(last_35_days):\n",
    "                test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "        \n",
    "        # Adjust n_splits if we couldn't create enough test periods\n",
    "        n_splits = len(test_periods)\n",
    "        \n",
    "        print(f\"Created {n_splits} test periods:\")\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Storage for results\n",
    "        cv_results = []\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        split_metrics = []\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Run time series cross-validation using our explicit test periods\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "            \n",
    "            # Define training period: everything before test_start\n",
    "            train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "            train_end_date = train_end.date()\n",
    "            \n",
    "            print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "            print(f\"Testing period: {test_start} to {test_end}\")\n",
    "            \n",
    "            # Split by date first\n",
    "            train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "            test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "            \n",
    "            date_filtered_train = train_data[train_date_mask]\n",
    "            date_filtered_test = train_data[test_date_mask]\n",
    "            \n",
    "            # Now further split by listing IDs\n",
    "            train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "            test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "            \n",
    "            split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "            split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "            \n",
    "            print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "            print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "            \n",
    "            # Check if we have enough data for this split\n",
    "            if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "                print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Split train data into train and validation\n",
    "            unique_train_listings = split_train_data['listing_id'].unique()\n",
    "            train_listings, val_listings = train_test_split(\n",
    "                unique_train_listings, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "            val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "            \n",
    "            # Manage memory before training\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train GNN model for this split\n",
    "            try:\n",
    "                print(f\"\\n----- Training GNN Model with Reduced LSTM (Split {i+1}) -----\")\n",
    "                gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, _ = train_gnn_model(\n",
    "                    train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                    price_history_features, hidden_dim=64, lstm_hidden_dim=lstm_hidden_dim, \n",
    "                    epochs=50, lr=0.001, device=device,\n",
    "                    lstm_layers=lstm_layers, price_history_seq_len=price_history_seq_len\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test data with LSTM parameters\n",
    "                print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "                test_predictions = predict_with_gnn(\n",
    "                    gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                    price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                    price_history_scaler, target_scaler, train_subset, device,\n",
    "                    price_history_seq_len=price_history_seq_len\n",
    "                )\n",
    "                \n",
    "                # Get actual test values (original scale)\n",
    "                test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "                \n",
    "                # Evaluate predictions\n",
    "                metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "                \n",
    "                print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "                \n",
    "                # Store results for this split\n",
    "                split_results = pd.DataFrame({\n",
    "                    'split': i,\n",
    "                    'date': split_test_data['date'],\n",
    "                    'listing_id': split_test_data['listing_id'],\n",
    "                    'price': test_actuals,\n",
    "                    'predicted': test_predictions.flatten(),\n",
    "                    'error': test_actuals - test_predictions.flatten(),\n",
    "                    'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                    'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "                })\n",
    "                \n",
    "                cv_results.append(split_results)\n",
    "                all_predictions.extend(test_predictions.flatten())\n",
    "                all_targets.extend(test_actuals)\n",
    "                \n",
    "                # Save model for this split if output_dir is provided\n",
    "                if output_dir:\n",
    "                    model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                    torch.save(gnn_model.state_dict(), model_path)\n",
    "                    print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "                \n",
    "                # Store metrics for this split\n",
    "                split_metrics.append({\n",
    "                    'split': i,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mae': metrics['mae'],\n",
    "                    'r2': metrics['r2'],\n",
    "                    'mape': metrics['mape'],\n",
    "                    'n_samples': len(test_actuals)\n",
    "                })\n",
    "                \n",
    "                # Memory management after each split\n",
    "                del gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "                del test_predictions, split_train_data, split_test_data, train_subset, val_subset\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in split {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if not cv_results:\n",
    "            print(\"No valid splits completed. Check your data and parameters.\")\n",
    "            return None\n",
    "                \n",
    "        all_results = pd.concat(cv_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        all_targets_array = np.array(all_targets)\n",
    "        all_predictions_array = np.array(all_predictions)\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "            'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "            'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "            'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "        }\n",
    "        \n",
    "        # Calculate daily metrics\n",
    "        all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        daily_metrics = []\n",
    "        for day, group in all_results.groupby('date_str'):\n",
    "            y_true_day = group['price']\n",
    "            y_pred_day = group['predicted']\n",
    "            \n",
    "            daily_metrics.append({\n",
    "                'date': day,\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "                'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "                'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "                'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "                'n_samples': len(y_true_day)\n",
    "            })\n",
    "        \n",
    "        daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "        daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "        daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "        \n",
    "        split_metrics_df = pd.DataFrame(split_metrics)\n",
    "        \n",
    "        # Create a results dictionary\n",
    "        evaluation_results = {\n",
    "            'overall_metrics': overall_metrics,\n",
    "            'split_metrics': split_metrics_df,\n",
    "            'daily_metrics': daily_metrics_df,\n",
    "            'all_results': all_results,\n",
    "            'train_listings': len(train_listing_ids),\n",
    "            'test_listings': len(test_listing_ids)\n",
    "        }\n",
    "        \n",
    "        # Save results if output directory is provided\n",
    "        if output_dir:\n",
    "            # Save all results\n",
    "            results_file = os.path.join(output_dir, 'gnn_rolling_window_results.csv')\n",
    "            all_results.to_csv(results_file, index=False)\n",
    "            print(f\"Results saved to {results_file}\")\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics_file = os.path.join(output_dir, 'gnn_rolling_window_metrics.csv')\n",
    "            daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "            print(f\"Daily metrics saved to {metrics_file}\")\n",
    "            \n",
    "            # Save summary\n",
    "            with open(os.path.join(output_dir, 'gnn_cv_summary.txt'), 'w') as f:\n",
    "                f.write(f\"GNN Rolling Window CV Model Summary\\n\")\n",
    "                f.write(f\"=================================\\n\\n\")\n",
    "                f.write(f\"Window size: {window_size} days\\n\")\n",
    "                f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "                f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "                f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "                f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "                f.write(f\"LSTM hidden dimension: {lstm_hidden_dim}\\n\")\n",
    "                f.write(f\"LSTM layers: {lstm_layers}\\n\\n\")\n",
    "                f.write(f\"Overall Metrics:\\n\")\n",
    "                for k, v in overall_metrics.items():\n",
    "                    f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "        print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "        \n",
    "        print(\"\\n=== Overall Metrics ===\")\n",
    "        print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "        print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "        \n",
    "        print(\"\\n=== Split Performance ===\")\n",
    "        print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "        \n",
    "        # Return evaluation results\n",
    "        return evaluation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in rolling window CV: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to plot results\n",
    "def plot_gnn_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot GNN prediction results\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    plt.annotate(f'Correlation: {corr:.4f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add mean, median error\n",
    "    mean_error = np.mean(errors)\n",
    "    median_error = np.median(errors)\n",
    "    plt.axvline(mean_error, color='g', linestyle='-', label=f'Mean: {mean_error:.2f}')\n",
    "    plt.axvline(median_error, color='b', linestyle='-', label=f'Median: {median_error:.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Add lowess trend line if statsmodels is available\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        lowess = sm.nonparametric.lowess(errors, y_true, frac=0.3)\n",
    "        plt.plot(lowess[:, 0], lowess[:, 1], 'r-', linewidth=2, label='Trend')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    \n",
    "    # Mark median and mean\n",
    "    median_pct = np.median(pct_errors)\n",
    "    mean_pct = np.mean(pct_errors)\n",
    "    plt.axvline(median_pct, color='r', linestyle='--', label=f'Median: {median_pct:.2f}%')\n",
    "    plt.axvline(mean_pct, color='g', linestyle='--', label=f'Mean: {mean_pct:.2f}%')\n",
    "    \n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'gnn_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'gnn_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Main function with optimized LSTM parameters\n",
    "def run_strap_with_gnn(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None, \n",
    "                      price_history_seq_len=30, lstm_hidden_dim=32, lstm_layers=1, \n",
    "                      hidden_dim=64, lr=0.001, epochs=50):\n",
    "    \"\"\"\n",
    "    Run enhanced STRAP model with GNN, log-transformed prices, and reduced LSTM dimension\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if not exists\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "\n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime if needed\n",
    "        if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "            train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Split data into train and test based on listing IDs\n",
    "        train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "        test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        train_df = train_data[train_mask].copy()\n",
    "        test_df = train_data[test_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Prepare sequential price features for LSTM\n",
    "        print(\"Preparing sequential price features for LSTM...\")\n",
    "        train_df, price_history_features = create_daily_price_history(train_df, seq_length=price_history_seq_len)\n",
    "        test_df, _ = create_daily_price_history(test_df, seq_length=price_history_seq_len)\n",
    "\n",
    "        # Define feature groups based on your dataset columns\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Temporal features - using your DTF prefixed features\n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "        amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "        basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "        available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "        amenity_features.extend(available_basic_features)\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "        amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "        price_history_features = [f for f in price_history_features if f in train_df.columns]\n",
    "        \n",
    "        # If any feature group is empty, create dummy features\n",
    "        if not amenity_features:\n",
    "            print(\"No amenity features found, creating dummy feature\")\n",
    "            train_df['dummy_amenity'] = 1\n",
    "            test_df['dummy_amenity'] = 1\n",
    "            amenity_features = ['dummy_amenity']\n",
    "        \n",
    "        if not price_history_features:\n",
    "            print(\"No price history features found, creating dummy feature\")\n",
    "            train_df['dummy_price_history'] = 1\n",
    "            test_df['dummy_price_history'] = 1\n",
    "            price_history_features = ['dummy_price_history']\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "        \n",
    "        # Apply log transformation to prices\n",
    "        train_df = apply_price_transformation(train_df)\n",
    "        test_df = apply_price_transformation(test_df)\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = train_df['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "        print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Memory management before training\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train GNN model with reduced LSTM dimension\n",
    "        print(\"\\n===== Training GNN Model with Reduced LSTM Dimension =====\")\n",
    "        gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, history = train_gnn_model(\n",
    "            train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "            price_history_features, hidden_dim=hidden_dim, lstm_hidden_dim=lstm_hidden_dim, \n",
    "            lstm_layers=lstm_layers, price_history_seq_len=price_history_seq_len, \n",
    "            epochs=epochs, lr=lr, device=device\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation RMSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['val_rmse'], label='Validation RMSE')\n",
    "        plt.title('Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        \n",
    "        # Plot validation MAE\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        # Plot learning rate\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['lr'], label='Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('LR')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        print(\"\\n===== Evaluating GNN on Test Data =====\")\n",
    "        test_predictions = predict_with_gnn(\n",
    "            gnn_model, test_df, spatial_features, temporal_features, amenity_features,\n",
    "            price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "            price_history_scaler, target_scaler, train_subset, device,\n",
    "            price_history_seq_len=price_history_seq_len\n",
    "        )\n",
    "        \n",
    "        # Get actual test values (original scale)\n",
    "        test_actuals = test_df['original_price'].values if 'original_price' in test_df.columns else test_df['price'].values\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        test_metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_gnn_results(test_actuals, test_predictions.flatten(), history, output_dir)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        if output_dir:\n",
    "            torch.save(gnn_model.state_dict(), os.path.join(output_dir, 'gnn_model.pt'))\n",
    "            torch.save({\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'amenity_scaler': amenity_scaler,\n",
    "                'price_history_scaler': price_history_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'price_history_seq_len': price_history_seq_len,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim\n",
    "            }, os.path.join(output_dir, 'scalers.pt'))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "            # Save test predictions\n",
    "            test_results = pd.DataFrame({\n",
    "                'listing_id': test_df['listing_id'].values,\n",
    "                'date': test_df['date'].values,\n",
    "                'actual': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            test_results.to_csv(os.path.join(output_dir, 'test_predictions.csv'), index=False)\n",
    "            print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.csv')}\")\n",
    "        \n",
    "        # Return model and scalers\n",
    "        return gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, test_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GNN model training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to compare original and reduced LSTM models\n",
    "def compare_lstm_models(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Compare the original and reduced LSTM models\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set parameters for each model\n",
    "    orig_lstm_dim = 64   # Original LSTM hidden dimension\n",
    "    reduced_lstm_dim = 32  # Reduced LSTM hidden dimension\n",
    "    price_history_seq_len = 30\n",
    "    \n",
    "    # Run original model\n",
    "    print(\"\\n===== Running Original LSTM Model (64 hidden dim) =====\")\n",
    "    result_orig = run_strap_with_gnn(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        output_dir=os.path.join(output_dir, 'original_lstm') if output_dir else None,\n",
    "        sample_size=sample_size,\n",
    "        price_history_seq_len=price_history_seq_len,\n",
    "        lstm_hidden_dim=orig_lstm_dim,\n",
    "        lstm_layers=1,\n",
    "        epochs=30  # Reduced epochs for faster comparison\n",
    "    )\n",
    "    \n",
    "    # Run reduced model\n",
    "    print(\"\\n===== Running Reduced LSTM Model (32 hidden dim) =====\")\n",
    "    result_reduced = run_strap_with_gnn(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        output_dir=os.path.join(output_dir, 'reduced_lstm') if output_dir else None,\n",
    "        sample_size=sample_size,\n",
    "        price_history_seq_len=price_history_seq_len,\n",
    "        lstm_hidden_dim=reduced_lstm_dim,\n",
    "        lstm_layers=1,\n",
    "        epochs=30  # Reduced epochs for faster comparison\n",
    "    )\n",
    "    \n",
    "    # Check if both models ran successfully\n",
    "    if result_orig and result_reduced:\n",
    "        # Extract model performance\n",
    "        _, _, _, _, _, _, metrics_orig = result_orig\n",
    "        _, _, _, _, _, _, metrics_reduced = result_reduced\n",
    "        \n",
    "        # Compare metrics\n",
    "        print(\"\\n===== Model Comparison =====\")\n",
    "        metrics = ['rmse', 'mae', 'r2', 'mape']\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Metric': metrics,\n",
    "            f'Original LSTM (dim={orig_lstm_dim})': [metrics_orig[m] for m in metrics],\n",
    "            f'Reduced LSTM (dim={reduced_lstm_dim})': [metrics_reduced[m] for m in metrics]\n",
    "        })\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        comparison_df['Improvement %'] = [\n",
    "            ((metrics_reduced[m] - metrics_orig[m]) / metrics_orig[m] * 100) if m not in ['r2'] else\n",
    "            ((metrics_reduced[m] - metrics_orig[m]) * 100) for m in metrics\n",
    "        ]\n",
    "        \n",
    "        print(comparison_df)\n",
    "        \n",
    "        # Save comparison if output_dir is provided\n",
    "        if output_dir:\n",
    "            comparison_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
    "            print(f\"Model comparison saved to {os.path.join(output_dir, 'model_comparison.csv')}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    else:\n",
    "        print(\"One or both models failed to run. Check error logs.\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\" \n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/reduced_lstm_gnn_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters with reduced LSTM dimension\n",
    "    price_history_seq_len = 30  # Sequence length for LSTM\n",
    "    lstm_hidden_dim = 8        # Reduced LSTM hidden dimension (half of original)\n",
    "    lstm_layers = 1             # Number of LSTM layers\n",
    "    hidden_dim = 64             # Hidden dimension size for rest of model\n",
    "    lr = 0.001                  # Learning rate \n",
    "    epochs = 20                 # Maximum number of epochs\n",
    "    \n",
    "    # Choose between different run modes\n",
    "    run_mode = \"rolling_window\"  # Options: \"single\", \"rolling_window\", \"compare\"\n",
    "    \n",
    "    try:\n",
    "        if run_mode == \"single\":\n",
    "            # Run single model training with reduced LSTM\n",
    "            result_tuple = run_strap_with_gnn(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                price_history_seq_len=price_history_seq_len,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                hidden_dim=hidden_dim,\n",
    "                lr=lr,\n",
    "                epochs=epochs,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            \n",
    "            if result_tuple:\n",
    "                gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler, test_metrics = result_tuple\n",
    "                print(\"GNN model with reduced LSTM dimension training completed successfully!\")\n",
    "                \n",
    "                # Print summary of reduced memory usage\n",
    "                print(\"\\n===== Memory Usage Summary =====\")\n",
    "                total_param = sum(p.numel() for p in gnn_model.parameters())\n",
    "                lstm_param = sum(p.numel() for name, p in gnn_model.named_parameters() if 'lstm' in name)\n",
    "                print(f\"Total model parameters: {total_param:,}\")\n",
    "                print(f\"LSTM parameters: {lstm_param:,}\")\n",
    "                print(f\"LSTM parameters as % of total: {lstm_param/total_param*100:.2f}%\")\n",
    "                \n",
    "        elif run_mode == \"rolling_window\":\n",
    "            # Run with rolling window cross-validation\n",
    "            results = run_gnn_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                price_history_seq_len=price_history_seq_len,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                sample_size=200  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            print(\"GNN model with reduced LSTM training with rolling window CV completed successfully!\")\n",
    "            \n",
    "        elif run_mode == \"compare\":\n",
    "            # Run comparison between original and reduced LSTM models\n",
    "            comparison = compare_lstm_models(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                sample_size=200  # Use small sample for faster comparison\n",
    "            )\n",
    "            print(\"Model comparison completed successfully!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
