{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 903142 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training GNN Model (Split 1) -----\n",
      "Building enhanced spatial graph with 180551 test listings and 10 nearest neighbors...\n",
      "Created graph with 3611020 edges\n",
      "Created enhanced graph with 903142 nodes and 3611020 edges\n",
      "Train nodes: 722591, Val nodes: 180551\n",
      "Epoch 1/30 - Loss: 0.6008, Val Loss: 0.3970, RMSE: 191.29, MAE: 112.96\n",
      "Epoch 2/30 - Loss: 0.3648, Val Loss: 0.3471, RMSE: 182.09, MAE: 105.85\n",
      "Epoch 3/30 - Loss: 0.2701, Val Loss: 0.3071, RMSE: 173.47, MAE: 99.56\n",
      "Epoch 4/30 - Loss: 0.2589, Val Loss: 0.2866, RMSE: 168.25, MAE: 95.90\n",
      "Epoch 5/30 - Loss: 0.2462, Val Loss: 0.2806, RMSE: 166.10, MAE: 94.49\n",
      "Epoch 6/30 - Loss: 0.2241, Val Loss: 0.2842, RMSE: 166.22, MAE: 94.70\n",
      "Epoch 7/30 - Loss: 0.2049, Val Loss: 0.2896, RMSE: 166.82, MAE: 95.26\n",
      "Epoch 8/30 - Loss: 0.1963, Val Loss: 0.2892, RMSE: 166.12, MAE: 94.87\n",
      "Epoch 9/30 - Loss: 0.1928, Val Loss: 0.2792, RMSE: 163.19, MAE: 92.82\n",
      "Epoch 10/30 - Loss: 0.1888, Val Loss: 0.2595, RMSE: 157.83, MAE: 89.00\n",
      "Epoch 11/30 - Loss: 0.1814, Val Loss: 0.2332, RMSE: 150.36, MAE: 83.67\n",
      "Epoch 12/30 - Loss: 0.1732, Val Loss: 0.2039, RMSE: 141.38, MAE: 77.32\n",
      "Epoch 13/30 - Loss: 0.1668, Val Loss: 0.1757, RMSE: 131.83, MAE: 70.62\n",
      "Epoch 14/30 - Loss: 0.1602, Val Loss: 0.1515, RMSE: 122.81, MAE: 64.28\n",
      "Epoch 15/30 - Loss: 0.1550, Val Loss: 0.1324, RMSE: 115.13, MAE: 58.84\n",
      "Epoch 16/30 - Loss: 0.1500, Val Loss: 0.1182, RMSE: 109.25, MAE: 54.57\n",
      "Epoch 17/30 - Loss: 0.1452, Val Loss: 0.1078, RMSE: 105.07, MAE: 51.41\n",
      "Epoch 18/30 - Loss: 0.1413, Val Loss: 0.0999, RMSE: 101.98, MAE: 48.98\n",
      "Epoch 19/30 - Loss: 0.1380, Val Loss: 0.0929, RMSE: 99.22, MAE: 46.85\n",
      "Epoch 20/30 - Loss: 0.1352, Val Loss: 0.0859, RMSE: 96.15, MAE: 44.64\n",
      "Epoch 21/30 - Loss: 0.1322, Val Loss: 0.0785, RMSE: 92.35, MAE: 42.17\n",
      "Epoch 22/30 - Loss: 0.1290, Val Loss: 0.0706, RMSE: 87.72, MAE: 39.42\n",
      "Epoch 23/30 - Loss: 0.1256, Val Loss: 0.0628, RMSE: 82.56, MAE: 36.61\n",
      "Epoch 24/30 - Loss: 0.1224, Val Loss: 0.0558, RMSE: 77.39, MAE: 34.08\n",
      "Epoch 25/30 - Loss: 0.1199, Val Loss: 0.0499, RMSE: 72.85, MAE: 32.14\n",
      "Epoch 26/30 - Loss: 0.1174, Val Loss: 0.0455, RMSE: 69.40, MAE: 30.94\n",
      "Epoch 27/30 - Loss: 0.1150, Val Loss: 0.0423, RMSE: 67.16, MAE: 30.35\n",
      "Epoch 28/30 - Loss: 0.1124, Val Loss: 0.0400, RMSE: 65.88, MAE: 30.13\n",
      "Epoch 29/30 - Loss: 0.1101, Val Loss: 0.0385, RMSE: 65.29, MAE: 30.10\n",
      "Epoch 30/30 - Loss: 0.1080, Val Loss: 0.0374, RMSE: 65.10, MAE: 30.16\n",
      "\n",
      "----- Evaluating GNN on Test Data (Split 1) -----\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Created enhanced graph with 733602 nodes and 220220 edges\n",
      "Train nodes: 722591, Val nodes: 11011\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 67.57\n",
      "MAE: 40.14\n",
      "R²: 0.8243\n",
      "MAPE: 20.68%\n",
      "Split 1 Results - RMSE: 67.5674, MAE: 40.1443, R²: 0.8243\n",
      "Model for split 1 saved to ./output/gnn_model\\gnn_model_split_1.pt\n",
      "\n",
      "===== Split 2/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 947179 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training GNN Model (Split 2) -----\n",
      "Building enhanced spatial graph with 189364 test listings and 10 nearest neighbors...\n",
      "Created graph with 3787280 edges\n",
      "Created enhanced graph with 947179 nodes and 3787280 edges\n",
      "Train nodes: 757815, Val nodes: 189364\n",
      "Epoch 1/30 - Loss: 0.6969, Val Loss: 0.4514, RMSE: 189.22, MAE: 119.09\n",
      "Epoch 2/30 - Loss: 0.4500, Val Loss: 0.4035, RMSE: 180.42, MAE: 111.81\n",
      "Epoch 3/30 - Loss: 0.3025, Val Loss: 0.3517, RMSE: 169.42, MAE: 103.45\n",
      "Epoch 4/30 - Loss: 0.2526, Val Loss: 0.3110, RMSE: 159.29, MAE: 96.23\n",
      "Epoch 5/30 - Loss: 0.2485, Val Loss: 0.2866, RMSE: 152.45, MAE: 91.46\n",
      "Epoch 6/30 - Loss: 0.2415, Val Loss: 0.2748, RMSE: 149.03, MAE: 88.96\n",
      "Epoch 7/30 - Loss: 0.2241, Val Loss: 0.2719, RMSE: 148.47, MAE: 88.26\n",
      "Epoch 8/30 - Loss: 0.2084, Val Loss: 0.2719, RMSE: 149.14, MAE: 88.25\n",
      "Epoch 9/30 - Loss: 0.1988, Val Loss: 0.2687, RMSE: 149.11, MAE: 87.69\n",
      "Epoch 10/30 - Loss: 0.1948, Val Loss: 0.2581, RMSE: 147.07, MAE: 85.70\n",
      "Epoch 11/30 - Loss: 0.1896, Val Loss: 0.2395, RMSE: 142.55, MAE: 81.99\n",
      "Epoch 12/30 - Loss: 0.1822, Val Loss: 0.2144, RMSE: 135.58, MAE: 76.68\n",
      "Epoch 13/30 - Loss: 0.1729, Val Loss: 0.1860, RMSE: 126.60, MAE: 70.14\n",
      "Epoch 14/30 - Loss: 0.1647, Val Loss: 0.1580, RMSE: 116.53, MAE: 63.05\n",
      "Epoch 15/30 - Loss: 0.1582, Val Loss: 0.1336, RMSE: 106.60, MAE: 56.25\n",
      "Epoch 16/30 - Loss: 0.1546, Val Loss: 0.1145, RMSE: 98.12, MAE: 50.56\n",
      "Epoch 17/30 - Loss: 0.1513, Val Loss: 0.1008, RMSE: 91.98, MAE: 46.47\n",
      "Epoch 18/30 - Loss: 0.1482, Val Loss: 0.0917, RMSE: 88.39, MAE: 43.96\n",
      "Epoch 19/30 - Loss: 0.1442, Val Loss: 0.0859, RMSE: 86.96, MAE: 42.65\n",
      "Epoch 20/30 - Loss: 0.1394, Val Loss: 0.0820, RMSE: 86.94, MAE: 42.06\n",
      "Epoch 21/30 - Loss: 0.1357, Val Loss: 0.0789, RMSE: 87.48, MAE: 41.72\n",
      "Epoch 22/30 - Loss: 0.1326, Val Loss: 0.0754, RMSE: 87.73, MAE: 41.24\n",
      "Epoch 23/30 - Loss: 0.1302, Val Loss: 0.0709, RMSE: 87.06, MAE: 40.33\n",
      "Epoch 24/30 - Loss: 0.1279, Val Loss: 0.0652, RMSE: 85.14, MAE: 38.89\n",
      "Epoch 25/30 - Loss: 0.1249, Val Loss: 0.0585, RMSE: 81.95, MAE: 37.01\n",
      "Epoch 26/30 - Loss: 0.1218, Val Loss: 0.0518, RMSE: 77.92, MAE: 34.99\n",
      "Epoch 27/30 - Loss: 0.1192, Val Loss: 0.0458, RMSE: 73.66, MAE: 33.16\n",
      "Epoch 28/30 - Loss: 0.1160, Val Loss: 0.0410, RMSE: 69.88, MAE: 31.82\n",
      "Epoch 29/30 - Loss: 0.1139, Val Loss: 0.0378, RMSE: 67.13, MAE: 31.05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d'\n",
    "    ]\n",
    "\n",
    "# 1. Price transformation function\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        The dataframe containing price data\n",
    "    inverse : bool\n",
    "        If True, apply inverse transformation; otherwise apply log transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with transformed prices\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Create calculated features (reuse from your existing code)\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataframe to add features to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with new features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 3. Build enhanced spatial graph for GNN\n",
    "def build_enhanced_spatial_graph(train_data, test_data, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (optional, can help propagate information)\n",
    "    if len(train_coords) <= 5000:  # Only for smaller datasets to avoid memory issues\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "# 4. Enhanced GNN model\n",
    "class EnhancedSpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 price_history_features_dim,  # New parameter for price history features\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(EnhancedSpatioTemporalGNN, self).__init__()\n",
    "        \n",
    "        # For multi-head attention, ensure hidden_dim is divisible by heads\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # Important: Make sure the output dimension is consistent\n",
    "        gat_out_dim = self.head_dim * heads  # This ensures exact dimensions\n",
    "        \n",
    "        # Replace GCN with GAT for better spatial relationship modeling\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Add batch normalization for more stable training - USING EXACT OUTPUT DIMENSIONS\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Enhanced temporal processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Enhanced amenity processing with residual connection\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # New: Price history features processing\n",
    "        self.price_history_layer1 = nn.Linear(price_history_features_dim, hidden_dim)\n",
    "        self.price_history_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.price_history_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.price_history_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - updated for 4 feature types\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object - updated to include price_history_x\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, price_history_x = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.price_history_x\n",
    "        )\n",
    "        \n",
    "        # First GAT layer with batch normalization and residual\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features with enhanced layers\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res  # Residual connection\n",
    "        \n",
    "        # Process amenity features with residual connection\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process price history features with residual connection\n",
    "        price_history_features = F.elu(self.price_history_layer1(price_history_x))\n",
    "        price_history_features = self.price_history_bn1(price_history_features)\n",
    "        price_history_features = self.dropout(price_history_features)\n",
    "        price_history_res = price_history_features\n",
    "        price_history_features = F.elu(self.price_history_layer2(price_history_features))\n",
    "        price_history_features = self.price_history_bn2(price_history_features)\n",
    "        price_history_features = price_history_features + price_history_res  # Residual connection\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type - now including price history\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            price_history_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "# 5. Function to prepare graph data\n",
    "def prepare_enhanced_graph(train_data, val_data, spatial_features, temporal_features, \n",
    "                          amenity_features, price_history_features, spatial_scaler, \n",
    "                          temporal_scaler, amenity_scaler, price_history_scaler, target_scaler,\n",
    "                          device, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with both train and validation listings using enhanced features\n",
    "    \"\"\"\n",
    "    # Scale features and convert to float32\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features].values).astype(np.float32)\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features].values).astype(np.float32)\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features].values).astype(np.float32)\n",
    "    X_train_price_history = price_history_scaler.transform(train_data[price_history_features].values).astype(np.float32)\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features].values).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features].values).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features].values).astype(np.float32)\n",
    "    X_val_price_history = price_history_scaler.transform(val_data[price_history_features].values).astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_price_history = np.vstack([X_train_price_history, X_val_price_history])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph with feature similarity\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        price_history_x=torch.FloatTensor(X_combined_price_history).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    print(f\"Created enhanced graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 6. Function to train GNN model\n",
    "def train_gnn_model(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                   price_history_features, hidden_dim=64, epochs=50, lr=0.001, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GNN model with log-transformed prices and price history features\n",
    "    \"\"\"\n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    price_history_scaler = StandardScaler()  # New scaler for price history\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    price_history_scaler.fit(train_data[price_history_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedSpatioTemporalGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        price_history_features_dim=len(price_history_features),\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# 7. Function to make predictions with GNN\n",
    "def predict_with_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                     price_history_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                     price_history_scaler, target_scaler, train_data, device):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained GNN model\n",
    "    \"\"\"\n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data = prepare_enhanced_graph(\n",
    "        train_data, test_data, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "        price_history_scaler, target_scaler, device\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "# 8. Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 9. Function to plot results\n",
    "def plot_gnn_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot GNN prediction results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(np.median(pct_errors), color='r', linestyle='--', \n",
    "              label=f'Median: {np.median(pct_errors):.2f}%')\n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'gnn_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'gnn_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 10. Main function to run the GNN model\n",
    "def run_strap_with_gnn(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run enhanced STRAP model with GNN, log-transformed prices, and price history features\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Split data into train and test based on listing IDs\n",
    "    train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "    test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "    \n",
    "    train_df = train_data[train_mask].copy()\n",
    "    test_df = train_data[test_mask].copy()\n",
    "    \n",
    "    print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "    print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "    \n",
    "    # Define feature groups based on your dataset columns\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # New: Price history features - price lags and rolling statistics\n",
    "    price_history_features = [\n",
    "        'price_lag_7d', 'price_lag_14d', 'price_lag_30d',\n",
    "        'rolling_mean_7d', 'rolling_max_7d', 'rolling_min_7d',\n",
    "        'rolling_mean_14d', 'rolling_max_14d', 'rolling_min_14d',\n",
    "        'rolling_mean_30d', 'rolling_max_30d', 'rolling_min_30d'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "    price_history_features = [f for f in price_history_features if f in train_df.columns]\n",
    "    \n",
    "    # If any feature group is empty, create dummy features\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_df['dummy_amenity'] = 1\n",
    "        test_df['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    if not price_history_features:\n",
    "        print(\"No price history features found, creating dummy feature\")\n",
    "        train_df['dummy_price_history'] = 1\n",
    "        test_df['dummy_price_history'] = 1\n",
    "        price_history_features = ['dummy_price_history']\n",
    "    \n",
    "    print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "          f\"{len(amenity_features)} amenity features, and {len(price_history_features)} price history features\")\n",
    "    \n",
    "    # Apply log transformation to prices\n",
    "    train_df = apply_price_transformation(train_df)\n",
    "    test_df = apply_price_transformation(test_df)\n",
    "    \n",
    "    # Split train data into train and validation\n",
    "    unique_train_listings = train_df['listing_id'].unique()\n",
    "    train_listings, val_listings = train_test_split(\n",
    "        unique_train_listings, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "    val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "    \n",
    "    print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "    print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Train GNN model\n",
    "    print(\"\\n===== Training GNN Model =====\")\n",
    "    gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "        train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "        price_history_features, hidden_dim=64, epochs=30, lr=0.001, device=device\n",
    "    )\n",
    "    \n",
    "    # Return model and scalers to be used for predictions\n",
    "    return gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler\n",
    "\n",
    "# Add this function to implement rolling window CV for the GNN model\n",
    "def run_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                   window_size=35, n_splits=5, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run GNN model with rolling window cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    # Drop legacy price columns if they exist\n",
    "    price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "    \n",
    "    for col in price_cols_to_remove:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping {col} column from the dataset\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        np.random.seed(42)\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Apply log transformation to price\n",
    "    train_data = apply_price_transformation(train_data)\n",
    "    \n",
    "    # Create calculated features\n",
    "    print(\"Creating calculated features...\")\n",
    "    train_data = create_calculated_features(train_data)\n",
    "    \n",
    "    # Check for NaN values in the dataset and fill them\n",
    "    nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "        print(\"Filling NaN values with column means/medians\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                # Fill with median for numeric columns\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "            else:\n",
    "                # For non-numeric, fill with mode\n",
    "                train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "    \n",
    "    # Define feature groups for your specific dataset\n",
    "    spatial_features = [\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Temporal features - using your DTF prefixed features\n",
    "    temporal_features = [\n",
    "        'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "        'DTF_season_sin', 'DTF_season_cos'\n",
    "    ]\n",
    "    \n",
    "    # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "    amenity_features.extend(available_basic_features)\n",
    "    \n",
    "    # Ensure all feature lists only contain columns that exist in the dataset\n",
    "    spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "    temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "    amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "    \n",
    "    # If amenity features is empty, create a dummy feature\n",
    "    if not amenity_features:\n",
    "        print(\"No amenity features found, creating dummy feature\")\n",
    "        train_data['dummy_amenity'] = 1\n",
    "        amenity_features = ['dummy_amenity']\n",
    "    \n",
    "    # Get unique dates and ensure they're properly sorted\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    \n",
    "    # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "    last_35_days = unique_dates[-35:]\n",
    "    \n",
    "    # Define explicit test periods - each 7 days\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * 7\n",
    "        end_idx = start_idx + 7\n",
    "        # Make sure we don't go beyond the available data\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    # Adjust n_splits if we couldn't create enough test periods\n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    split_metrics = []\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "        split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Check if we have enough data for this split\n",
    "        if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "            print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = split_train_data['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        # Train GNN model for this split\n",
    "        try:\n",
    "            print(f\"\\n----- Training GNN Model (Split {i+1}) -----\")\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = train_gnn_model(\n",
    "                train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, hidden_dim=64, epochs=30, lr=0.001, device=device\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            print(f\"\\n----- Evaluating GNN on Test Data (Split {i+1}) -----\")\n",
    "            test_predictions = predict_with_gnn(\n",
    "                gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                price_history_features, spatial_scaler, temporal_scaler, amenity_scaler,\n",
    "                price_history_scaler, target_scaler, train_subset, device\n",
    "            )\n",
    "            \n",
    "            # Get actual test values (original scale)\n",
    "            test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "            \n",
    "            print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "            \n",
    "            # Store results for this split\n",
    "            split_results = pd.DataFrame({\n",
    "                'split': i,\n",
    "                'date': split_test_data['date'],\n",
    "                'listing_id': split_test_data['listing_id'],\n",
    "                'price': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            cv_results.append(split_results)\n",
    "            all_predictions.extend(test_predictions.flatten())\n",
    "            all_targets.extend(test_actuals)\n",
    "            \n",
    "            # Save model for this split if output_dir is provided\n",
    "            if output_dir:\n",
    "                model_path = os.path.join(output_dir, f'gnn_model_split_{i+1}.pt')\n",
    "                torch.save(gnn_model.state_dict(), model_path)\n",
    "                print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "            \n",
    "            # Store metrics for this split\n",
    "            split_metrics.append({\n",
    "                'split': i,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape'],\n",
    "                'n_samples': len(test_actuals)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in split {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if not cv_results:\n",
    "        print(\"No valid splits completed. Check your data and parameters.\")\n",
    "        return None\n",
    "        \n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_targets_array = np.array(all_targets)\n",
    "    all_predictions_array = np.array(all_predictions)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "        'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "        'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "        'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids)\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'gnn_rolling_window_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'gnn_rolling_window_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'gnn_cv_summary.txt'), 'w') as f:\n",
    "            f.write(f\"GNN Rolling Window CV Model Summary\\n\")\n",
    "            f.write(f\"=================================\\n\\n\")\n",
    "            f.write(f\"Window size: {window_size} days\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics:\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "    print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "    print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_gnn_rolling_window_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Function to plot rolling window results\n",
    "def plot_gnn_rolling_window_results(evaluation_results):\n",
    "    \"\"\"Plot the results from GNN rolling window cross-validation\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Add title\n",
    "    fig.suptitle('GNN Model Evaluation with Rolling Window CV', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Cross-validation splits performance\n",
    "    splits_x = splits['split']\n",
    "    metrics_to_plot = ['rmse', 'mae']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        sns.lineplot(\n",
    "            x=splits_x,\n",
    "            y=splits[metric],\n",
    "            marker='o',\n",
    "            label=metric.upper(),\n",
    "            ax=axes[0, 1]\n",
    "        )\n",
    "    \n",
    "    axes[0, 1].set_title('Performance Across CV Splits')\n",
    "    axes[0, 1].set_xlabel('CV Split')\n",
    "    axes[0, 1].set_ylabel('Error Metric')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted (colored by split)\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['price'],\n",
    "        all_results['predicted'],\n",
    "        c=all_results['split'],\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    min_val = min(all_results['price'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['price'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by CV Split)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('CV Split')\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: Performance by listing ID count\n",
    "    # Group by listing ID and calculate average absolute error for each listing\n",
    "    listing_errors = all_results.groupby('listing_id')['abs_error'].mean().reset_index()\n",
    "    listing_errors = listing_errors.sort_values('abs_error')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(listing_errors)), listing_errors['abs_error'], alpha=0.6)\n",
    "    plt.axhline(y=listing_errors['abs_error'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {listing_errors[\"abs_error\"].mean():.2f}')\n",
    "    plt.title('Average Absolute Error by Listing')\n",
    "    plt.xlabel('Listing Index (sorted by error)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\"\n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/gnn_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Choose between regular training or rolling window CV\n",
    "    use_rolling_window = True  # Set to True to use rolling window CV\n",
    "    \n",
    "    try:\n",
    "        if use_rolling_window:\n",
    "            # Run with rolling window cross-validation\n",
    "            results = run_gnn_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                sample_size=None  # Use full dataset, set to a number for testing\n",
    "            )\n",
    "            print(\"GNN model training with rolling window CV completed successfully!\")\n",
    "        else:\n",
    "            # Run standard GNN model training\n",
    "            gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, price_history_scaler, target_scaler = run_strap_with_gnn(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                sample_size=None  # Use full dataset, set to a number for testing\n",
    "            )\n",
    "            print(\"GNN model training completed successfully!\")\n",
    "            \n",
    "            # Save model and scalers\n",
    "            torch.save(gnn_model.state_dict(), os.path.join(output_dir, \"gnn_model.pt\"))\n",
    "            torch.save({\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'amenity_scaler': amenity_scaler,\n",
    "                'price_history_scaler': price_history_scaler,\n",
    "                'target_scaler': target_scaler\n",
    "            }, os.path.join(output_dir, \"scalers.pt\"))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
