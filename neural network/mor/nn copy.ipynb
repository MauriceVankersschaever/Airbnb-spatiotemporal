{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 GPUs:\n",
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "Selecting NVIDIA GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda:0\n",
      "\n",
      "Preparing data...\n",
      "\n",
      "Starting data preparation...\n",
      "Loading listings data...\n",
      "Loading calendar data...\n",
      "\n",
      "Initial data statistics:\n",
      "Total listings: 500\n",
      "Total records: 339131\n",
      "\n",
      "Processing dates...\n",
      "Cleaning price data...\n",
      "Missing prices: 0\n",
      "Sorting data...\n",
      "\n",
      "Creating features...\n",
      "Adding listing features...\n",
      "\n",
      "Scaling features...\n",
      "Scaling targets...\n",
      "\n",
      "Converting to tensors...\n",
      "\n",
      "Tensor shapes:\n",
      "Features: torch.Size([339131, 8])\n",
      "Targets: torch.Size([339131, 1])\n",
      "\n",
      "Checking data continuity...\n",
      "Warning: Data has gaps between dates\n",
      "\n",
      "Dataset created with 339095 samples\n",
      "Lookback: 30, Forecast horizon: 7\n",
      "\n",
      "Creating data loaders...\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - 52.6s\n",
      "Train Loss: 0.1398, Val Loss: 0.1057\n",
      "New best model saved (Val Loss: 0.1057)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] - 52.6s\n",
      "Train Loss: 0.0590, Val Loss: 0.0511\n",
      "New best model saved (Val Loss: 0.0511)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] - 52.2s\n",
      "Train Loss: 0.0494, Val Loss: 0.0501\n",
      "New best model saved (Val Loss: 0.0501)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] - 78.0s\n",
      "Train Loss: 0.0472, Val Loss: 0.0837\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 [Train]:  22%|████████▎                            | 1892/8478 [00:13<01:00, 108.64it/s]"
     ]
    }
   ],
   "source": [
    "# First, import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Then import sklearn before torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Now import torch and its modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Finally import cuda specific modules\n",
    "from torch.cuda import amp\n",
    "\n",
    "def select_gpu():\n",
    "    \"\"\"Select the NVIDIA GPU if available.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get number of GPUs\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\nFound {gpu_count} GPUs:\")\n",
    "        \n",
    "        # Find NVIDIA GPU\n",
    "        for i in range(gpu_count):\n",
    "            gpu_properties = torch.cuda.get_device_properties(i)\n",
    "            print(f\"GPU {i}: {gpu_properties.name}\")\n",
    "            \n",
    "            if 'NVIDIA' in gpu_properties.name:\n",
    "                print(f\"\\nSelecting NVIDIA GPU: {gpu_properties.name}\")\n",
    "                torch.cuda.set_device(i)\n",
    "                return torch.device(f'cuda:{i}')\n",
    "    \n",
    "    print(\"\\nNo NVIDIA GPU found, using CPU\")\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Print GPU information if available.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_properties = torch.cuda.get_device_properties(i)\n",
    "            print(f\"\\nGPU {i}: {gpu_properties.name}\")\n",
    "            print(f\"Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU available, using CPU\")\n",
    "\n",
    "class TemporalFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset class with minimal output.\"\"\"\n",
    "    def __init__(self, features, targets, lookback=30, forecast_horizon=7):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        # Calculate valid indices\n",
    "        self.valid_indices = len(features) - lookback - forecast_horizon + 1\n",
    "        \n",
    "        # Print only once during initialization\n",
    "        print(f\"\\nDataset created with {self.valid_indices} samples\")\n",
    "        print(f\"Lookback: {lookback}, Forecast horizon: {forecast_horizon}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.valid_indices\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.valid_indices:\n",
    "            raise IndexError(f\"Index {idx} out of bounds\")\n",
    "            \n",
    "        x = self.features[idx:idx + self.lookback]\n",
    "        y = self.targets[idx + self.lookback:idx + self.lookback + self.forecast_horizon]\n",
    "        y = y.squeeze()\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "class STRAPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attended))\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "class STRAPModel(nn.Module):\n",
    "    \"\"\"Modified STRAP model with explicit shape handling.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, num_heads=4, \n",
    "                 dropout=0.1, forecast_horizon=7):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        self.strap_layers = nn.ModuleList([\n",
    "            STRAPBlock(input_dim if i == 0 else hidden_dim,\n",
    "                      hidden_dim,\n",
    "                      num_heads,\n",
    "                      dropout)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, forecast_horizon)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, lookback, features)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process through STRAP layers\n",
    "        for layer in self.strap_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Global average pooling over temporal dimension\n",
    "        x = torch.mean(x, dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Project to forecast horizon\n",
    "        x = self.output_layer(x)  # Shape: (batch_size, forecast_horizon)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def clean_price(price_series):\n",
    "    \"\"\"Clean price data without type checking.\"\"\"\n",
    "    try:\n",
    "        if isinstance(price_series.iloc[0], str):\n",
    "            return pd.to_numeric(price_series.str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "        return pd.to_numeric(price_series, errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning prices: {e}\")\n",
    "        return pd.to_numeric(price_series, errors='coerce')\n",
    "\n",
    "def prepare_data(calendar_path, listings_path, n_listings=500):\n",
    "    \"\"\"Modified prepare_data function with additional checks.\"\"\"\n",
    "    print(\"\\nStarting data preparation...\")\n",
    "    \n",
    "    try:\n",
    "        # Load listings data\n",
    "        print(\"Loading listings data...\")\n",
    "        listings_df = pd.read_csv(listings_path)\n",
    "        sampled_listings = listings_df['id'].sample(n=n_listings, random_state=42)\n",
    "        \n",
    "        # Load calendar data\n",
    "        print(\"Loading calendar data...\")\n",
    "        calendar_df = pd.read_csv(calendar_path)\n",
    "        calendar_df = calendar_df[calendar_df['listing_id'].isin(sampled_listings)]\n",
    "        \n",
    "        print(f\"\\nInitial data statistics:\")\n",
    "        print(f\"Total listings: {len(sampled_listings)}\")\n",
    "        print(f\"Total records: {len(calendar_df)}\")\n",
    "        \n",
    "        # Convert dates\n",
    "        print(\"\\nProcessing dates...\")\n",
    "        calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
    "        \n",
    "        # Clean prices\n",
    "        print(\"Cleaning price data...\")\n",
    "        if isinstance(calendar_df['price'].iloc[0], str):\n",
    "            calendar_df['price_numeric'] = pd.to_numeric(\n",
    "                calendar_df['price'].str.replace('$', '').str.replace(',', ''), \n",
    "                errors='coerce'\n",
    "            )\n",
    "        else:\n",
    "            calendar_df['price_numeric'] = calendar_df['price']\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_prices = calendar_df['price_numeric'].isna().sum()\n",
    "        print(f\"Missing prices: {missing_prices}\")\n",
    "        \n",
    "        # Remove invalid prices\n",
    "        calendar_df = calendar_df.dropna(subset=['price_numeric'])\n",
    "        \n",
    "        # Sort by listing and date\n",
    "        print(\"Sorting data...\")\n",
    "        calendar_df = calendar_df.sort_values(['listing_id', 'date'])\n",
    "        \n",
    "        # Create features\n",
    "        print(\"\\nCreating features...\")\n",
    "        calendar_df['day_of_week'] = calendar_df['date'].dt.dayofweek\n",
    "        calendar_df['month'] = calendar_df['date'].dt.month\n",
    "        calendar_df['day_of_year'] = calendar_df['date'].dt.dayofyear\n",
    "        calendar_df['is_weekend'] = calendar_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        calendar_df['week_of_year'] = calendar_df['date'].dt.isocalendar().week\n",
    "        \n",
    "        # Merge with listings\n",
    "        print(\"Adding listing features...\")\n",
    "        calendar_df = pd.merge(\n",
    "            calendar_df,\n",
    "            listings_df[['id', 'latitude', 'longitude']],\n",
    "            left_on='listing_id',\n",
    "            right_on='id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Define features\n",
    "        feature_cols = [\n",
    "            'price_numeric', \n",
    "            'day_of_week', \n",
    "            'month', \n",
    "            'day_of_year',\n",
    "            'week_of_year',\n",
    "            'is_weekend',\n",
    "            'latitude',\n",
    "            'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Scale features\n",
    "        print(\"\\nScaling features...\")\n",
    "        feature_scaler = StandardScaler()\n",
    "        features_scaled = feature_scaler.fit_transform(calendar_df[feature_cols])\n",
    "        \n",
    "        # Scale targets\n",
    "        print(\"Scaling targets...\")\n",
    "        target_scaler = StandardScaler()\n",
    "        targets_scaled = target_scaler.fit_transform(calendar_df['price_numeric'].values.reshape(-1, 1))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        print(\"\\nConverting to tensors...\")\n",
    "        features_tensor = torch.FloatTensor(features_scaled)\n",
    "        targets_tensor = torch.FloatTensor(targets_scaled)\n",
    "        \n",
    "        print(\"\\nTensor shapes:\")\n",
    "        print(f\"Features: {features_tensor.shape}\")\n",
    "        print(f\"Targets: {targets_tensor.shape}\")\n",
    "        \n",
    "        # Verify data is properly ordered\n",
    "        print(\"\\nChecking data continuity...\")\n",
    "        date_diffs = calendar_df.groupby('listing_id')['date'].diff().dt.days\n",
    "        if date_diffs.max() > 1:\n",
    "            print(\"Warning: Data has gaps between dates\")\n",
    "        \n",
    "        return features_tensor, targets_tensor, target_scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in data preparation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
    "    \"\"\"Training function with cleaner output.\"\"\"\n",
    "    print(\"\\nStarting model training...\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'epoch_times': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        # Update tqdm to refresh less frequently\n",
    "        train_progress = tqdm(\n",
    "            train_loader, \n",
    "            desc=f'Epoch {epoch+1}/{num_epochs} [Train]',\n",
    "            ncols=100,  # Fixed width\n",
    "            mininterval=1.0,  # Update every second\n",
    "            leave=False  # Don't leave progress bars\n",
    "        )\n",
    "        \n",
    "        for batch_x, batch_y in train_progress:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(\n",
    "                val_loader, \n",
    "                desc=f'Epoch {epoch+1}/{num_epochs} [Val]',\n",
    "                ncols=100,\n",
    "                mininterval=1.0,\n",
    "                leave=False\n",
    "            )\n",
    "            \n",
    "            for batch_x, batch_y in val_progress:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                with amp.autocast():\n",
    "                    outputs = model(batch_x)\n",
    "                    val_loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - {epoch_time:.1f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_model.pth')\n",
    "            print(f'New best model saved (Val Loss: {best_val_loss:.4f})')\n",
    "        \n",
    "        print('-' * 60)  # Add separator between epochs\n",
    "    \n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    device = select_gpu()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nPreparing data...\")\n",
    "        features, targets, target_scaler = prepare_data(\n",
    "            r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\paris_merged_calendar.csv',\n",
    "            r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\2024-06-10\\listings.csv'\n",
    "        )\n",
    "        \n",
    "        dataset = TemporalFeatureDataset(\n",
    "            features, \n",
    "            targets,\n",
    "            lookback=30,\n",
    "            forecast_horizon=7\n",
    "        )\n",
    "        \n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        \n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=32,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = STRAPModel(\n",
    "            input_dim=features.shape[1],\n",
    "            hidden_dim=128,\n",
    "            num_layers=2,\n",
    "            num_heads=4,\n",
    "            dropout=0.1,\n",
    "            forecast_horizon=7\n",
    "        ).to(device)\n",
    "        \n",
    "        history = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            num_epochs=100,\n",
    "            learning_rate=1e-4,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSaving final model...\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'history': history,\n",
    "            'scaler': target_scaler\n",
    "        }, 'final_model.pth')\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
