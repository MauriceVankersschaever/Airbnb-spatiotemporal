{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HORIZON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Total listings in sample: 500\n",
      "Total records in sample: 339131\n",
      "Date range: 2023-06-07 00:00:00 to 2025-09-12 00:00:00\n",
      "Extracting tsfresh features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:520: UserWarning: Your time stamps are not uniformly sampled, which makes rolling nonsensical in some domains.\n",
      "  warnings.warn(\n",
      "Rolling: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n",
      "Feature Extraction: 100%|██████████| 500/500 [00:00<00:00, 1048.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 8 valid tsfresh features\n",
      "Preparing features with history...\n",
      "Using Paris as reference city\n",
      "Created spatial features:\n",
      "- distance_to_center: Distance in km from city center\n",
      "- north_south: Positive values are north of city center\n",
      "- normalized_latitude: 0-1 scale from south to north\n",
      "- knn_price_mean: Average price of k nearest neighbors\n",
      "- knn_price_std: Standard deviation of k nearest neighbor prices\n",
      "- price_diff_from_neighbors: Price difference from neighbor average\n",
      "\n",
      "Training model...\n",
      "\n",
      "Data Split Information:\n",
      "Training period: 2024-07-14 00:00:00 to 2025-05-14 00:00:00\n",
      "Testing period: 2025-05-15 00:00:00 to 2025-07-14 00:00:00\n",
      "Training samples: 94603\n",
      "Testing samples: 23949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:58:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"early_stopping\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions...\n",
      "\n",
      "Model Performance Metrics:\n",
      "RMSE: 23.8129\n",
      "MAE: 9.2217\n",
      "R2: 0.9976\n",
      "\n",
      "Monthly Error Analysis:\n",
      "           abs_error            pct_error           \n",
      "                mean        std      mean        std\n",
      "date                                                \n",
      "2025-05-31  8.899027  21.848284  6.716404  11.587952\n",
      "2025-06-30  9.187735  21.582365  6.316491  10.905755\n",
      "2025-07-31  9.722606  23.023905  5.422482   9.067460\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "             feature  importance\n",
      "28  rolling_min_180d    0.269329\n",
      "10     price_lag_61d    0.234078\n",
      "8       price_lag_1d    0.196678\n",
      "9      price_lag_31d    0.189669\n",
      "20   rolling_min_30d    0.036352\n",
      "21  rolling_mean_90d    0.023633\n",
      "7          longitude    0.011679\n",
      "19   rolling_max_30d    0.008715\n",
      "14    price_lag_181d    0.005494\n",
      "24   rolling_min_90d    0.004464\n",
      "\n",
      "Prediction Error by Month:\n",
      "First Month:\n",
      "Mean Absolute Error: 8.94\n",
      "Mean Percentage Error: 6.67%\n",
      "\n",
      "Second Month:\n",
      "Mean Absolute Error: 9.47\n",
      "Mean Percentage Error: 5.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvk\\AppData\\Local\\Temp\\ipykernel_30412\\769777432.py:307: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_analysis = results_df.set_index('date').resample('M').agg({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "from sklearn.neighbors import BallTree\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def load_and_prepare_data(calendar_path, listings_path, n_listings=500):\n",
    "    \"\"\"Load and prepare the data with basic cleaning and sampling.\"\"\"\n",
    "    # Load listings first to get the sample\n",
    "    listings_df = pd.read_csv(listings_path)\n",
    "    sampled_listings = listings_df['id'].sample(n=n_listings, random_state=42)\n",
    "    \n",
    "    # Clean and prepare listings data\n",
    "    listings_cleaned = listings_df[listings_df['id'].isin(sampled_listings)][\n",
    "        ['id', 'neighbourhood_cleansed', 'latitude', 'longitude']\n",
    "    ]\n",
    "    listings_cleaned = listings_cleaned.rename(columns={'id': 'listing_id'})\n",
    "    \n",
    "    # Load and filter calendar data\n",
    "    calendar_df = pd.read_csv(calendar_path)\n",
    "    calendar_df = calendar_df[calendar_df['listing_id'].isin(sampled_listings)]\n",
    "    calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
    "    \n",
    "    # Merge calendar with listings data\n",
    "    df = pd.merge(calendar_df, listings_cleaned, on='listing_id', how='left')\n",
    "    \n",
    "    print(f\"Total listings in sample: {len(df['listing_id'].unique())}\")\n",
    "    print(f\"Total records in sample: {len(df)}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_tsfresh_features(df):\n",
    "    \"\"\"Extract time series features using tsfresh.\"\"\"\n",
    "    # Create a proper time series dataframe for tsfresh\n",
    "    df_tsfresh = df[['listing_id', 'date', 'price']].copy()\n",
    "    #df_tsfresh['price'] = clean_price(df_tsfresh['price'])\n",
    "    df_tsfresh = df_tsfresh.sort_values(['listing_id', 'date'])\n",
    "   \n",
    "    # Create rolling windows\n",
    "    df_rolled = roll_time_series(\n",
    "        df_tsfresh,\n",
    "        column_id='listing_id',\n",
    "        column_sort='date',\n",
    "        max_timeshift=7,\n",
    "        rolling_direction=1\n",
    "    )\n",
    "   \n",
    "    # Define minimal but meaningful feature set\n",
    "    fc_parameters = {\n",
    "        \"mean\": None,\n",
    "        \"median\": None,\n",
    "        \"standard_deviation\": None,\n",
    "        \"variance\": None,\n",
    "        \"maximum\": None,\n",
    "        \"minimum\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        features_filtered = extract_features(\n",
    "            df_rolled,\n",
    "            column_id='listing_id',\n",
    "            column_sort='date',\n",
    "            column_value='price',\n",
    "            default_fc_parameters=fc_parameters,\n",
    "            n_jobs=0\n",
    "        )\n",
    "       \n",
    "        # Validate features\n",
    "        features_filtered = features_filtered.replace([np.inf, -np.inf], np.nan)\n",
    "        features_filtered = features_filtered.dropna(axis=1, how='all')\n",
    "        non_constant_cols = features_filtered.columns[features_filtered.nunique() > 1]\n",
    "        features_filtered = features_filtered[non_constant_cols]\n",
    "       \n",
    "        print(f\"\\nExtracted {len(non_constant_cols)} valid tsfresh features\")\n",
    "        return features_filtered\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error in tsfresh feature extraction: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_history_features(df, history_window=365, forecast_horizon=60):\n",
    "    \"\"\"Create lagged features based on full year history and 2-month forecast horizon.\"\"\"\n",
    "    df['price_numeric'] = pd.to_numeric(df['price'].replace(r'[\\$,]', '', regex=True), errors='coerce')\n",
    "    df = df.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for listing_id in df['listing_id'].unique():\n",
    "        listing_data = df[df['listing_id'] == listing_id].copy()\n",
    "        \n",
    "        # Create lagged features at different intervals throughout the year\n",
    "        # Monthly lags for first 6 months\n",
    "        for i in range(1, 181, 30):\n",
    "            listing_data[f'price_lag_{i}d'] = listing_data['price_numeric'].shift(i)\n",
    "        \n",
    "        # Quarterly lags for the rest of the year\n",
    "        for i in range(181, 366, 90):\n",
    "            listing_data[f'price_lag_{i}d'] = listing_data['price_numeric'].shift(i)\n",
    "        \n",
    "        # Create rolling statistics with different windows\n",
    "        windows = [30, 90, 180, 365]  # Monthly, quarterly, half-yearly, yearly\n",
    "        for window in windows:\n",
    "            listing_data[f'rolling_mean_{window}d'] = listing_data['price_numeric'].rolling(window=window).mean()\n",
    "            listing_data[f'rolling_std_{window}d'] = listing_data['price_numeric'].rolling(window=window).std()\n",
    "            listing_data[f'rolling_max_{window}d'] = listing_data['price_numeric'].rolling(window=window).max()\n",
    "            listing_data[f'rolling_min_{window}d'] = listing_data['price_numeric'].rolling(window=window).min()\n",
    "        \n",
    "        # Add year-over-year price change\n",
    "        listing_data['yoy_price_change'] = listing_data['price_numeric'] / listing_data['price_numeric'].shift(365) - 1\n",
    "        \n",
    "        # Create future target (price after forecast_horizon days)\n",
    "        listing_data['target_price'] = listing_data['price_numeric'].shift(-forecast_horizon)\n",
    "        \n",
    "        processed_data.append(listing_data)\n",
    "    \n",
    "    processed_df = pd.concat(processed_data)\n",
    "    processed_df = processed_df.dropna()\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def prepare_features_with_history(df, tsfresh_features, history_window=365, forecast_horizon=60):\n",
    "    \"\"\"Enhanced feature preparation including spatial features.\"\"\"\n",
    "    # Create temporal features\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['season_sin'] = np.sin(2 * np.pi * df['date'].dt.dayofyear/365.25)\n",
    "    df['season_cos'] = np.cos(2 * np.pi * df['date'].dt.dayofyear/365.25)\n",
    "    \n",
    "    # Add history-based features\n",
    "    df_with_history = create_history_features(df, history_window, forecast_horizon)\n",
    "    \n",
    "    # Create spatial features\n",
    "    spatial_features = create_spatial_features(df_with_history)\n",
    "    df_with_history = pd.concat([df_with_history, spatial_features], axis=1)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    df_with_history['neighbourhood_cleansed_encoded'] = le.fit_transform(\n",
    "        df_with_history['neighbourhood_cleansed'].fillna('Unknown')\n",
    "    )\n",
    "    \n",
    "    # Define feature sets\n",
    "    basic_features = [\n",
    "        'day_of_week', 'month', 'is_weekend', 'season_sin', 'season_cos',\n",
    "        'neighbourhood_cleansed_encoded', 'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    lag_features = [col for col in df_with_history.columns if 'price_lag_' in col]\n",
    "    rolling_features = [col for col in df_with_history.columns if 'rolling_' in col]\n",
    "    spatial_feature_cols = spatial_features.columns.tolist()\n",
    "    \n",
    "    all_features = (\n",
    "        basic_features + \n",
    "        lag_features + \n",
    "        rolling_features + \n",
    "        spatial_feature_cols + \n",
    "        ['yoy_price_change']\n",
    "    )\n",
    "    \n",
    "    X = df_with_history[all_features]\n",
    "    \n",
    "    return X, df_with_history['target_price'], df_with_history['date']\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points using the Haversine formula.\n",
    "    Returns distance in kilometers.\n",
    "    \"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def create_spatial_features(df, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Create spatial features combining:\n",
    "    - Distance to city center\n",
    "    - North-South position\n",
    "    - K-nearest neighbors price statistics\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing latitude, longitude and price\n",
    "        k_neighbors: Number of nearest neighbors for price statistics\n",
    "    \"\"\"\n",
    "    # City centers (approximate coordinates)\n",
    "    city_centers = {\n",
    "        'amsterdam': (52.3676, 4.9041),\n",
    "        'paris': (48.8566, 2.3522),\n",
    "        'london': (51.5074, -0.1278),\n",
    "        'berlin': (52.5200, 13.4050),\n",
    "        'rome': (41.9028, 12.4964),\n",
    "        'barcelona': (41.3851, 2.1734)\n",
    "    }\n",
    "    \n",
    "    # Determine which city based on coordinates\n",
    "    sample_lat = df['latitude'].median()\n",
    "    sample_lon = df['longitude'].median()\n",
    "    \n",
    "    # Find closest city center\n",
    "    closest_city = min(city_centers.items(), \n",
    "                      key=lambda x: calculate_distance(sample_lat, sample_lon, x[1][0], x[1][1]))\n",
    "    \n",
    "    city_center_lat, city_center_lon = city_centers[closest_city[0]]\n",
    "    print(f\"Using {closest_city[0].title()} as reference city\")\n",
    "    \n",
    "    # Create spatial features DataFrame\n",
    "    spatial_features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Basic spatial features\n",
    "    # Distance to city center\n",
    "    spatial_features['distance_to_center'] = df.apply(\n",
    "        lambda row: calculate_distance(\n",
    "            row['latitude'], \n",
    "            row['longitude'], \n",
    "            city_center_lat, \n",
    "            city_center_lon\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # North-South position relative to city center\n",
    "    spatial_features['north_south'] = df['latitude'] - city_center_lat\n",
    "    \n",
    "    # Normalized latitude (0 = southernmost, 1 = northernmost)\n",
    "    min_lat = df['latitude'].min()\n",
    "    max_lat = df['latitude'].max()\n",
    "    spatial_features['normalized_latitude'] = (df['latitude'] - min_lat) / (max_lat - min_lat)\n",
    "    \n",
    "    # 2. K-Nearest Neighbors Price Statistics\n",
    "    # Convert price to numeric if needed\n",
    "    if not pd.api.types.is_numeric_dtype(df['price']):\n",
    "        price_numeric = pd.to_numeric(df['price'].str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "    else:\n",
    "        price_numeric = df['price']\n",
    "    \n",
    "    # Convert coordinates to radians for BallTree\n",
    "    coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Create BallTree for efficient nearest neighbor search\n",
    "    tree = BallTree(coords, metric='haversine')\n",
    "    \n",
    "    # Get indices and distances for k nearest neighbors\n",
    "    distances, indices = tree.query(coords, k=k_neighbors + 1)  # +1 because first point is self\n",
    "    distances = distances * 6371.0  # Convert to kilometers (Earth radius)\n",
    "    \n",
    "    # Calculate neighbor price statistics\n",
    "    neighbor_prices = np.array([price_numeric.iloc[idx[1:]].values for idx in indices])\n",
    "    \n",
    "    spatial_features['knn_price_mean'] = np.nanmean(neighbor_prices, axis=1)\n",
    "    spatial_features['knn_price_std'] = np.nanstd(neighbor_prices, axis=1)\n",
    "    spatial_features['price_diff_from_neighbors'] = price_numeric.values - spatial_features['knn_price_mean']\n",
    "    \n",
    "    print(\"Created spatial features:\")\n",
    "    print(\"- distance_to_center: Distance in km from city center\")\n",
    "    print(\"- north_south: Positive values are north of city center\")\n",
    "    print(\"- normalized_latitude: 0-1 scale from south to north\")\n",
    "    print(\"- knn_price_mean: Average price of k nearest neighbors\")\n",
    "    print(\"- knn_price_std: Standard deviation of k nearest neighbor prices\")\n",
    "    print(\"- price_diff_from_neighbors: Price difference from neighbor average\")\n",
    "    \n",
    "    return spatial_features\n",
    "\n",
    "def evaluate_predictions(y_test, y_pred, test_dates):\n",
    "    \"\"\"\n",
    "    Simple evaluation of model predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_test: Array of actual values\n",
    "        y_pred: Array of predicted values\n",
    "        test_dates: Array of dates corresponding to predictions\n",
    "    \"\"\"\n",
    "    # Calculate basic metrics\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'date': test_dates,\n",
    "        'actual': y_test,\n",
    "        'predicted': y_pred,\n",
    "        'abs_error': np.abs(y_test - y_pred),\n",
    "        'pct_error': np.abs((y_test - y_pred) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    # Simple monthly analysis\n",
    "    monthly_analysis = results_df.set_index('date').resample('M').agg({\n",
    "        'abs_error': ['mean', 'std'],\n",
    "        'pct_error': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    return metrics, results_df, monthly_analysis\n",
    "\n",
    "def train_model_with_history(X, y, dates):\n",
    "    \"\"\"Train XGBoost model using 1 year of data and predict 2 months ahead.\"\"\"\n",
    "    # Calculate cutoff dates\n",
    "    latest_date = dates.max()\n",
    "    training_start = latest_date - timedelta(days=365)  # 1 year\n",
    "    validation_start = latest_date - timedelta(days=60)  # Last 2 months for validation\n",
    "    \n",
    "    # Split data based on dates\n",
    "    train_mask = (dates >= training_start) & (dates < validation_start)\n",
    "    test_mask = dates >= validation_start\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train = X[train_mask]\n",
    "    X_test = X[test_mask]\n",
    "    y_train = y[train_mask]\n",
    "    y_test = y[test_mask]\n",
    "    \n",
    "    print(\"\\nData Split Information:\")\n",
    "    print(f\"Training period: {dates[train_mask].min()} to {dates[train_mask].max()}\")\n",
    "    print(f\"Testing period: {dates[test_mask].min()} to {dates[test_mask].max()}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize XGBoost with parameters tuned for longer forecast horizon\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        early_stopping=20\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    xgb_model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return xgb_model, X_train_scaled, X_test_scaled, y_train, y_test, dates[test_mask]\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    calendar_path = r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\paris_merged_calendar.csv'\n",
    "    listings_path = r'C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\data_new\\paris\\2024-06-10\\listings.csv'\n",
    "    \n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_and_prepare_data(calendar_path, listings_path, n_listings=500)\n",
    "    \n",
    "    # Extract tsfresh features\n",
    "    print(\"Extracting tsfresh features...\")\n",
    "    tsfresh_features = extract_tsfresh_features(df)\n",
    "\n",
    "    \n",
    "    # Prepare features with full year history and 2-month forecast\n",
    "    print(\"Preparing features with history...\")\n",
    "    X, y, dates = prepare_features_with_history(\n",
    "        df, \n",
    "        tsfresh_features,\n",
    "        history_window=365,  # Full year of history\n",
    "        forecast_horizon=60  # 2 months ahead\n",
    "    )\n",
    "    \n",
    "    # Train model and make predictions\n",
    "    print(\"\\nTraining model...\")\n",
    "    model, X_train_scaled, X_test_scaled, y_train, y_test, test_dates = train_model_with_history(X, y, dates)\n",
    "    \n",
    "    print(\"\\nMaking predictions...\")\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    metrics, results_df, monthly_analysis = evaluate_predictions(y_test, y_pred, test_dates)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nMonthly Error Analysis:\")\n",
    "    print(monthly_analysis)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Analysis by prediction month\n",
    "    first_month_mask = results_df['date'] < (results_df['date'].min() + timedelta(days=30))\n",
    "    second_month_mask = ~first_month_mask\n",
    "    \n",
    "    print(\"\\nPrediction Error by Month:\")\n",
    "    print(\"First Month:\")\n",
    "    print(f\"Mean Absolute Error: {results_df[first_month_mask]['abs_error'].mean():.2f}\")\n",
    "    print(f\"Mean Percentage Error: {results_df[first_month_mask]['pct_error'].mean():.2f}%\")\n",
    "    print(\"\\nSecond Month:\")\n",
    "    print(f\"Mean Absolute Error: {results_df[second_month_mask]['abs_error'].mean():.2f}\")\n",
    "    print(f\"Mean Percentage Error: {results_df[second_month_mask]['pct_error'].mean():.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
