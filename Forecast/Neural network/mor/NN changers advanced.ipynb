{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import BallTree\n",
    "from math import radians\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Time2VecEncoding(nn.Module):\n",
    "    \"\"\"Time2Vec encoding for temporal features with improved initialization\"\"\"\n",
    "    def __init__(self, h_dim, scale=1):\n",
    "        super(Time2VecEncoding, self).__init__()\n",
    "        # Better initialization for stability\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(1, 1) * 0.1)\n",
    "        self.b0 = nn.parameter.Parameter(torch.zeros(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(1, h_dim-1) * 0.1)\n",
    "        self.b = nn.parameter.Parameter(torch.zeros(h_dim-1))\n",
    "        self.f = torch.sin\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, time):\n",
    "        # Ensure time is 2D: [batch_size, 1]\n",
    "        if len(time.shape) == 1:\n",
    "            time = time.unsqueeze(-1)\n",
    "        elif len(time.shape) == 3:\n",
    "            # If time has shape [batch_size, seq_len, 1], take the last time step\n",
    "            time = time[:, -1, :].unsqueeze(-1)\n",
    "            \n",
    "        # Scale time\n",
    "        time = time / self.scale\n",
    "        \n",
    "        # Calculate linear and periodic components\n",
    "        v1 = torch.matmul(time, self.w0) + self.b0  # Shape: [batch_size, 1]\n",
    "        v2 = self.f(torch.matmul(time, self.w) + self.b)  # Shape: [batch_size, h_dim-1]\n",
    "        \n",
    "        return torch.cat([v1, v2], dim=1)  # Shape: [batch_size, h_dim]\n",
    "\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism for feature importance\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim // 2, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(x)\n",
    "        # Apply attention\n",
    "        return x * attention_weights\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for improved gradient flow\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n",
    "\n",
    "\n",
    "class EnhancedSTRAP(nn.Module):\n",
    "    \"\"\"Enhanced implementation of ST-RAP model with stability improvements\"\"\"\n",
    "    def __init__(self, input_dim, temporal_dim, hidden_dim=256, num_gru_layers=3, dropout_rate=0.2):\n",
    "        super(EnhancedSTRAP, self).__init__()\n",
    "        \n",
    "        # Input feature attention\n",
    "        self.feature_attention = FeatureAttention(input_dim)\n",
    "        \n",
    "        # Feature embeddings with increased dimensionality\n",
    "        self.property_embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Enhanced time embedding\n",
    "        self.time_embedding = Time2VecEncoding(hidden_dim, scale=1000)\n",
    "        self.time_projection = nn.Linear(temporal_dim, hidden_dim)\n",
    "        \n",
    "        # Temporal GRU layers with bidirectional processing\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_gru_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate if num_gru_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Reduce GRU output dimension (bidirectional doubles the output size)\n",
    "        self.temporal_reduction = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.temporal_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Residual blocks for deeper representation\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Using only SAGEConv for spatial modeling (more stable than GATConv)\n",
    "        self.graph_conv1 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.graph_conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.spatial_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index=None, time_features=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply feature attention\n",
    "        x = self.feature_attention(x)\n",
    "        \n",
    "        # Property embeddings\n",
    "        property_emb = self.property_embedding(x)\n",
    "        \n",
    "        # Process time features if provided\n",
    "        if time_features is not None:\n",
    "            # Project all time features\n",
    "            time_emb = self.time_projection(time_features)\n",
    "            property_emb = property_emb + time_emb\n",
    "        \n",
    "        # Reshape for GRU (adding sequence dimension)\n",
    "        temporal_input = property_emb.unsqueeze(1)  # Shape: [batch_size, 1, hidden_dim]\n",
    "            \n",
    "        # Process with bidirectional GRU\n",
    "        temporal_output, _ = self.gru(temporal_input)  # Shape: [batch_size, 1, hidden_dim*2]\n",
    "        \n",
    "        # Reduce dimension from bidirectional output\n",
    "        temporal_output = self.temporal_reduction(temporal_output[:, -1])  # Shape: [batch_size, hidden_dim]\n",
    "        temporal_output = self.temporal_norm(temporal_output)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        temporal_output = self.residual_blocks(temporal_output)\n",
    "        \n",
    "        # Process with graph convolution if edge_index provided and valid\n",
    "        spatial_output = torch.zeros_like(temporal_output)\n",
    "        if edge_index is not None and edge_index.numel() > 0:\n",
    "            try:\n",
    "                # Validate edge_index to ensure it's within valid range\n",
    "                if edge_index.min() >= 0 and edge_index.max() < batch_size:\n",
    "                    # First graph layer\n",
    "                    spatial_output = self.graph_conv1(temporal_output, edge_index)\n",
    "                    spatial_output = F.relu(spatial_output)\n",
    "                    spatial_output = F.dropout(spatial_output, p=0.2, training=self.training)\n",
    "                    \n",
    "                    # Second graph layer\n",
    "                    spatial_output = self.graph_conv2(spatial_output, edge_index)\n",
    "                    spatial_output = self.spatial_norm(spatial_output)\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid edge_index values. Min: {edge_index.min()}, Max: {edge_index.max()}, Batch size: {batch_size}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in graph convolution: {e}\")\n",
    "                # If the graph convolution fails, proceed without it\n",
    "                spatial_output = torch.zeros_like(temporal_output)\n",
    "        \n",
    "        # Combine temporal and spatial outputs\n",
    "        combined = torch.cat([temporal_output, spatial_output], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.predictor(combined).squeeze(-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def create_edge_index_efficient(data, k_neighbors=5, distance_threshold=2.0, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Create spatial edge index efficiently using BallTree and chunking\n",
    "    with additional safety checks\n",
    "    \"\"\"\n",
    "    print(\"Creating spatial edge index efficiently...\")\n",
    "    \n",
    "    try:\n",
    "        coords = data[['latitude', 'longitude']].values\n",
    "        n_samples = len(coords)\n",
    "        \n",
    "        # Check for NaN or infinite values\n",
    "        if np.isnan(coords).any() or np.isinf(coords).any():\n",
    "            print(\"Warning: NaN or infinite values found in coordinates. Cleaning data...\")\n",
    "            coords = np.nan_to_num(coords, nan=0.0, posinf=90.0, neginf=-90.0)\n",
    "        \n",
    "        # Convert to radians for BallTree\n",
    "        coords_rad = np.radians(coords)\n",
    "        \n",
    "        # Create BallTree\n",
    "        tree = BallTree(coords_rad, metric='haversine')\n",
    "        \n",
    "        # Create edge index\n",
    "        edge_index = []\n",
    "        \n",
    "        # Process in chunks to avoid memory issues\n",
    "        for i in tqdm(range(0, n_samples, chunk_size)):\n",
    "            end_idx = min(i + chunk_size, n_samples)\n",
    "            chunk_coords = coords_rad[i:end_idx]\n",
    "            \n",
    "            # Query k+1 nearest neighbors (including self)\n",
    "            distances, indices = tree.query(chunk_coords, k=min(k_neighbors+1, n_samples))\n",
    "            \n",
    "            # Convert distances from radians to km\n",
    "            distances = distances * 6371.0  # Earth radius in km\n",
    "            \n",
    "            # Add edges for each point in chunk\n",
    "            for j in range(len(chunk_coords)):\n",
    "                point_idx = i + j\n",
    "                for k in range(1, min(k_neighbors+1, indices.shape[1])):  # Skip self (index 0)\n",
    "                    if k < indices.shape[1]:  # Ensure we don't go out of bounds\n",
    "                        neighbor_idx = indices[j, k]\n",
    "                        \n",
    "                        # Safety check for valid indices\n",
    "                        if neighbor_idx < n_samples and neighbor_idx >= 0:\n",
    "                            distance = distances[j, k]\n",
    "                            \n",
    "                            if distance <= distance_threshold:\n",
    "                                edge_index.append([point_idx, neighbor_idx])\n",
    "                                # Add reverse edge for undirected graph\n",
    "                                edge_index.append([neighbor_idx, point_idx])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        if edge_index:\n",
    "            edge_index = list(set(tuple(edge) for edge in edge_index))\n",
    "            edge_index = [list(edge) for edge in edge_index]\n",
    "            \n",
    "            # Final safety check\n",
    "            edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "            \n",
    "            # Check for valid indices\n",
    "            if edge_index_tensor.min() < 0 or edge_index_tensor.max() >= n_samples:\n",
    "                print(\"Warning: Invalid indices in edge_index. Creating a safer version...\")\n",
    "                valid_mask = (edge_index_tensor[0] >= 0) & (edge_index_tensor[0] < n_samples) & \\\n",
    "                             (edge_index_tensor[1] >= 0) & (edge_index_tensor[1] < n_samples)\n",
    "                edge_index_tensor = edge_index_tensor[:, valid_mask]\n",
    "            \n",
    "            print(f\"Created edge index with shape {edge_index_tensor.shape}\")\n",
    "            return edge_index_tensor\n",
    "        else:\n",
    "            print(\"Warning: No valid edges found. Returning empty edge index.\")\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating edge index: {e}\")\n",
    "        print(\"Returning empty edge index.\")\n",
    "        return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def create_enhanced_spatial_features(df, k_neighbors=5, chunk_size=1000, n_jobs=-1):\n",
    "    \"\"\"Create enhanced spatial features with error handling\"\"\"\n",
    "    print(\"Creating enhanced spatial features...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Paris coordinates\n",
    "    city_center_lat, city_center_lon = 48.8566, 2.3522\n",
    "    \n",
    "    try:\n",
    "        # Calculate distance to city center and north-south position\n",
    "        df['distance_to_center'] = df.apply(\n",
    "            lambda row: calculate_distance(\n",
    "                row['latitude'], \n",
    "                row['longitude'], \n",
    "                city_center_lat, \n",
    "                city_center_lon\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Calculate directional features\n",
    "        df['north_south'] = df['latitude'] - city_center_lat\n",
    "        df['east_west'] = df['longitude'] - city_center_lon\n",
    "        \n",
    "        # Check for NaN or infinite values\n",
    "        coords = df[['latitude', 'longitude']].values\n",
    "        if np.isnan(coords).any() or np.isinf(coords).any():\n",
    "            print(\"Warning: NaN or infinite values found in coordinates. Cleaning data...\")\n",
    "            df['latitude'] = np.nan_to_num(df['latitude'].values, nan=city_center_lat)\n",
    "            df['longitude'] = np.nan_to_num(df['longitude'].values, nan=city_center_lon)\n",
    "        \n",
    "        # Create BallTree for nearest neighbor calculations\n",
    "        coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "        tree = BallTree(coords, metric='haversine')\n",
    "        \n",
    "        # Process in chunks\n",
    "        n_chunks = math.ceil(len(df) / chunk_size)\n",
    "        chunks = np.array_split(df, n_chunks)\n",
    "        \n",
    "        # KNN features\n",
    "        knn_price_mean = np.zeros(len(df))\n",
    "        knn_price_std = np.zeros(len(df))\n",
    "        knn_price_median = np.zeros(len(df))\n",
    "        price_diff = np.zeros(len(df))\n",
    "        \n",
    "        start_idx = 0\n",
    "        for chunk in tqdm(chunks, desc=\"Processing chunks for spatial features\"):\n",
    "            chunk_size = len(chunk)\n",
    "            chunk_coords = np.radians(chunk[['latitude', 'longitude']].values)\n",
    "            \n",
    "            # Find k+1 nearest neighbors (including self)\n",
    "            k_to_use = min(k_neighbors+1, len(df))  # Ensure k is not larger than dataset\n",
    "            distances, indices = tree.query(chunk_coords, k=k_to_use)\n",
    "            \n",
    "            # Calculate neighbor statistics\n",
    "            for i in range(chunk_size):\n",
    "                # Skip self (index 0)\n",
    "                neighbor_indices = indices[i, 1:]\n",
    "                \n",
    "                # Filter out invalid indices\n",
    "                valid_indices = [idx for idx in neighbor_indices if 0 <= idx < len(df)]\n",
    "                \n",
    "                if valid_indices:\n",
    "                    prices = df.iloc[valid_indices]['price'].values\n",
    "                    \n",
    "                    # Handle potential NaN or Inf values\n",
    "                    prices = prices[~np.isnan(prices) & ~np.isinf(prices)]\n",
    "                    \n",
    "                    if len(prices) > 0:\n",
    "                        knn_price_mean[start_idx + i] = np.mean(prices)\n",
    "                        knn_price_std[start_idx + i] = np.std(prices) if len(prices) > 1 else 0\n",
    "                        knn_price_median[start_idx + i] = np.median(prices)\n",
    "                        price_diff[start_idx + i] = chunk.iloc[i]['price'] - np.mean(prices)\n",
    "            \n",
    "            start_idx += chunk_size\n",
    "        \n",
    "        # Add features to dataframe\n",
    "        df['knn_price_mean'] = knn_price_mean\n",
    "        df['knn_price_std'] = knn_price_std\n",
    "        df['knn_price_median'] = knn_price_median\n",
    "        df['price_diff_from_neighbors'] = price_diff\n",
    "        \n",
    "        # Standardize the new features\n",
    "        spatial_features = ['distance_to_center', 'north_south', 'east_west',\n",
    "                            'knn_price_mean', 'knn_price_std', 'knn_price_median', \n",
    "                            'price_diff_from_neighbors']\n",
    "        \n",
    "        for col in spatial_features:\n",
    "            # Replace NaN or infinite values\n",
    "            df[col] = np.nan_to_num(df[col].values, nan=0.0)\n",
    "            \n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            if std_val > 0:\n",
    "                df[col] = (df[col] - mean_val) / std_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating spatial features: {e}\")\n",
    "        print(\"Proceeding with original features only.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_error_autocorrelation(errors, max_lag=7):\n",
    "    \"\"\"Calculate autocorrelation of prediction errors with error handling\"\"\"\n",
    "    try:\n",
    "        # Remove NaN or infinite values\n",
    "        clean_errors = errors[~np.isnan(errors) & ~np.isinf(errors)]\n",
    "        \n",
    "        if len(clean_errors) < max_lag + 1:\n",
    "            print(\"Warning: Not enough valid error values for autocorrelation calculation.\")\n",
    "            return np.zeros(max_lag)\n",
    "        \n",
    "        # Calculate autocorrelation\n",
    "        error_acf = acf(clean_errors, nlags=max_lag)\n",
    "        \n",
    "        # Return values excluding lag 0 (which is always 1)\n",
    "        return error_acf[1:]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating autocorrelation: {e}\")\n",
    "        return np.zeros(max_lag)\n",
    "\n",
    "\n",
    "def calculate_error_stability(all_results):\n",
    "    \"\"\"Calculate error stability metrics with error handling\"\"\"\n",
    "    try:\n",
    "        # Group by date\n",
    "        grouped = all_results.groupby('date_str')\n",
    "        \n",
    "        # Calculate standard deviation of errors for each day\n",
    "        daily_error_std = grouped['error'].std()\n",
    "        \n",
    "        # Calculate MAE for each day\n",
    "        daily_mae = grouped['abs_error'].mean()\n",
    "        \n",
    "        # Calculate stability metrics\n",
    "        mae_mean = daily_mae.mean() if len(daily_mae) > 0 else 1.0\n",
    "        mae_stability = daily_mae.std() / mae_mean if mae_mean > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'daily_error_std': daily_error_std,\n",
    "            'daily_mae': daily_mae,\n",
    "            'mae_stability_coefficient': mae_stability\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating error stability: {e}\")\n",
    "        # Return empty objects with the expected structure\n",
    "        return {\n",
    "            'daily_error_std': pd.Series(),\n",
    "            'daily_mae': pd.Series(),\n",
    "            'mae_stability_coefficient': 0.0\n",
    "        }\n",
    "\n",
    "\n",
    "def run_day_by_day_enhanced_strap_prediction(train_path, test_path, features_to_drop=None, prediction_days=7, output_path=None):\n",
    "    \"\"\"Run enhanced ST-RAP model with day-by-day retraining for multiple days prediction with robust error handling\"\"\"\n",
    "    print(f\"Processing dataset for {prediction_days}-day prediction with enhanced ST-RAP model\")\n",
    "    \n",
    "    # Default features to drop if none specified\n",
    "    if features_to_drop is None:\n",
    "        features_to_drop = []\n",
    "    \n",
    "    print(f\"Features being dropped: {features_to_drop}\")\n",
    "    \n",
    "    try:\n",
    "        # Load training and test data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        # Convert date columns to datetime\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "        \n",
    "        # Drop specified columns if they exist\n",
    "        for col in features_to_drop:\n",
    "            if col in train_data.columns:\n",
    "                print(f\"Dropping column: {col}\")\n",
    "                train_data = train_data.drop(col, axis=1)\n",
    "            if col in test_data.columns:\n",
    "                test_data = test_data.drop(col, axis=1)\n",
    "        \n",
    "        # Sort by date\n",
    "        train_data = train_data.sort_values('date')\n",
    "        test_data = test_data.sort_values('date')\n",
    "        \n",
    "        # Get unique dates in test set\n",
    "        test_dates = test_data['date'].dt.date.unique()\n",
    "        print(f\"Test set contains {len(test_dates)} unique dates.\")\n",
    "        \n",
    "        # Limit to specified prediction days\n",
    "        if len(test_dates) > prediction_days:\n",
    "            test_dates = test_dates[:prediction_days]\n",
    "            print(f\"Limited to first {prediction_days} days for prediction.\")\n",
    "        \n",
    "        # Add enhanced spatial features\n",
    "        train_data = create_enhanced_spatial_features(train_data, k_neighbors=5)\n",
    "        test_data = create_enhanced_spatial_features(test_data, k_neighbors=5)\n",
    "        \n",
    "        # Create feature matrices    \n",
    "        feature_cols = [col for col in train_data.columns \n",
    "                      if col not in ['listing_id', 'date', 'price']]\n",
    "        \n",
    "        # Handle NaN or infinite values in feature matrices\n",
    "        for col in feature_cols:\n",
    "            train_data[col] = np.nan_to_num(train_data[col].values, nan=0.0)\n",
    "            test_data[col] = np.nan_to_num(test_data[col].values, nan=0.0)\n",
    "        \n",
    "        X_train = train_data[feature_cols].values\n",
    "        y_train = train_data['price'].values\n",
    "        \n",
    "        # Initialize device\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Create a simpler model if on CPU to speed up training\n",
    "        if not use_cuda:\n",
    "            print(\"Running on CPU - using a more efficient model configuration\")\n",
    "            model = EnhancedSTRAP(\n",
    "                input_dim=len(feature_cols),\n",
    "                temporal_dim=5,\n",
    "                hidden_dim=128,  # Smaller hidden dimension\n",
    "                num_gru_layers=2,  # Fewer GRU layers\n",
    "                dropout_rate=0.2\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = EnhancedSTRAP(\n",
    "                input_dim=len(feature_cols),\n",
    "                temporal_dim=5,\n",
    "                hidden_dim=256,\n",
    "                num_gru_layers=3,\n",
    "                dropout_rate=0.3\n",
    "            ).to(device)\n",
    "        \n",
    "        # Define optimizer with learning rate scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = nn.HuberLoss(delta=1.0)  # Huber loss is more robust to outliers than MSE\n",
    "        \n",
    "        # Initialize results storage\n",
    "        daily_results = []\n",
    "        \n",
    "        # Build edge index once for efficiency\n",
    "        # Use a smaller k_neighbors value for stability\n",
    "        edge_index = None\n",
    "        try:\n",
    "            print(\"Attempting to build edge index for graph structure...\")\n",
    "            # Create a subset of data for edge index to reduce memory usage\n",
    "            if len(train_data) > 5000:\n",
    "                edge_data = pd.concat([train_data, test_data]).sample(5000, random_state=42)\n",
    "            else:\n",
    "                edge_data = pd.concat([train_data, test_data])\n",
    "                \n",
    "            edge_index = create_edge_index_efficient(\n",
    "                edge_data[['latitude', 'longitude']],\n",
    "                k_neighbors=5,  # Reduced from 8 to 5 for stability\n",
    "                distance_threshold=3.0\n",
    "            )\n",
    "            \n",
    "            if edge_index.numel() > 0:\n",
    "                edge_index = edge_index.to(device)\n",
    "                print(f\"Successfully built edge index with shape {edge_index.shape}\")\n",
    "            else:\n",
    "                print(\"Edge index is empty. Graph convolution will be skipped.\")\n",
    "                edge_index = None\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to build edge index: {e}\")\n",
    "            print(\"Falling back to non-graph mode\")\n",
    "            edge_index = None\n",
    "        \n",
    "        # Process each day in the test set\n",
    "        for day in tqdm(test_dates, desc=\"Processing days\"):\n",
    "            # Convert day to datetime for filtering\n",
    "            day_dt = pd.to_datetime(day)\n",
    "            \n",
    "            # Get test data for the current day\n",
    "            day_test = test_data[test_data['date'].dt.date == day]\n",
    "            \n",
    "            X_test_day = day_test[feature_cols].values\n",
    "            y_test_day = day_test['price'].values\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "            y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "            X_test_tensor = torch.FloatTensor(X_test_day).to(device)\n",
    "            y_test_tensor = torch.FloatTensor(y_test_day).to(device)\n",
    "            \n",
    "            # Extract time features\n",
    "            time_cols = ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
    "            if all(col in train_data.columns for col in time_cols):\n",
    "                time_features_train = torch.FloatTensor(train_data[time_cols].values).to(device)\n",
    "                time_features_test = torch.FloatTensor(day_test[time_cols].values).to(device)\n",
    "            else:\n",
    "                # If time columns don't exist, create dummy time features\n",
    "                print(\"Warning: Time columns not found. Using dummy time features.\")\n",
    "                time_features_train = torch.zeros((len(X_train_tensor), 5), device=device)\n",
    "                time_features_test = torch.zeros((len(X_test_tensor), 5), device=device)\n",
    "            \n",
    "            # Create DataLoader for training with variable batch size based on data size\n",
    "            batch_size = min(64, len(X_train_tensor) // 10) if len(X_train_tensor) > 640 else 32\n",
    "            train_dataset = torch.utils.data.TensorDataset(X_train_tensor, time_features_train, y_train_tensor)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            # Train model with dynamic epochs based on dataset size\n",
    "            model.train()\n",
    "            epochs = 10 if use_cuda else 5  # Fewer epochs if on CPU\n",
    "            patience = 5\n",
    "            patience_counter = 0\n",
    "            best_val_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                batch_count = 0\n",
    "                \n",
    "                # Training phase with error handling\n",
    "                for batch_x, batch_time, batch_y in train_loader:\n",
    "                    try:\n",
    "                        # Forward pass\n",
    "                        outputs = model(batch_x, edge_index, batch_time)\n",
    "                        \n",
    "                        # Check for NaN or Inf values\n",
    "                        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                            print(\"Warning: NaN or Inf values in model outputs. Skipping batch.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate loss\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        \n",
    "                        # Backward and optimize\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)  # Increased clip value\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        batch_count += 1\n",
    "                    except RuntimeError as e:\n",
    "                        if \"CUDA\" in str(e):\n",
    "                            print(f\"CUDA error during training: {e}\")\n",
    "                            print(\"Attempting to continue with next batch...\")\n",
    "                            # Clear any stored gradients\n",
    "                            optimizer.zero_grad()\n",
    "                            torch.cuda.empty_cache()  # Try to free GPU memory\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                \n",
    "                # Calculate average loss for the epoch\n",
    "                avg_loss = total_loss/batch_count if batch_count > 0 else float('inf')\n",
    "                \n",
    "                # Validation phase - use a small subset of training data\n",
    "                val_size = min(1000, len(X_train_tensor))\n",
    "                val_indices = torch.randperm(len(X_train_tensor))[:val_size]\n",
    "                val_x = X_train_tensor[val_indices]\n",
    "                val_time = time_features_train[val_indices]\n",
    "                val_y = y_train_tensor[val_indices]\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        val_outputs = model(val_x, edge_index, val_time)\n",
    "                        val_loss = criterion(val_outputs, val_y).item()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during validation: {e}\")\n",
    "                        val_loss = float('inf')\n",
    "                model.train()\n",
    "                \n",
    "                # Update learning rate based on validation loss\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Print epoch stats\n",
    "                print(f\"Day: {day}, Epoch: {epoch+1}/{epochs}, Train Loss: {avg_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Generate predictions in smaller batches to avoid OOM\n",
    "                test_batch_size = min(64, len(X_test_tensor))\n",
    "                predictions = []\n",
    "                \n",
    "                for i in range(0, len(X_test_tensor), test_batch_size):\n",
    "                    try:\n",
    "                        end_idx = min(i + test_batch_size, len(X_test_tensor))\n",
    "                        batch_x = X_test_tensor[i:end_idx]\n",
    "                        batch_time = time_features_test[i:end_idx]\n",
    "                        batch_pred = model(batch_x, edge_index, batch_time)\n",
    "                        predictions.append(batch_pred)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during prediction batch {i}-{end_idx}: {e}\")\n",
    "                        # Fill with zeros for the failed batch\n",
    "                        dummy_pred = torch.zeros(end_idx - i, device=device)\n",
    "                        predictions.append(dummy_pred)\n",
    "                \n",
    "                # Concatenate all prediction batches\n",
    "                if predictions:\n",
    "                    y_pred = torch.cat(predictions)\n",
    "                    # Ensure the length matches\n",
    "                    if len(y_pred) != len(y_test_tensor):\n",
    "                        print(f\"Warning: Prediction length mismatch. Expected {len(y_test_tensor)}, got {len(y_pred)}\")\n",
    "                        # Pad or truncate to match\n",
    "                        if len(y_pred) < len(y_test_tensor):\n",
    "                            y_pred = torch.cat([y_pred, torch.zeros(len(y_test_tensor) - len(y_pred), device=device)])\n",
    "                        else:\n",
    "                            y_pred = y_pred[:len(y_test_tensor)]\n",
    "                    \n",
    "                    test_loss = criterion(y_pred, y_test_tensor).item()\n",
    "                else:\n",
    "                    print(\"No valid predictions generated. Using zeros.\")\n",
    "                    y_pred = torch.zeros_like(y_test_tensor)\n",
    "                    test_loss = float('inf')\n",
    "            \n",
    "            # Convert predictions to numpy\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            \n",
    "            # Store results for the day\n",
    "            day_results_df = pd.DataFrame({\n",
    "                'date': day_test['date'],\n",
    "                'listing_id': day_test['listing_id'],\n",
    "                'price': y_test_day,\n",
    "                'predicted': y_pred_np,\n",
    "                'error': y_test_day - y_pred_np,\n",
    "                'abs_error': np.abs(y_test_day - y_pred_np),\n",
    "                'pct_error': np.abs((y_test_day - y_pred_np) / (np.abs(y_test_day) + 1e-8)) * 100\n",
    "            })\n",
    "            \n",
    "            daily_results.append(day_results_df)\n",
    "            \n",
    "            # Update training data with the current day's actual values\n",
    "            # This simulates getting actual values at the end of each day\n",
    "            # before predicting the next day\n",
    "            X_train = np.concatenate([X_train, X_test_day])\n",
    "            y_train = np.concatenate([y_train, y_test_day])\n",
    "            \n",
    "            # Instead of growing the full dataframe (which can be slow),\n",
    "            # just update the necessary columns for the next iteration\n",
    "            new_train_data = day_test.copy()\n",
    "            train_data = pd.concat([train_data, new_train_data], ignore_index=True)\n",
    "            \n",
    "            # Free GPU memory\n",
    "            if use_cuda:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine all daily results\n",
    "        all_results = pd.concat(daily_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall metrics with error handling\n",
    "        y_true = all_results['price'].values\n",
    "        y_pred = all_results['predicted'].values\n",
    "        \n",
    "        # Remove any potential NaN or Inf values\n",
    "        valid_mask = ~np.isnan(y_true) & ~np.isinf(y_true) & ~np.isnan(y_pred) & ~np.isinf(y_pred)\n",
    "        y_true_clean = y_true[valid_mask]\n",
    "        y_pred_clean = y_pred[valid_mask]\n",
    "        \n",
    "        # Only calculate metrics if we have valid predictions\n",
    "        if len(y_true_clean) > 0:\n",
    "            metrics = {\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_clean, y_pred_clean)),\n",
    "                'mae': mean_absolute_error(y_true_clean, y_pred_clean),\n",
    "                'r2': r2_score(y_true_clean, y_pred_clean) if len(set(y_true_clean)) > 1 else 0,\n",
    "                'mape': np.mean(np.abs((y_true_clean - y_pred_clean) / (np.abs(y_true_clean) + 1e-8))) * 100\n",
    "            }\n",
    "        else:\n",
    "            print(\"Warning: No valid predictions for metric calculation.\")\n",
    "            metrics = {'rmse': float('inf'), 'mae': float('inf'), 'r2': 0, 'mape': float('inf')}\n",
    "        \n",
    "        # Calculate daily metrics with error handling\n",
    "        daily_metrics = []\n",
    "        for day_df in daily_results:\n",
    "            try:\n",
    "                day = day_df['date'].iloc[0]\n",
    "                y_true_day = day_df['price'].values\n",
    "                y_pred_day = day_df['predicted'].values\n",
    "                \n",
    "                # Remove any NaN or Inf values\n",
    "                valid_mask = ~np.isnan(y_true_day) & ~np.isinf(y_true_day) & ~np.isnan(y_pred_day) & ~np.isinf(y_pred_day)\n",
    "                y_true_day_clean = y_true_day[valid_mask]\n",
    "                y_pred_day_clean = y_pred_day[valid_mask]\n",
    "                \n",
    "                if len(y_true_day_clean) > 0:\n",
    "                    daily_metrics.append({\n",
    "                        'date': day,\n",
    "                        'rmse': np.sqrt(mean_squared_error(y_true_day_clean, y_pred_day_clean)),\n",
    "                        'mae': mean_absolute_error(y_true_day_clean, y_pred_day_clean),\n",
    "                        'r2': r2_score(y_true_day_clean, y_pred_day_clean) if len(set(y_true_day_clean)) > 1 else np.nan,\n",
    "                        'mape': np.mean(np.abs((y_true_day_clean - y_pred_day_clean) / (np.abs(y_true_day_clean) + 1e-8))) * 100,\n",
    "                        'n_samples': len(y_true_day_clean)\n",
    "                    })\n",
    "                else:\n",
    "                    # Add placeholder metrics if no valid data\n",
    "                    daily_metrics.append({\n",
    "                        'date': day,\n",
    "                        'rmse': np.nan,\n",
    "                        'mae': np.nan,\n",
    "                        'r2': np.nan,\n",
    "                        'mape': np.nan,\n",
    "                        'n_samples': 0\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics for day {day_df['date'].iloc[0]}: {e}\")\n",
    "        \n",
    "        daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "        \n",
    "        # Create evaluation results dictionary\n",
    "        evaluation_results = {\n",
    "            'overall_metrics': metrics,\n",
    "            'daily_metrics': daily_metrics_df,\n",
    "            'all_results': all_results\n",
    "        }\n",
    "        \n",
    "        # Add date_str column for grouping\n",
    "        all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Calculate error autocorrelation\n",
    "        error_autocorrelation = calculate_error_autocorrelation(all_results['error'].values)\n",
    "\n",
    "        # Calculate error stability metrics\n",
    "        error_stability = calculate_error_stability(all_results)\n",
    "\n",
    "        # Add to evaluation results\n",
    "        evaluation_results['error_autocorrelation'] = error_autocorrelation\n",
    "        evaluation_results['error_stability'] = error_stability\n",
    "        \n",
    "        # Save results to CSV if output path is provided\n",
    "        if output_path:\n",
    "            try:\n",
    "                # Make sure the directory exists\n",
    "                os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "                \n",
    "                # Include location data from test data if available\n",
    "                if 'longitude' in test_data.columns and 'latitude' in test_data.columns:\n",
    "                    location_data = test_data[['listing_id', 'longitude', 'latitude']].drop_duplicates()\n",
    "                    results_with_location = all_results.merge(location_data, on='listing_id', how='left')\n",
    "                    results_with_location.to_csv(output_path, index=False)\n",
    "                    print(f\"Results saved to {output_path} with location data\")\n",
    "                else:\n",
    "                    all_results.to_csv(output_path, index=False)\n",
    "                    print(f\"Results saved to {output_path}\")\n",
    "                \n",
    "                # Also save daily metrics\n",
    "                metrics_path = output_path.replace('.csv', '_daily_metrics.csv')\n",
    "                daily_metrics_df.to_csv(metrics_path, index=False)\n",
    "                print(f\"Daily metrics saved to {metrics_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving results: {e}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in prediction pipeline: {e}\")\n",
    "        # Return minimal valid output to avoid further errors\n",
    "        return {\n",
    "            'overall_metrics': {'rmse': float('inf'), 'mae': float('inf'), 'r2': 0, 'mape': float('inf')},\n",
    "            'daily_metrics': pd.DataFrame(),\n",
    "            'all_results': pd.DataFrame(),\n",
    "            'error_autocorrelation': np.zeros(7),\n",
    "            'error_stability': {\n",
    "                'daily_error_std': pd.Series(),\n",
    "                'daily_mae': pd.Series(),\n",
    "                'mae_stability_coefficient': 0.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def plot_enhanced_results(evaluation_results):\n",
    "    \"\"\"Plot the results from predictions with enhanced visualizations and error handling\"\"\"\n",
    "    try:\n",
    "        # Set style with fallback option\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        except:\n",
    "            try:\n",
    "                plt.style.use('seaborn-whitegrid')  # Fallback for older versions\n",
    "            except:\n",
    "                pass  # Continue with default style if both fail\n",
    "        \n",
    "        # Extract data\n",
    "        daily_metrics = evaluation_results['daily_metrics']\n",
    "        all_results = evaluation_results['all_results']\n",
    "        \n",
    "        # Check if we have valid data to plot\n",
    "        if len(daily_metrics) == 0 or len(all_results) == 0:\n",
    "            print(\"No valid data for plotting. Skipping visualization.\")\n",
    "            return\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "        \n",
    "        # Plot 1: Daily MAE with trend line\n",
    "        if len(daily_metrics) > 0 and 'date' in daily_metrics.columns and 'mae' in daily_metrics.columns:\n",
    "            dates = pd.to_datetime(daily_metrics['date'])\n",
    "            \n",
    "            # Filter out NaN values\n",
    "            valid_mae = daily_metrics['mae'].notna()\n",
    "            if valid_mae.any():\n",
    "                sns.lineplot(\n",
    "                    x=dates[valid_mae],\n",
    "                    y=daily_metrics.loc[valid_mae, 'mae'],\n",
    "                    marker='o',\n",
    "                    linewidth=2,\n",
    "                    color='royalblue',\n",
    "                    ax=axes[0, 0]\n",
    "                )\n",
    "                \n",
    "                # Add trend line if we have at least 2 points\n",
    "                if sum(valid_mae) >= 2:\n",
    "                    x_indices = np.arange(len(dates[valid_mae]))\n",
    "                    z = np.polyfit(x_indices, daily_metrics.loc[valid_mae, 'mae'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    axes[0, 0].plot(dates[valid_mae], p(x_indices), \"r--\", alpha=0.8, \n",
    "                                   label=f\"Trend: {'increasing' if z[0] > 0 else 'decreasing'}\")\n",
    "                \n",
    "                axes[0, 0].set_title('Mean Absolute Error by Day', fontsize=14)\n",
    "                axes[0, 0].set_xlabel('Date', fontsize=12)\n",
    "                axes[0, 0].set_ylabel('MAE', fontsize=12)\n",
    "                axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Daily RMSE with confidence interval\n",
    "        if len(daily_metrics) > 0 and 'date' in daily_metrics.columns and 'rmse' in daily_metrics.columns:\n",
    "            # Filter out NaN values\n",
    "            valid_rmse = daily_metrics['rmse'].notna()\n",
    "            if valid_rmse.any():\n",
    "                sns.lineplot(\n",
    "                    x=dates[valid_rmse],\n",
    "                    y=daily_metrics.loc[valid_rmse, 'rmse'],\n",
    "                    marker='o',\n",
    "                    linewidth=2,\n",
    "                    color='forestgreen',\n",
    "                    ax=axes[0, 1]\n",
    "                )\n",
    "                \n",
    "                # Add error bands (assuming std dev of 10% for illustration)\n",
    "                rmse_values = daily_metrics.loc[valid_rmse, 'rmse']\n",
    "                rmse_std = rmse_values * 0.1\n",
    "                axes[0, 1].fill_between(\n",
    "                    dates[valid_rmse], \n",
    "                    rmse_values - rmse_std,\n",
    "                    rmse_values + rmse_std,\n",
    "                    alpha=0.2,\n",
    "                    color='forestgreen'\n",
    "                )\n",
    "                \n",
    "                axes[0, 1].set_title('Root Mean Squared Error by Day', fontsize=14)\n",
    "                axes[0, 1].set_xlabel('Date', fontsize=12)\n",
    "                axes[0, 1].set_ylabel('RMSE', fontsize=12)\n",
    "        \n",
    "        # Plot 3: Actual vs Predicted with improved styling\n",
    "        if len(all_results) > 0 and 'price' in all_results.columns and 'predicted' in all_results.columns:\n",
    "            # Filter out rows with NaN values\n",
    "            valid_results = all_results.dropna(subset=['price', 'predicted', 'date'])\n",
    "            \n",
    "            if len(valid_results) > 0:\n",
    "                valid_results['date_str'] = pd.to_datetime(valid_results['date']).dt.strftime('%Y-%m-%d')\n",
    "                \n",
    "                # Create colormap based on date\n",
    "                unique_dates = pd.to_datetime(valid_results['date']).dt.date.unique()\n",
    "                date_map = {date: i for i, date in enumerate(unique_dates)}\n",
    "                valid_results['date_num'] = pd.to_datetime(valid_results['date']).dt.date.map(date_map)\n",
    "                \n",
    "                scatter = axes[1, 0].scatter(\n",
    "                    valid_results['price'],\n",
    "                    valid_results['predicted'],\n",
    "                    c=valid_results['date_num'],\n",
    "                    cmap='viridis',\n",
    "                    alpha=0.7,\n",
    "                    edgecolors='w',\n",
    "                    linewidths=0.2\n",
    "                )\n",
    "                \n",
    "                # Calculate and plot regression line\n",
    "                min_val = min(valid_results['price'].min(), valid_results['predicted'].min())\n",
    "                max_val = max(valid_results['price'].max(), valid_results['predicted'].max())\n",
    "                \n",
    "                # Perfect prediction line\n",
    "                axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, label='Perfect Prediction')\n",
    "                \n",
    "                # Add regression line if we have enough points\n",
    "                if len(valid_results) >= 2:\n",
    "                    z = np.polyfit(valid_results['price'], valid_results['predicted'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_reg = np.linspace(min_val, max_val, 100)\n",
    "                    axes[1, 0].plot(x_reg, p(x_reg), color='red', linestyle='-', alpha=0.7, \n",
    "                                  label=f'Regression Line (slope={z[0]:.2f})')\n",
    "                \n",
    "                axes[1, 0].set_title('Actual vs Predicted Prices', fontsize=14)\n",
    "                axes[1, 0].set_xlabel('Actual Price', fontsize=12)\n",
    "                axes[1, 0].set_ylabel('Predicted Price', fontsize=12)\n",
    "                axes[1, 0].legend(loc='upper left')\n",
    "                \n",
    "                # Add colorbar for dates\n",
    "                cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "                cbar.set_label('Date Progression')\n",
    "        \n",
    "        # Plot 4: Error distribution\n",
    "        if len(all_results) > 0 and 'error' in all_results.columns:\n",
    "            all_results['error'] = all_results['price'] - all_results['predicted']\n",
    "            \n",
    "            # Remove outliers for better visualization (keep within 3 std deviations)\n",
    "            mean_error = all_results['error'].mean()\n",
    "            std_error = all_results['error'].std()\n",
    "            \n",
    "            lower_bound = mean_error - 3 * std_error\n",
    "            upper_bound = mean_error + 3 * std_error\n",
    "            \n",
    "            filtered_errors = all_results['error'][(all_results['error'] >= lower_bound) & \n",
    "                                                   (all_results['error'] <= upper_bound)]\n",
    "            \n",
    "            if len(filtered_errors) > 0:\n",
    "                sns.histplot(\n",
    "                    filtered_errors, \n",
    "                    kde=True, \n",
    "                    ax=axes[1, 1],\n",
    "                    bins=min(30, len(filtered_errors) // 10 + 5),\n",
    "                    color='darkviolet',\n",
    "                    edgecolor='white',\n",
    "                    linewidth=0.5,\n",
    "                    stat='density'\n",
    "                )\n",
    "                \n",
    "                # Add vertical lines for mean and median\n",
    "                axes[1, 1].axvline(filtered_errors.mean(), color='red', linestyle='--', alpha=0.7, \n",
    "                                  label=f'Mean: {filtered_errors.mean():.3f}')\n",
    "                axes[1, 1].axvline(filtered_errors.median(), color='green', linestyle='--', alpha=0.7,\n",
    "                                  label=f'Median: {filtered_errors.median():.3f}')\n",
    "                axes[1, 1].axvline(0, color='blue', linestyle='-', alpha=0.7, label='Zero Error')\n",
    "                \n",
    "                axes[1, 1].set_title('Error Distribution (Outliers Filtered)', fontsize=14)\n",
    "                axes[1, 1].set_xlabel('Error (Actual - Predicted)', fontsize=12)\n",
    "                axes[1, 1].set_ylabel('Density', fontsize=12)\n",
    "                axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        try:\n",
    "            plt.savefig('enhanced_strap_performance.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"Saved performance plot to 'enhanced_strap_performance.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save plot: {e}\")\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        # Create additional plot for sample size and MAPE if we have data\n",
    "        if len(daily_metrics) > 0 and 'n_samples' in daily_metrics.columns and 'mape' in daily_metrics.columns:\n",
    "            valid_metrics = daily_metrics.dropna(subset=['n_samples', 'mape'])\n",
    "            \n",
    "            if len(valid_metrics) > 0:\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                ax1 = plt.gca()\n",
    "                ax2 = ax1.twinx()\n",
    "                \n",
    "                # Format dates for x-axis\n",
    "                date_labels = pd.to_datetime(valid_metrics['date']).dt.strftime('%Y-%m-%d')\n",
    "                \n",
    "                # Sample size bars\n",
    "                bars = ax1.bar(\n",
    "                    date_labels,\n",
    "                    valid_metrics['n_samples'],\n",
    "                    color='skyblue',\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='navy',\n",
    "                    linewidth=1\n",
    "                )\n",
    "                \n",
    "                # Add data labels on top of bars\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    ax1.text(\n",
    "                        bar.get_x() + bar.get_width()/2., \n",
    "                        height + 5,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=9,\n",
    "                        rotation=0\n",
    "                    )\n",
    "                \n",
    "                ax1.set_xlabel('Date', fontsize=12)\n",
    "                ax1.set_ylabel('Number of Samples', color='navy', fontsize=12)\n",
    "                plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "                \n",
    "                # MAPE line\n",
    "                sns.lineplot(\n",
    "                    x=date_labels,\n",
    "                    y=valid_metrics['mape'],\n",
    "                    marker='o',\n",
    "                    markersize=8,\n",
    "                    color='crimson',\n",
    "                    linewidth=2,\n",
    "                    ax=ax2\n",
    "                )\n",
    "                \n",
    "                # Add data labels to line points\n",
    "                for i, mape in enumerate(valid_metrics['mape']):\n",
    "                    ax2.annotate(\n",
    "                        f'{mape:.2f}%', \n",
    "                        (i, mape),\n",
    "                        textcoords=\"offset points\",\n",
    "                        xytext=(0, 10),\n",
    "                        ha='center',\n",
    "                        fontsize=9,\n",
    "                        color='crimson'\n",
    "                    )\n",
    "                \n",
    "                ax2.set_ylabel('MAPE (%)', color='crimson', fontsize=12)\n",
    "                ax2.tick_params(axis='y', colors='crimson')\n",
    "                \n",
    "                plt.title('Sample Size and Mean Absolute Percentage Error by Day', fontsize=14)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                try:\n",
    "                    plt.savefig('enhanced_strap_mape_samples.png', dpi=300, bbox_inches='tight')\n",
    "                    print(\"Saved MAPE plot to 'enhanced_strap_mape_samples.png'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not save MAPE plot: {e}\")\n",
    "                    \n",
    "                plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_enhanced_results: {e}\")\n",
    "        print(\"Skipping visualization.\")\n",
    "\n",
    "\n",
    "def print_detailed_summary(evaluation_results):\n",
    "    \"\"\"Print a detailed performance summary with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        overall = evaluation_results['overall_metrics']\n",
    "        daily = evaluation_results['daily_metrics']\n",
    "        error_autocorr = evaluation_results['error_autocorrelation']\n",
    "        error_stability = evaluation_results['error_stability']\n",
    "        all_results = evaluation_results['all_results']\n",
    "        \n",
    "        # Check if we have valid data\n",
    "        if len(all_results) == 0:\n",
    "            print(\"No valid results to summarize.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"{' ENHANCED ST-RAP MODEL EVALUATION ':=^80}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*35 + \" OVERALL METRICS \" + \"-\"*35)\n",
    "        print(f\"{'RMSE:':<25} {overall['rmse']:.4f}\")\n",
    "        print(f\"{'MAE:':<25} {overall['mae']:.4f}\")\n",
    "        print(f\"{'R:':<25} {overall['r2']:.4f}\")\n",
    "        print(f\"{'MAPE:':<25} {overall['mape']:.4f}%\")\n",
    "        \n",
    "        # Distribution of errors with error handling\n",
    "        errors = all_results['error'].dropna()\n",
    "        abs_errors = all_results['abs_error'].dropna()\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            error_skew = errors.skew()\n",
    "            error_kurtosis = errors.kurtosis()\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*35 + \" ERROR DISTRIBUTION \" + \"-\"*34)\n",
    "            print(f\"{'Mean Error:':<25} {errors.mean():.4f}\")\n",
    "            print(f\"{'Median Error:':<25} {errors.median():.4f}\")\n",
    "            print(f\"{'Error Std Dev:':<25} {errors.std():.4f}\")\n",
    "            print(f\"{'Error Skewness:':<25} {error_skew:.4f} ({'Symmetric' if abs(error_skew) < 0.5 else 'Skewed'})\")\n",
    "            print(f\"{'Error Kurtosis:':<25} {error_kurtosis:.4f}\")\n",
    "            \n",
    "            # Calculate percentiles\n",
    "            error_percentiles = {\n",
    "                '5%': errors.quantile(0.05),\n",
    "                '25%': errors.quantile(0.25),\n",
    "                '50%': errors.median(),\n",
    "                '75%': errors.quantile(0.75),\n",
    "                '95%': errors.quantile(0.95),\n",
    "            }\n",
    "            \n",
    "            abs_error_percentiles = {\n",
    "                '5%': abs_errors.quantile(0.05),\n",
    "                '25%': abs_errors.quantile(0.25),\n",
    "                '50%': abs_errors.median(),\n",
    "                '75%': abs_errors.quantile(0.75),\n",
    "                '95%': abs_errors.quantile(0.95),\n",
    "            }\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*30 + \" ERROR PERCENTILES (SIGNED) \" + \"-\"*30)\n",
    "            for label, value in error_percentiles.items():\n",
    "                print(f\"{label + ':':<25} {value:.4f}\")\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*30 + \" ERROR PERCENTILES (ABSOLUTE) \" + \"-\"*29)\n",
    "            for label, value in abs_error_percentiles.items():\n",
    "                print(f\"{label + ':':<25} {value:.4f}\")\n",
    "        \n",
    "        if len(daily) > 0:\n",
    "            print(\"\\n\" + \"-\"*35 + \" DAILY PERFORMANCE \" + \"-\"*34)\n",
    "            print(daily[['date', 'rmse', 'mae', 'r2', 'mape', 'n_samples']].to_string(index=False))\n",
    "            \n",
    "            # Check if we have enough data for statistics\n",
    "            if len(daily['mae'].dropna()) > 0:\n",
    "                # Calculate additional metrics\n",
    "                mae_cv = daily['mae'].std() / daily['mae'].mean() if daily['mae'].mean() != 0 else float('inf')\n",
    "                rmse_cv = daily['rmse'].std() / daily['rmse'].mean() if daily['rmse'].mean() != 0 else float('inf')\n",
    "                \n",
    "                print(\"\\n\" + \"-\"*33 + \" PERFORMANCE STATISTICS \" + \"-\"*33)\n",
    "                print(\"MAE:\")\n",
    "                print(f\"  {'Average:':<20} {daily['mae'].mean():.4f}\")\n",
    "                print(f\"  {'Std Dev:':<20} {daily['mae'].std():.4f}\")\n",
    "                print(f\"  {'CV (Stability):':<20} {mae_cv:.4f} ({'Stable' if mae_cv < 0.1 else 'Variable'})\")\n",
    "                \n",
    "                if not daily['mae'].isna().all():\n",
    "                    min_idx = daily['mae'].idxmin()\n",
    "                    max_idx = daily['mae'].idxmax()\n",
    "                    print(f\"  {'Min:':<20} {daily['mae'].min():.4f} (Day: {daily.loc[min_idx, 'date']})\")\n",
    "                    print(f\"  {'Max:':<20} {daily['mae'].max():.4f} (Day: {daily.loc[max_idx, 'date']})\")\n",
    "                \n",
    "                print(\"\\nRMSE:\")\n",
    "                print(f\"  {'Average:':<20} {daily['rmse'].mean():.4f}\")\n",
    "                print(f\"  {'Std Dev:':<20} {daily['rmse'].std():.4f}\")\n",
    "                print(f\"  {'CV (Stability):':<20} {rmse_cv:.4f}\")\n",
    "                \n",
    "                if not daily['rmse'].isna().all():\n",
    "                    min_idx = daily['rmse'].idxmin()\n",
    "                    max_idx = daily['rmse'].idxmax()\n",
    "                    print(f\"  {'Min:':<20} {daily['rmse'].min():.4f} (Day: {daily.loc[min_idx, 'date']})\")\n",
    "                    print(f\"  {'Max:':<20} {daily['rmse'].max():.4f} (Day: {daily.loc[max_idx, 'date']})\")\n",
    "                \n",
    "                print(\"\\nMAPE:\")\n",
    "                print(f\"  {'Average:':<20} {daily['mape'].mean():.2f}%\")\n",
    "                print(f\"  {'Std Dev:':<20} {daily['mape'].std():.2f}%\")\n",
    "                \n",
    "                if not daily['mape'].isna().all():\n",
    "                    min_idx = daily['mape'].idxmin()\n",
    "                    max_idx = daily['mape'].idxmax()\n",
    "                    print(f\"  {'Min:':<20} {daily['mape'].min():.2f}% (Day: {daily.loc[min_idx, 'date']})\")\n",
    "                    print(f\"  {'Max:':<20} {daily['mape'].max():.2f}% (Day: {daily.loc[max_idx, 'date']})\")\n",
    "        \n",
    "        if isinstance(error_autocorr, np.ndarray) and len(error_autocorr) > 0:\n",
    "            print(\"\\n\" + \"-\"*35 + \" ERROR AUTOCORRELATION \" + \"-\"*33)\n",
    "            for lag, acf_value in enumerate(error_autocorr, 1):\n",
    "                significance = \"\"\n",
    "                if abs(acf_value) > (1.96 / np.sqrt(len(all_results))):\n",
    "                    significance = \" *SIGNIFICANT*\"\n",
    "                print(f\"  {'Lag ' + str(lag) + ':':<20} {acf_value:.4f}{significance}\")\n",
    "        \n",
    "        if 'mae_stability_coefficient' in error_stability:\n",
    "            print(\"\\n\" + \"-\"*38 + \" ERROR STABILITY \" + \"-\"*37)\n",
    "            print(f\"  {'MAE Stability Coef:':<25} {error_stability['mae_stability_coefficient']:.4f}\")\n",
    "            print(\"  (Lower values indicate more consistent predictions across days)\")\n",
    "        \n",
    "        # Identify best and worst performing groups if we have enough data\n",
    "        if len(all_results) >= 10:\n",
    "            print(\"\\n\" + \"-\"*35 + \" PERFORMANCE ANALYSIS \" + \"-\"*33)\n",
    "            \n",
    "            try:\n",
    "                # Group by listing_id to find consistently well/poorly predicted listings\n",
    "                listing_perf = all_results.groupby('listing_id').agg({\n",
    "                    'abs_error': 'mean',\n",
    "                    'pct_error': 'mean'\n",
    "                }).sort_values('abs_error')\n",
    "                \n",
    "                if len(listing_perf) > 0:\n",
    "                    # Get top and bottom 5 (or fewer if less are available)\n",
    "                    top_n = min(5, len(listing_perf))\n",
    "                    print(f\"\\nBest Predicted Listings (Lowest Average Absolute Error):\")\n",
    "                    print(listing_perf.head(top_n).reset_index())\n",
    "                    \n",
    "                    print(f\"\\nWorst Predicted Listings (Highest Average Absolute Error):\")\n",
    "                    print(listing_perf.tail(top_n).reset_index())\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing listing performance: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in print_detailed_summary: {e}\")\n",
    "        print(\"Could not print complete summary due to errors.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Specify paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train.csv\"\n",
    "    test_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_feb.csv\"\n",
    "    output_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Output\\NN\\enhanced_strap_results.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Make sure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Define features to drop if needed (empty list means keep all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for 7-day prediction with enhanced ST-RAP model\n",
      "Features being dropped: []\n",
      "Loading data...\n",
      "Test set contains 7 unique dates.\n",
      "Creating enhanced spatial features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing chunks for spatial features: 100%|| 1641/1641 [16:47<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced spatial features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing chunks for spatial features: 100%|| 56/56 [00:22<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to build edge index for graph structure...\n",
      "Creating spatial edge index efficiently...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1696/1696 [07:26<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built edge index with shape torch.Size([2, 26651408])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing days:   0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m features_to_drop \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Run prediction with enhanced model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_day_by_day_enhanced_strap_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_to_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_to_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Print detailed summary\u001b[39;00m\n\u001b[0;32m     25\u001b[0m print_detailed_summary(results)\n",
      "Cell \u001b[1;32mIn[1], line 521\u001b[0m, in \u001b[0;36mrun_day_by_day_enhanced_strap_prediction\u001b[1;34m(train_path, test_path, features_to_drop, prediction_days, output_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_time, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 176\u001b[0m, in \u001b[0;36mEnhancedSTRAP.forward\u001b[1;34m(self, x, edge_index, time_features)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Process with graph convolution if edge_index provided\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# First graph layer with attention\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     spatial_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemporal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Second graph layer\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     spatial_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(spatial_output)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_af3zsg1q.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[1;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[0;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[0;32m    167\u001b[0m                 alpha_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    168\u001b[0m                 alpha_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    173\u001b[0m             )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:408\u001b[0m, in \u001b[0;36mGATConv.edge_update\u001b[1;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    405\u001b[0m     alpha_edge \u001b[38;5;241m=\u001b[39m (edge_attr \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_edge)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    406\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m alpha_edge\n\u001b[1;32m--> 408\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m alpha \u001b[38;5;241m=\u001b[39m softmax(alpha, index, ptr, dim_size)\n\u001b[0;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\torch\\nn\\functional.py:1902\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1900\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Specify paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train.csv\"\n",
    "    test_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_feb.csv\"\n",
    "    output_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Output\\NN\\enhanced_strap_results.csv\"\n",
    "    \n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    features_to_drop = []\n",
    "    \n",
    "    # Set CUDA options for better error handling\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Makes CUDA errors more informative\n",
    "    \n",
    "    # Enable warnings to catch potential issues\n",
    "    warnings.filterwarnings('always')\n",
    "    \n",
    "    print(\"Starting enhanced STRAP model training and evaluation...\")\n",
    "    \n",
    "    # Run prediction with enhanced model\n",
    "    results = run_day_by_day_enhanced_strap_prediction(\n",
    "        train_path=train_path,\n",
    "        test_path=test_path,\n",
    "        features_to_drop=features_to_drop,\n",
    "        prediction_days=7,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print_detailed_summary(results)\n",
    "    \n",
    "    # Generate enhanced visualizations\n",
    "    plot_enhanced_results(results)\n",
    "    \n",
    "    print(\"\\nModel training and evaluation complete. Results and visualizations have been saved.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Critical error in main execution: {e}\")\n",
    "    print(\"Please check your data paths and environment setup.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
