{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for 7-day prediction\n",
      "Features being dropped: []\n",
      "Loading data...\n",
      "Test set contains 7 unique dates.\n",
      "Creating spatial features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing chunks:  94%|█████████▍| 646/686 [03:00<00:11,  3.58it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import BallTree\n",
    "from math import radians\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Time2VecEncoding(nn.Module):\n",
    "    \"\"\"Time2Vec encoding for temporal features\"\"\"\n",
    "    def __init__(self, h_dim, scale=1):\n",
    "        super(Time2VecEncoding, self).__init__()\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(1, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(1, h_dim-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(h_dim-1))\n",
    "        self.f = torch.sin\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, time):\n",
    "        # Ensure time is 2D: [batch_size, 1]\n",
    "        if len(time.shape) == 1:\n",
    "            time = time.unsqueeze(-1)\n",
    "        elif len(time.shape) == 3:\n",
    "            # If time has shape [batch_size, seq_len, 1], take the last time step\n",
    "            time = time[:, -1, :].unsqueeze(-1)\n",
    "            \n",
    "        # Scale time\n",
    "        time = time / self.scale\n",
    "        \n",
    "        # Calculate linear and periodic components\n",
    "        v1 = torch.matmul(time, self.w0) + self.b0  # Shape: [batch_size, 1]\n",
    "        v2 = self.f(torch.matmul(time, self.w) + self.b)  # Shape: [batch_size, h_dim-1]\n",
    "        \n",
    "        return torch.cat([v1, v2], dim=1)  # Shape: [batch_size, h_dim]\n",
    "\n",
    "class STRAP(nn.Module):\n",
    "    \"\"\"Streamlined implementation of ST-RAP model\"\"\"\n",
    "    def __init__(self, input_dim, temporal_dim, hidden_dim=128, num_gru_layers=2):\n",
    "        super(STRAP, self).__init__()\n",
    "        \n",
    "        # Feature embeddings\n",
    "        self.property_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.time_embedding = Time2VecEncoding(hidden_dim, scale=1000)\n",
    "        \n",
    "        # Temporal GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_gru_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.temporal_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Spatial graph layers\n",
    "        self.graph_conv = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.spatial_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index=None, time_features=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Property embeddings\n",
    "        property_emb = self.property_embedding(x)\n",
    "        \n",
    "        # Process time features if provided\n",
    "        if time_features is not None:\n",
    "            # Take the first feature as the temporal value\n",
    "            # In practice, you might want to use a more sophisticated approach\n",
    "            time_val = time_features[:, 0]  # Shape: [batch_size]\n",
    "            time_emb = self.time_embedding(time_val)  # Shape: [batch_size, hidden_dim]\n",
    "            property_emb = property_emb + time_emb\n",
    "        \n",
    "        # Reshape for GRU (adding sequence dimension)\n",
    "        temporal_input = property_emb.unsqueeze(1)  # Shape: [batch_size, 1, hidden_dim]\n",
    "            \n",
    "        # Process with GRU\n",
    "        temporal_output, _ = self.gru(temporal_input)  # Shape: [batch_size, 1, hidden_dim]\n",
    "        temporal_output = self.temporal_norm(temporal_output[:, -1])  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process with graph convolution if edge_index provided\n",
    "        if edge_index is not None and edge_index.numel() > 0:\n",
    "            spatial_output = self.graph_conv(temporal_output, edge_index)\n",
    "            spatial_output = self.spatial_norm(spatial_output)\n",
    "        else:\n",
    "            spatial_output = torch.zeros_like(temporal_output)\n",
    "        \n",
    "        # Combine temporal and spatial outputs\n",
    "        combined = torch.cat([temporal_output, spatial_output], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.predictor(combined).squeeze(-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_edge_index_efficient(data, k_neighbors=5, distance_threshold=2.0, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Create spatial edge index efficiently using BallTree and chunking\n",
    "    \"\"\"\n",
    "    print(\"Creating spatial edge index efficiently...\")\n",
    "    coords = data[['latitude', 'longitude']].values\n",
    "    n_samples = len(coords)\n",
    "    \n",
    "    # Convert to radians for BallTree\n",
    "    coords_rad = np.radians(coords)\n",
    "    \n",
    "    # Create BallTree\n",
    "    tree = BallTree(coords_rad, metric='haversine')\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    for i in tqdm(range(0, n_samples, chunk_size)):\n",
    "        end_idx = min(i + chunk_size, n_samples)\n",
    "        chunk_coords = coords_rad[i:end_idx]\n",
    "        \n",
    "        # Query k+1 nearest neighbors (including self)\n",
    "        distances, indices = tree.query(chunk_coords, k=k_neighbors+1)\n",
    "        \n",
    "        # Convert distances from radians to km\n",
    "        distances = distances * 6371.0  # Earth radius in km\n",
    "        \n",
    "        # Add edges for each point in chunk\n",
    "        for j in range(len(chunk_coords)):\n",
    "            point_idx = i + j\n",
    "            for k in range(1, k_neighbors+1):  # Skip self (index 0)\n",
    "                neighbor_idx = indices[j, k]\n",
    "                distance = distances[j, k]\n",
    "                \n",
    "                if distance <= distance_threshold:\n",
    "                    edge_index.append([point_idx, neighbor_idx])\n",
    "                    # Add reverse edge for undirected graph\n",
    "                    edge_index.append([neighbor_idx, point_idx])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    edge_index = list(set(tuple(edge) for edge in edge_index))\n",
    "    edge_index = [list(edge) for edge in edge_index]\n",
    "    \n",
    "    return torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def create_spatial_features(df, k_neighbors=5, chunk_size=1000, n_jobs=-1):\n",
    "    \"\"\"Create spatial features similar to XGBoost implementation\"\"\"\n",
    "    print(\"Creating spatial features...\")\n",
    "    city_center_lat, city_center_lon = 48.8566, 2.3522  # Paris coordinates\n",
    "    \n",
    "    # Calculate distance to city center and north-south position\n",
    "    df['distance_to_center'] = df.apply(\n",
    "        lambda row: calculate_distance(\n",
    "            row['latitude'], \n",
    "            row['longitude'], \n",
    "            city_center_lat, \n",
    "            city_center_lon\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df['north_south'] = df['latitude'] - city_center_lat\n",
    "    \n",
    "    # Create BallTree for nearest neighbor calculations\n",
    "    coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "    tree = BallTree(coords, metric='haversine')\n",
    "    \n",
    "    # Process in chunks\n",
    "    n_chunks = math.ceil(len(df) / chunk_size)\n",
    "    chunks = np.array_split(df, n_chunks)\n",
    "    \n",
    "    # KNN features\n",
    "    knn_price_mean = np.zeros(len(df))\n",
    "    knn_price_std = np.zeros(len(df))\n",
    "    knn_price_median = np.zeros(len(df))\n",
    "    price_diff = np.zeros(len(df))\n",
    "    \n",
    "    start_idx = 0\n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        chunk_size = len(chunk)\n",
    "        chunk_coords = np.radians(chunk[['latitude', 'longitude']].values)\n",
    "        \n",
    "        # Find k+1 nearest neighbors (including self)\n",
    "        distances, indices = tree.query(chunk_coords, k=k_neighbors+1)\n",
    "        \n",
    "        # Calculate neighbor statistics\n",
    "        for i in range(chunk_size):\n",
    "            # Skip self (index 0)\n",
    "            neighbor_indices = indices[i, 1:]\n",
    "            prices = df.iloc[neighbor_indices]['price'].values\n",
    "            \n",
    "            # Calculate statistics\n",
    "            knn_price_mean[start_idx + i] = np.mean(prices)\n",
    "            knn_price_std[start_idx + i] = np.std(prices)\n",
    "            knn_price_median[start_idx + i] = np.median(prices)\n",
    "            price_diff[start_idx + i] = chunk.iloc[i]['price'] - np.mean(prices)\n",
    "        \n",
    "        start_idx += chunk_size\n",
    "    \n",
    "    # Add features to dataframe\n",
    "    df['knn_price_mean'] = knn_price_mean\n",
    "    df['knn_price_std'] = knn_price_std\n",
    "    df['knn_price_median'] = knn_price_median\n",
    "    df['price_diff_from_neighbors'] = price_diff\n",
    "    \n",
    "    # Standardize the new features\n",
    "    spatial_features = ['distance_to_center', 'north_south', \n",
    "                        'knn_price_mean', 'knn_price_std', \n",
    "                        'knn_price_median', 'price_diff_from_neighbors']\n",
    "    \n",
    "    for col in spatial_features:\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std()\n",
    "        if std_val > 0:\n",
    "            df[col] = (df[col] - mean_val) / std_val\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_error_autocorrelation(errors, max_lag=7):\n",
    "    \"\"\"Calculate autocorrelation of prediction errors\"\"\"\n",
    "    # Calculate autocorrelation\n",
    "    error_acf = acf(errors, nlags=max_lag)\n",
    "    \n",
    "    # Return values excluding lag 0 (which is always 1)\n",
    "    return error_acf[1:]\n",
    "\n",
    "def calculate_error_stability(all_results):\n",
    "    \"\"\"\n",
    "    Calculate error stability metrics:\n",
    "    1. Standard deviation of errors within each day\n",
    "    2. Consistency of MAE across days\n",
    "    \"\"\"\n",
    "    # Group by date\n",
    "    grouped = all_results.groupby('date_str')\n",
    "    \n",
    "    # Calculate standard deviation of errors for each day\n",
    "    daily_error_std = grouped['error'].std()\n",
    "    \n",
    "    # Calculate MAE for each day\n",
    "    daily_mae = grouped['abs_error'].mean()\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    mae_stability = daily_mae.std() / daily_mae.mean()  # Lower is better\n",
    "    \n",
    "    return {\n",
    "        'daily_error_std': daily_error_std,\n",
    "        'daily_mae': daily_mae,\n",
    "        'mae_stability_coefficient': mae_stability\n",
    "    }\n",
    "\n",
    "def run_day_by_day_strap_prediction(train_path, test_path, features_to_drop=None, prediction_days=7):\n",
    "    \"\"\"Run ST-RAP model with day-by-day retraining for multiple days prediction\"\"\"\n",
    "    print(f\"Processing dataset for {prediction_days}-day prediction\")\n",
    "    \n",
    "    # Default features to drop if none specified\n",
    "    if features_to_drop is None:\n",
    "        features_to_drop = []\n",
    "    \n",
    "    print(f\"Features being dropped: {features_to_drop}\")\n",
    "    \n",
    "    # Load training and test data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "    \n",
    "    # Drop specified columns if they exist\n",
    "    for col in features_to_drop:\n",
    "        if col in train_data.columns:\n",
    "            print(f\"Dropping column: {col}\")\n",
    "            train_data = train_data.drop(col, axis=1)\n",
    "        if col in test_data.columns:\n",
    "            test_data = test_data.drop(col, axis=1)\n",
    "    \n",
    "    # Sort by date\n",
    "    train_data = train_data.sort_values('date')\n",
    "    test_data = test_data.sort_values('date')\n",
    "    \n",
    "    # Get unique dates in test set\n",
    "    test_dates = test_data['date'].dt.date.unique()\n",
    "    print(f\"Test set contains {len(test_dates)} unique dates.\")\n",
    "    \n",
    "    # Limit to specified prediction days\n",
    "    if len(test_dates) > prediction_days:\n",
    "        test_dates = test_dates[:prediction_days]\n",
    "        print(f\"Limited to first {prediction_days} days for prediction.\")\n",
    "    \n",
    "    # Add spatial features to enhance prediction\n",
    "    train_data = create_spatial_features(train_data)\n",
    "    test_data = create_spatial_features(test_data)\n",
    "    \n",
    "    # Create feature matrices    \n",
    "    feature_cols = [col for col in train_data.columns \n",
    "                  if col not in ['listing_id', 'date', 'price']]\n",
    "    \n",
    "    X_train = train_data[feature_cols].values\n",
    "    y_train = train_data['price'].values\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = STRAP(\n",
    "        input_dim=len(feature_cols),\n",
    "        temporal_dim=5,  # Temporal features: day, month, weekend, season_sin, season_cos\n",
    "        hidden_dim=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    daily_results = []\n",
    "    \n",
    "    # We won't use graph structure to avoid computational overhead\n",
    "    # Set edge_index to None\n",
    "    edge_index = None\n",
    "    \n",
    "    # Process each day in the test set\n",
    "    for day in tqdm(test_dates, desc=\"Processing days\"):\n",
    "        # Convert day to datetime for filtering\n",
    "        day_dt = pd.to_datetime(day)\n",
    "        \n",
    "        # Get test data for the current day\n",
    "        day_test = test_data[test_data['date'].dt.date == day]\n",
    "        \n",
    "        X_test_day = day_test[feature_cols].values\n",
    "        y_test_day = day_test['price'].values\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_day).to(device)\n",
    "        y_test_tensor = torch.FloatTensor(y_test_day).to(device)\n",
    "        \n",
    "        # Extract time features\n",
    "        time_cols = ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
    "        if all(col in train_data.columns for col in time_cols):\n",
    "            time_features_train = torch.FloatTensor(train_data[time_cols].values).to(device)\n",
    "            time_features_test = torch.FloatTensor(day_test[time_cols].values).to(device)\n",
    "        else:\n",
    "            # If time columns don't exist, create dummy time features\n",
    "            print(\"Warning: Time columns not found. Using dummy time features.\")\n",
    "            time_features_train = torch.zeros((len(X_train_tensor), 5), device=device)\n",
    "            time_features_test = torch.zeros((len(X_test_tensor), 5), device=device)\n",
    "        \n",
    "        # Create DataLoader for training\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, time_features_train, y_train_tensor)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Train model\n",
    "        model.train()\n",
    "        epochs = 3  # Reduced epochs for faster training\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "            for batch_x, batch_time, batch_y in train_loader:\n",
    "                # Forward pass - pass None for edge_index\n",
    "                outputs = model(batch_x, None, batch_time)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Print epoch stats\n",
    "            avg_loss = total_loss/batch_count if batch_count > 0 else 0\n",
    "            print(f\"Day: {day}, Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor, None, time_features_test)\n",
    "            test_loss = criterion(y_pred, y_test_tensor).item()\n",
    "        \n",
    "        # Convert predictions to numpy\n",
    "        y_pred_np = y_pred.cpu().numpy()\n",
    "        \n",
    "        # Store results for the day\n",
    "        day_results = pd.DataFrame({\n",
    "            'date': day_test['date'],\n",
    "            'listing_id': day_test['listing_id'],\n",
    "            'actual': y_test_day,\n",
    "            'predicted': y_pred_np,\n",
    "            'abs_error': np.abs(y_test_day - y_pred_np),\n",
    "            'pct_error': np.abs((y_test_day - y_pred_np) / y_test_day) * 100\n",
    "        })\n",
    "        \n",
    "        daily_results.append(day_results)\n",
    "        \n",
    "        # Update training data with the current day's actual values\n",
    "        # This simulates getting actual values at the end of each day\n",
    "        # before predicting the next day\n",
    "        X_train = np.concatenate([X_train, X_test_day])\n",
    "        y_train = np.concatenate([y_train, y_test_day])\n",
    "        train_data = pd.concat([train_data, day_test], ignore_index=True)\n",
    "    \n",
    "    # Combine all daily results\n",
    "    all_results = pd.concat(daily_results, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    y_true = all_results['actual'].values\n",
    "    y_pred = all_results['predicted'].values\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100  # Added small epsilon to avoid division by zero\n",
    "    }\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    daily_metrics = []\n",
    "    for day_df in daily_results:\n",
    "        day = day_df['date'].iloc[0]\n",
    "        y_true_day = day_df['actual'].values\n",
    "        y_pred_day = day_df['predicted'].values\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,  # Added small epsilon\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    \n",
    "    # Create evaluation results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': metrics,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results\n",
    "    }\n",
    "    \n",
    "    # Calculate error autocorrelation\n",
    "    all_results['error'] = all_results['actual'] - all_results['predicted']\n",
    "    error_autocorrelation = calculate_error_autocorrelation(all_results['error'].values)\n",
    "\n",
    "    # Add date_str column for grouping\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Calculate error stability metrics\n",
    "    error_stability = calculate_error_stability(all_results)\n",
    "\n",
    "    # Add to evaluation results\n",
    "    evaluation_results['error_autocorrelation'] = error_autocorrelation\n",
    "    evaluation_results['error_stability'] = error_stability\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "def plot_results(evaluation_results):\n",
    "    \"\"\"Plot the results from predictions\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    \n",
    "    # Plot 2: Daily RMSE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['rmse'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 1]\n",
    "    )\n",
    "    axes[0, 1].set_title('Root Mean Squared Error by Day')\n",
    "    axes[0, 1].set_xlabel('Date')\n",
    "    axes[0, 1].set_ylabel('RMSE')\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['actual'],\n",
    "        all_results['predicted'],\n",
    "        c=pd.factorize(all_results['date_str'])[0],\n",
    "        alpha=0.6\n",
    "    )\n",
    "    min_val = min(all_results['actual'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['actual'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by Day)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Create legend for the scatter plot\n",
    "    legend1 = axes[1, 0].legend(*scatter.legend_elements(),\n",
    "                           title=\"Day\", loc=\"upper left\", bbox_to_anchor=(1.05, 1))\n",
    "    axes[1, 0].add_artist(legend1)\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    all_results['error'] = all_results['actual'] - all_results['predicted']\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create additional plot for sample size and MAPE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax1 = plt.gca()\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    sns.barplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']).dt.strftime('%Y-%m-%d'),\n",
    "        y=daily_metrics['n_samples'],\n",
    "        color='skyblue',\n",
    "        alpha=0.7,\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Samples', color='blue')\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']).dt.strftime('%Y-%m-%d'),\n",
    "        y=daily_metrics['mape'],\n",
    "        marker='o',\n",
    "        color='red',\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_ylabel('MAPE (%)', color='red')\n",
    "    \n",
    "    plt.title('Sample Size and MAPE by Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_analysis(evaluation_results):\n",
    "    \"\"\"Plot error autocorrelation and stability metrics\"\"\"\n",
    "    all_results = evaluation_results['all_results']\n",
    "    error_stability = evaluation_results['error_stability']\n",
    "    \n",
    "    # Set up figure with subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Plot error autocorrelation\n",
    "    error_acf = acf(all_results['error'].values, nlags=7)\n",
    "    lags = range(len(error_acf))\n",
    "    \n",
    "    axes[0].bar(lags, error_acf, alpha=0.7)\n",
    "    axes[0].axhline(y=0, linestyle='--', color='gray')\n",
    "    \n",
    "    # Add confidence intervals (95%)\n",
    "    conf_interval = 1.96 / np.sqrt(len(all_results['error']))\n",
    "    axes[0].axhline(y=conf_interval, linestyle='--', color='red', alpha=0.5)\n",
    "    axes[0].axhline(y=-conf_interval, linestyle='--', color='red', alpha=0.5)\n",
    "    \n",
    "    axes[0].set_xlabel('Lag')\n",
    "    axes[0].set_ylabel('Autocorrelation')\n",
    "    axes[0].set_title('Error Autocorrelation')\n",
    "    \n",
    "    # Plot error stability (daily MAE and error std dev)\n",
    "    daily_mae = error_stability['daily_mae']\n",
    "    daily_error_std = error_stability['daily_error_std']\n",
    "    \n",
    "    ax1 = axes[1]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot MAE\n",
    "    ax1.plot(daily_mae.index, daily_mae.values, 'b-', marker='o', label='MAE')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Mean Absolute Error', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    # Plot error std dev\n",
    "    ax2.plot(daily_error_std.index, daily_error_std.values, 'r-', marker='x', label='Error Std Dev')\n",
    "    ax2.set_ylabel('Error Standard Deviation', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    # Add legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    axes[1].set_title('Error Stability Over Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_summary(evaluation_results):\n",
    "    \"\"\"Print a summary of performance\"\"\"\n",
    "    overall = evaluation_results['overall_metrics']\n",
    "    daily = evaluation_results['daily_metrics']\n",
    "    error_autocorr = evaluation_results['error_autocorrelation']\n",
    "    error_stability = evaluation_results['error_stability']\n",
    "    \n",
    "    print(\"\\n===== ST-RAP MODEL EVALUATION =====\")\n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall['mae']:.4f}\")\n",
    "    print(f\"R²: {overall['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall['mape']:.4f}%\")\n",
    "    \n",
    "    print(\"\\n=== Daily Performance ===\")\n",
    "    print(daily[['date', 'rmse', 'mae', 'mape', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Performance Statistics ===\")\n",
    "    print(\"MAE:\")\n",
    "    print(f\"  Average: {daily['mae'].mean():.4f}\")\n",
    "    print(f\"  Min: {daily['mae'].min():.4f} (Day: {daily.loc[daily['mae'].idxmin(), 'date']})\")\n",
    "    print(f\"  Max: {daily['mae'].max():.4f} (Day: {daily.loc[daily['mae'].idxmax(), 'date']})\")\n",
    "    \n",
    "    print(\"\\nRMSE:\")\n",
    "    print(f\"  Average: {daily['rmse'].mean():.4f}\")\n",
    "    print(f\"  Min: {daily['rmse'].min():.4f} (Day: {daily.loc[daily['rmse'].idxmin(), 'date']})\")\n",
    "    print(f\"  Max: {daily['rmse'].max():.4f} (Day: {daily.loc[daily['rmse'].idxmax(), 'date']})\")\n",
    "    \n",
    "    print(\"\\nMAPE:\")\n",
    "    print(f\"  Average: {daily['mape'].mean():.2f}%\")\n",
    "    print(f\"  Min: {daily['mape'].min():.2f}% (Day: {daily.loc[daily['mape'].idxmin(), 'date']})\")\n",
    "    print(f\"  Max: {daily['mape'].max():.2f}% (Day: {daily.loc[daily['mape'].idxmax(), 'date']})\")\n",
    "    \n",
    "    print(\"\\n=== Error Autocorrelation ===\")\n",
    "    for lag, acf_value in enumerate(error_autocorr, 1):\n",
    "        print(f\"  Lag {lag}: {acf_value:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Error Stability ===\")\n",
    "    print(f\"  MAE Stability Coefficient: {error_stability['mae_stability_coefficient']:.4f}\")\n",
    "    print(\"  (Lower values indicate more consistent predictions across days)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Specify paths to your data\n",
    "    train_path=r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\richest_neighborhoods_subset\\train.csv\"\n",
    "    test_path=r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\richest_neighborhoods_subset\\test_feb.csv\"\n",
    "    \n",
    "    # Run prediction for 7 days\n",
    "    results = run_day_by_day_strap_prediction(\n",
    "        train_path=train_path,\n",
    "        test_path=test_path,\n",
    "        prediction_days=7\n",
    "    )\n",
    "    \n",
    "    # Print summary and plot results\n",
    "    print_summary(results)\n",
    "    plot_results(results)\n",
    "\n",
    "    # Add new error analysis plots\n",
    "    plot_error_analysis(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
