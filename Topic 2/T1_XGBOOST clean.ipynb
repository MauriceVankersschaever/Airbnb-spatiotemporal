{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:844: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:844: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\mvk\\AppData\\Local\\Temp\\ipykernel_20100\\4066011582.py:844: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  output_dir = \".\\output\\improved_xgboost\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Date range in filtered data: 2023-08-07 00:00:00 to 2024-02-08 00:00:00\n",
      "Number of days with data: 186\n",
      "Warning: No data found between 2023-07-08 and 2023-08-07\n",
      "Using available data starting from 2023-08-07\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "\n",
      "Split 1/5\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 903142 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "Found time feature columns: ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
      "Training on 903142 samples with 380 features\n",
      "Testing on 11011 samples\n",
      "Sample feature names: ['latitude', 'longitude', 'accommodates', 'bedrooms', 'has_kitchen']\n",
      "Inverting log transformation for predictions\n",
      "Split 1 Results - RMSE: 56.3624, MAE: 24.0745, R²: 0.8777, MAPE: 8.96%\n",
      "\n",
      "Split 2/5\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 947179 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "Found time feature columns: ['DTF_day_of_week', 'DTF_month', 'DTF_is_weekend', 'DTF_season_sin', 'DTF_season_cos']\n",
      "Training on 947179 samples with 394 features\n",
      "Testing on 11011 samples\n",
      "Sample feature names: ['latitude', 'longitude', 'accommodates', 'bedrooms', 'has_kitchen']\n",
      "Inverting log transformation for predictions\n",
      "Split 2 Results - RMSE: 34.4140, MAE: 11.7936, R²: 0.9522, MAPE: 3.73%\n",
      "\n",
      "Split 3/5\n",
      "Training period: 2023-08-07 to 2024-01-18\n",
      "Testing period: 2024-01-19 to 2024-01-25\n",
      "Train data: 991216 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n"
     ]
    }
   ],
   "source": [
    "#with history\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, max_error, median_absolute_error\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from xgboost import XGBRegressor\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def create_advanced_spatial_features(df, k_neighbors=5, train_data_only=None):\n",
    "    \"\"\"\n",
    "    Create both simple and advanced spatial features without using complex price information\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The data to create features for\n",
    "    k_neighbors : int\n",
    "        Number of neighbors to use for spatial clustering\n",
    "    train_data_only : DataFrame, optional\n",
    "        If provided, use only this data for computing the spatial features\n",
    "        This prevents data leakage by ensuring test listings don't influence each other\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame containing the spatial features\n",
    "    \"\"\"\n",
    "    # Create an explicit copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reference data for computing spatial relationships\n",
    "    reference_data = train_data_only if train_data_only is not None else df\n",
    "    \n",
    "    city_center_lat, city_center_lon = 48.8566, 2.3522\n",
    "    \n",
    "    # Basic spatial features\n",
    "    spatial_data = {\n",
    "        'distance_to_center': df.apply(\n",
    "            lambda row: calculate_distance(\n",
    "                row['latitude'], \n",
    "                row['longitude'], \n",
    "                city_center_lat, \n",
    "                city_center_lon\n",
    "            ),\n",
    "            axis=1\n",
    "        ).values,\n",
    "        'north_south': (df['latitude'] - city_center_lat).values\n",
    "    }\n",
    "    \n",
    "    # Add more advanced spatial features\n",
    "    from sklearn.neighbors import BallTree\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create latitude/longitude arrays for spatial calculation\n",
    "    # In create_advanced_spatial_features:\n",
    "    coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "    ref_coords = np.radians(reference_data[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Build BallTree for efficiently finding nearest neighbors\n",
    "    tree = BallTree(ref_coords, metric='haversine')\n",
    "    \n",
    "    # Find k+1 nearest neighbors (including self)\n",
    "    k_query = min(k_neighbors + 1, len(ref_coords))\n",
    "    distances, indices = tree.query(coords, k=k_query)\n",
    "    \n",
    "    # Calculate neighborhood density (inverse of average distance to neighbors)\n",
    "    # Skip the first neighbor (self)\n",
    "    if k_query > 1:\n",
    "        avg_neighbor_distance = np.mean(distances[:, 1:], axis=1)\n",
    "        spatial_data['neighborhood_density'] = 1 / (avg_neighbor_distance + 1e-6)\n",
    "    else:\n",
    "        spatial_data['neighborhood_density'] = np.zeros(len(df))\n",
    "    \n",
    "    # Calculate the standard deviation of distances to neighbors\n",
    "    if k_query > 1:\n",
    "        spatial_data['neighbor_distance_std'] = np.std(distances[:, 1:], axis=1)\n",
    "    else:\n",
    "        spatial_data['neighbor_distance_std'] = np.zeros(len(df))\n",
    "    \n",
    "    # Calculate the radius covering all neighbors\n",
    "    if k_query > 1:\n",
    "        spatial_data['neighbor_radius'] = np.max(distances[:, 1:], axis=1)\n",
    "    else:\n",
    "        spatial_data['neighbor_radius'] = np.zeros(len(df))\n",
    "    \n",
    "    # Create DataFrame with features\n",
    "    feature_df = pd.DataFrame(spatial_data, index=df.index)\n",
    "    \n",
    "    # Add neighborhood categorical features if they exist\n",
    "    if 'neighbourhood_cleansed_encoded' in df.columns:\n",
    "        feature_df['neighbourhood_encoded'] = df['neighbourhood_cleansed_encoded']\n",
    "    \n",
    "    # Standardize features\n",
    "    for col in feature_df.columns:\n",
    "        # Skip categorical neighborhood encoding\n",
    "        if col == 'neighbourhood_encoded':\n",
    "            continue\n",
    "            \n",
    "        mean_val = feature_df[col].mean()\n",
    "        std_val = feature_df[col].std()\n",
    "        # Avoid division by zero\n",
    "        if std_val > 0:\n",
    "            feature_df[col] = (feature_df[col] - mean_val) / std_val\n",
    "        else:\n",
    "            feature_df[col] = 0\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "   \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        The dataframe containing price data\n",
    "    inverse : bool\n",
    "        If True, apply inverse transformation; otherwise apply log transformation\n",
    "       \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Modified dataframe with transformed prices\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "   \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "   \n",
    "    return df\n",
    "\n",
    "def create_simplified_features(df):\n",
    "    \"\"\"\n",
    "    Create simplified spatial and categorical features without complex neighborhood calculations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The data to create features for\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame containing the spatial and other features\n",
    "    \"\"\"\n",
    "    # Create an explicit copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    city_center_lat, city_center_lon = 48.8566, 2.3522\n",
    "    \n",
    "    # Create basic spatial features\n",
    "    spatial_data = {\n",
    "        'distance_to_center': df.apply(\n",
    "            lambda row: calculate_distance(\n",
    "                row['latitude'], \n",
    "                row['longitude'], \n",
    "                city_center_lat, \n",
    "                city_center_lon\n",
    "            ),\n",
    "            axis=1\n",
    "        ).values,\n",
    "        'north_south': (df['latitude'] - city_center_lat).values    }\n",
    "    \n",
    "    # Create DataFrame with features\n",
    "    feature_df = pd.DataFrame(spatial_data, index=df.index)\n",
    "    \n",
    "    # Standardize features\n",
    "    for col in feature_df.columns:\n",
    "        mean_val = feature_df[col].mean()\n",
    "        std_val = feature_df[col].std()\n",
    "        # Avoid division by zero\n",
    "        if std_val > 0:\n",
    "            feature_df[col] = (feature_df[col] - mean_val) / std_val\n",
    "        else:\n",
    "            feature_df[col] = 0\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def compute_error_stability_metrics(results_df):\n",
    "    \"\"\"\n",
    "    Compute metrics related to error stability across listings and time\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : DataFrame\n",
    "        DataFrame containing prediction results with 'error', 'listing_id', 'date', etc.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing error stability metrics\n",
    "    \"\"\"\n",
    "    # Error stability over time (by day)\n",
    "    daily_error_std = results_df.groupby(pd.to_datetime(results_df['date']).dt.date)['error'].std().mean()\n",
    "    \n",
    "    # Error stability across listings\n",
    "    listing_error_std = results_df.groupby('listing_id')['error'].std().mean()\n",
    "    \n",
    "    # Coefficient of variation (standardized measure of dispersion)\n",
    "    cv_error = results_df['error'].std() / results_df['error'].mean() if results_df['error'].mean() != 0 else float('inf')\n",
    "    \n",
    "    # Price segment error stability (analyze error stability across price ranges)\n",
    "    # Create price buckets by percentile to ensure equal sizes\n",
    "    results_df['price_bucket'] = pd.qcut(results_df['price'], q=5, labels=False)\n",
    "    price_segment_error_std = results_df.groupby('price_bucket')['error'].std()\n",
    "    \n",
    "    # Temporal stability (correlation between consecutive days' errors)\n",
    "    results_df = results_df.sort_values(['listing_id', 'date'])\n",
    "    temporal_stability = results_df.groupby('listing_id').apply(\n",
    "        lambda x: x['error'].autocorr() if len(x) > 1 else np.nan\n",
    "    ).mean()\n",
    "    \n",
    "    return {\n",
    "        'daily_error_std': daily_error_std,\n",
    "        'listing_error_std': listing_error_std,\n",
    "        'cv_error': cv_error,\n",
    "        'price_segment_error_std': price_segment_error_std,\n",
    "        'temporal_stability': temporal_stability\n",
    "    }\n",
    "\n",
    "def run_improved_xgboost(train_path, train_ids_path, test_ids_path, output_dir=None, window_size=35, n_splits=5, sample_size=None):    \n",
    "    \"\"\"\n",
    "    Run improved XGBoost with simplified features for predicting prices without price history.\n",
    "    Uses separate listing IDs for training and testing to prevent data leakage.\n",
    "    Applies log transformation to prices during training and transforms back for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training data CSV file\n",
    "    train_ids_path : str\n",
    "        Path to text file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to text file with test listing IDs\n",
    "    output_dir : str, optional\n",
    "        Directory to save results\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in days\n",
    "    n_splits : int, optional\n",
    "        Number of splits for time series cross-validation\n",
    "    sample_size : int, optional\n",
    "        Limit dataset to this number of random listings (for testing)\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Load listing IDs for train/test split\n",
    "    print(\"Loading train/test listing IDs...\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "    if 'simulated_price' in train_data.columns:\n",
    "        print(\"Dropping simulated_price column from the dataset\")\n",
    "        train_data = train_data.drop('simulated_price', axis=1)\n",
    "        \n",
    "    # Apply log transformation to prices\n",
    "    train_data = apply_price_transformation(train_data, inverse=False)\n",
    "    \n",
    "    # For testing - take only a small sample of listings if specified\n",
    "    if sample_size:\n",
    "        print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "        selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.7), replace=False)\n",
    "        selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.3), replace=False)\n",
    "        train_listing_ids = selected_train.tolist()\n",
    "        test_listing_ids = selected_test.tolist()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Filter data to include only dates from 7/8/23 till 2/8/24 (as specified)\n",
    "    start_date = pd.to_datetime('2023-07-08')\n",
    "    end_date = pd.to_datetime('2024-02-08')\n",
    "    train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "    \n",
    "    # Check if we have data for the entire expected range\n",
    "    print(f\"Date range in filtered data: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "    print(f\"Number of days with data: {len(train_data['date'].dt.date.unique())}\")\n",
    "\n",
    "    # Get unique dates and ensure they are properly sorted\n",
    "    unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "    first_date = unique_dates[0]\n",
    "    last_date = unique_dates[-1]\n",
    "\n",
    "    # Check if there's a gap between the expected start date and actual first date\n",
    "    if first_date > start_date.date():\n",
    "        print(f\"Warning: No data found between {start_date.date()} and {first_date}\")\n",
    "        print(f\"Using available data starting from {first_date}\")\n",
    "    \n",
    "    # Sort by date\n",
    "    train_data = train_data.sort_values('date')\n",
    "    \n",
    "    # Create explicit test periods - last 5 weeks (35 days) split into 5 equal parts (7 days each)\n",
    "    last_35_days = unique_dates[-35:]\n",
    "    \n",
    "    # Define explicit test periods - each 7 days\n",
    "    test_periods = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * 7\n",
    "        end_idx = start_idx + 7\n",
    "        # Make sure we don't go beyond the available data\n",
    "        if end_idx <= len(last_35_days):\n",
    "            test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "    \n",
    "    # Adjust n_splits if we couldn't create enough test periods\n",
    "    n_splits = len(test_periods)\n",
    "    \n",
    "    print(f\"Created {n_splits} test periods:\")\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "    \n",
    "    # Initialize XGBoost model with more conservative parameters\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=800,          # More trees for better convergence\n",
    "        learning_rate=0.03,        # Slower learning rate for better generalization\n",
    "        max_depth=8,               # Moderate depth to balance complexity and overfitting\n",
    "        min_child_weight=2,        # Slightly higher to reduce overfitting\n",
    "        subsample=0.85,            # Use most but not all samples per tree\n",
    "        colsample_bytree=0.8,      # Use most but not all features per tree\n",
    "        colsample_bylevel=0.8,     # Use most but not all features per level\n",
    "        gamma=0.1,                 # Minimum loss reduction for further tree split\n",
    "        reg_alpha=0.2,             # L1 regularization (helps with feature selection)\n",
    "        reg_lambda=1.5,            # L2 regularization (helps with general regularization)\n",
    "        tree_method='hist',        # Faster algorithm\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=50   # More patience\n",
    "    )\n",
    "    \n",
    "    # Storage for results\n",
    "    cv_results = []\n",
    "    feature_importance_over_time = []\n",
    "    \n",
    "    # Run time series cross-validation using our explicit test periods\n",
    "    for i, (test_start, test_end) in enumerate(test_periods):\n",
    "        print(f\"\\nSplit {i+1}/{n_splits}\")\n",
    "        \n",
    "        # Define training period: everything before test_start\n",
    "        train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "        train_end_date = train_end.date()\n",
    "        \n",
    "        print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "        print(f\"Testing period: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Split by date first\n",
    "        train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "        test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "        \n",
    "        date_filtered_train = train_data[train_date_mask]\n",
    "        date_filtered_test = train_data[test_date_mask]\n",
    "        \n",
    "        # Now further split by listing IDs\n",
    "        train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "        test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        split_train_data = date_filtered_train[train_id_mask]\n",
    "        split_test_data = date_filtered_test[test_id_mask]\n",
    "        \n",
    "        print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "                # Create simplified spatial features\n",
    "        train_spatial = create_advanced_spatial_features(\n",
    "            split_train_data, \n",
    "            k_neighbors=5,\n",
    "            train_data_only=date_filtered_train[train_id_mask]  # Use only training data as reference\n",
    "        )\n",
    "        test_spatial = create_advanced_spatial_features(\n",
    "            split_test_data,\n",
    "            k_neighbors=5, \n",
    "            train_data_only=date_filtered_train[train_id_mask]  # Use only training data as reference\n",
    "        )\n",
    "        \n",
    "        # Define columns to exclude from training features\n",
    "        exclude_cols = ['listing_id', 'date', 'price', 'original_price']\n",
    "        \n",
    "        # Filter out columns that don't exist\n",
    "        exclude_cols = [col for col in exclude_cols if col in split_train_data.columns]\n",
    "        \n",
    "        # Get categorical and time features (including those with DTF prefix)\n",
    "        dtf_columns = [col for col in split_train_data.columns if col.startswith('DTF_')]\n",
    "        print(f\"Found time feature columns: {dtf_columns}\")\n",
    "        \n",
    "        # Prepare feature matrices\n",
    "        # 1. Core features (without excluded columns)\n",
    "        X_train_parts = [split_train_data.drop(exclude_cols, axis=1)]\n",
    "        X_test_parts = [split_test_data.drop(exclude_cols, axis=1)]\n",
    "        \n",
    "        # 2. Add spatial features\n",
    "        X_train_parts.append(train_spatial)\n",
    "        X_test_parts.append(test_spatial)\n",
    "        \n",
    "        # 3. Create dummy variables for categorical features\n",
    "        for dtf_col in dtf_columns:\n",
    "            if dtf_col in split_train_data.columns and dtf_col in split_test_data.columns:\n",
    "                # Create dummies and add as separate DataFrames\n",
    "                train_dummies = pd.get_dummies(split_train_data[dtf_col], prefix=dtf_col)\n",
    "                test_dummies = pd.get_dummies(split_test_data[dtf_col], prefix=dtf_col)\n",
    "                \n",
    "                X_train_parts.append(train_dummies)\n",
    "                X_test_parts.append(test_dummies)\n",
    "        \n",
    "        # Combine all features\n",
    "        X_train = pd.concat(X_train_parts, axis=1)\n",
    "        X_test = pd.concat(X_test_parts, axis=1)\n",
    "        \n",
    "        y_train = split_train_data['price']  # This is log-transformed price\n",
    "        y_test = split_test_data['price']    # This is log-transformed price\n",
    "        \n",
    "        # Store original prices for later evaluation\n",
    "        y_test_original = split_test_data['original_price'].values\n",
    "        \n",
    "        # Handle missing columns in a more efficient way\n",
    "        train_cols = set(X_train.columns)\n",
    "        test_cols = set(X_test.columns)\n",
    "\n",
    "        # Add missing columns to X_test\n",
    "        if train_cols - test_cols:\n",
    "            missing_df = pd.DataFrame(0, index=X_test.index, columns=list(train_cols - test_cols))\n",
    "            X_test = pd.concat([X_test, missing_df], axis=1)\n",
    "\n",
    "        # Add missing columns to X_train\n",
    "        if test_cols - train_cols:\n",
    "            missing_df = pd.DataFrame(0, index=X_train.index, columns=list(test_cols - train_cols))\n",
    "            X_train = pd.concat([X_train, missing_df], axis=1)\n",
    "\n",
    "        # Ensure the columns are in the same order\n",
    "        X_test = X_test[X_train.columns]\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training on {len(X_train)} samples with {X_train.shape[1]} features\")\n",
    "        print(f\"Testing on {len(X_test)} samples\")\n",
    "        \n",
    "        # Print a sample of feature names\n",
    "        print(f\"Sample feature names: {list(X_train.columns[:5])}\")\n",
    "        \n",
    "        xgb_model.fit(\n",
    "            X_train, \n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions (still in log-transformed space)\n",
    "        y_pred_log = xgb_model.predict(X_test)\n",
    "        \n",
    "        # Create a temporary dataframe to transform predictions back\n",
    "        temp_pred_df = pd.DataFrame({'price': y_pred_log})\n",
    "        temp_pred_df = apply_price_transformation(temp_pred_df, inverse=True)\n",
    "        y_pred = temp_pred_df['price'].values\n",
    "        \n",
    "        # Store feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': xgb_model.feature_importances_,\n",
    "            'split': i\n",
    "        })\n",
    "        feature_importance_over_time.append(feature_importance)\n",
    "        \n",
    "        # Calculate metrics using the original price scale\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "        mae = mean_absolute_error(y_test_original, y_pred)\n",
    "        r2 = r2_score(y_test_original, y_pred) if len(set(y_test_original)) > 1 else np.nan\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            mape = np.mean(np.abs((y_test_original - y_pred) / (y_test_original + 1e-8))) * 100\n",
    "        \n",
    "        print(f\"Split {i+1} Results - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}, MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # Store results for this split\n",
    "        split_results = pd.DataFrame({\n",
    "            'split': i,\n",
    "            'date': split_test_data['date'],\n",
    "            'listing_id': split_test_data['listing_id'],\n",
    "            'price': y_test_original,  # Original price scale\n",
    "            'predicted': y_pred,       # Original price scale\n",
    "            'error': y_test_original - y_pred,\n",
    "            'abs_error': np.abs(y_test_original - y_pred),\n",
    "            'pct_error': np.abs((y_test_original - y_pred) / (y_test_original + 1e-8)) * 100\n",
    "        })\n",
    "        \n",
    "        cv_results.append(split_results)\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(cv_results, ignore_index=True)\n",
    "    all_feature_importance = pd.concat(feature_importance_over_time, ignore_index=True)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    y_true = all_results['price']  # This is in original price scale from our changes above\n",
    "    y_pred = all_results['predicted']  # This is in original price scale from our changes above\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100,\n",
    "        'explained_variance': explained_variance_score(y_true, y_pred),\n",
    "        'max_error': max_error(y_true, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Calculate split-level metrics\n",
    "    split_metrics = []\n",
    "    for split in range(n_splits):\n",
    "        split_data = all_results[all_results['split'] == split]\n",
    "        if not split_data.empty:\n",
    "            y_true_split = split_data['price']\n",
    "            y_pred_split = split_data['predicted']\n",
    "            \n",
    "            split_metrics.append({\n",
    "                'split': split,\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_split, y_pred_split)),\n",
    "                'mae': mean_absolute_error(y_true_split, y_pred_split),\n",
    "                'r2': r2_score(y_true_split, y_pred_split) if len(set(y_true_split)) > 1 else np.nan,\n",
    "                'mape': np.mean(np.abs((y_true_split - y_pred_split) / (y_true_split + 1e-8))) * 100,\n",
    "                'n_samples': len(y_true_split)\n",
    "            })\n",
    "    \n",
    "    split_metrics_df = pd.DataFrame(split_metrics)\n",
    "    \n",
    "    # Calculate daily metrics\n",
    "    all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    daily_metrics = []\n",
    "    for day, group in all_results.groupby('date_str'):\n",
    "        y_true_day = group['price']\n",
    "        y_pred_day = group['predicted']\n",
    "        \n",
    "        daily_metrics.append({\n",
    "            'date': day,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "            'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "            'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "            'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "            'n_samples': len(y_true_day)\n",
    "        })\n",
    "    \n",
    "    daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "    daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "    daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "    \n",
    "    # Calculate error stability metrics\n",
    "    error_stability = compute_error_stability_metrics(all_results)\n",
    "    \n",
    "    # Calculate MAE stability coefficient\n",
    "    mae_stability = daily_metrics_df['mae'].std() / daily_metrics_df['mae'].mean()\n",
    "    print(f\"MAE Stability (CV of daily MAE): {mae_stability:.4f}\")\n",
    "    \n",
    "    # Add to error_stability dictionary\n",
    "    error_stability['mae_stability'] = mae_stability\n",
    "    \n",
    "    # Error analysis - autocorrelation\n",
    "    errors = all_results['error'].values\n",
    "    error_autocorr = acf(errors, nlags=7)[1:]  # Exclude lag 0\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    evaluation_results = {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'split_metrics': split_metrics_df,\n",
    "        'daily_metrics': daily_metrics_df,\n",
    "        'all_results': all_results,\n",
    "        'feature_importance': all_feature_importance,\n",
    "        'error_autocorrelation': error_autocorr,\n",
    "        'error_stability': error_stability,\n",
    "        'train_listings': len(train_listing_ids),\n",
    "        'test_listings': len(test_listing_ids)\n",
    "    }\n",
    "    \n",
    "    # Save results if output directory is provided\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save all results\n",
    "        results_file = os.path.join(output_dir, 'improved_xgboost_results.csv')\n",
    "        all_results.to_csv(results_file, index=False)\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_file = os.path.join(output_dir, 'improved_xgboost_metrics.csv')\n",
    "        daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"Daily metrics saved to {metrics_file}\")\n",
    "        \n",
    "        # Save feature importance\n",
    "        importance_file = os.path.join(output_dir, 'improved_xgboost_feature_importance.csv')\n",
    "        all_feature_importance.to_csv(importance_file, index=False)\n",
    "        print(f\"Feature importance saved to {importance_file}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'improved_summary.txt'), 'w') as f:\n",
    "            f.write(f\"Improved XGBoost Model Summary (Simplified Features)\\n\")\n",
    "            f.write(f\"===================================================\\n\\n\")\n",
    "            f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "            f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "            f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "            f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "            f.write(f\"Overall Metrics (on original price scale):\\n\")\n",
    "            f.write(f\"  MAE Stability (day-to-day): {error_stability.get('mae_stability', float('nan')):.4f}\\n\")\n",
    "            for k, v in overall_metrics.items():\n",
    "                f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print_summary(evaluation_results)\n",
    "    \n",
    "    # Create plots\n",
    "    plot_results(evaluation_results)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def print_summary(evaluation_results):\n",
    "    \"\"\"Print a summary of the improved model results\"\"\"\n",
    "    overall = evaluation_results['overall_metrics']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    daily = evaluation_results['daily_metrics']\n",
    "    feature_importance = evaluation_results['feature_importance']\n",
    "    error_autocorr = evaluation_results.get('error_autocorrelation', [0] * 7)\n",
    "    \n",
    "    # Print new info about listing splits if available\n",
    "    train_listings = evaluation_results.get('train_listings', 'N/A')\n",
    "    test_listings = evaluation_results.get('test_listings', 'N/A')\n",
    "    \n",
    "    print(\"\\n===== IMPROVED XGBOOST MODEL WITH SIMPLIFIED FEATURES =====\")\n",
    "    if train_listings != 'N/A':\n",
    "        print(f\"Using {train_listings} listings for training and {test_listings} listings for testing\")\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"RMSE: {overall['rmse']:.4f}\")\n",
    "    print(f\"MAE: {overall['mae']:.4f}\")\n",
    "    print(f\"R²: {overall['r2']:.4f}\")\n",
    "    print(f\"MAPE: {overall['mape']:.4f}%\")\n",
    "    print(f\"Explained Variance: {overall['explained_variance']:.4f}\")\n",
    "    print(f\"Median Abs Error: {overall['median_absolute_error']:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Split Performance ===\")\n",
    "    print(splits[['split', 'rmse', 'mae', 'mape', 'r2', 'n_samples']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Split Statistics ===\")\n",
    "    print(\"MAE:\")\n",
    "    print(f\"  Average: {splits['mae'].mean():.4f}\")\n",
    "    print(f\"  Min: {splits['mae'].min():.4f} (Split {splits.loc[splits['mae'].idxmin(), 'split']})\")\n",
    "    print(f\"  Max: {splits['mae'].max():.4f} (Split {splits.loc[splits['mae'].idxmax(), 'split']})\")\n",
    "    \n",
    "    print(\"\\nRMSE:\")\n",
    "    print(f\"  Average: {splits['rmse'].mean():.4f}\")\n",
    "    print(f\"  Min: {splits['rmse'].min():.4f} (Split {splits.loc[splits['rmse'].idxmin(), 'split']})\")\n",
    "    print(f\"  Max: {splits['rmse'].max():.4f} (Split {splits.loc[splits['rmse'].idxmax(), 'split']})\")\n",
    "    \n",
    "    # Calculate RMSE for later folds (excluding fold 1)\n",
    "    later_folds = splits[splits['split'] > 0]\n",
    "    later_rmse = later_folds['rmse'].mean()\n",
    "    print(f\"  Later folds RMSE (excl. fold 1): {later_rmse:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Error Autocorrelation ===\")\n",
    "    for lag, acf_value in enumerate(error_autocorr, 1):\n",
    "        print(f\"  Lag {lag}: {acf_value:.4f}\")\n",
    "    \n",
    "    # Add this after printing the error autocorrelation\n",
    "    print(\"\\n=== Error Stability Metrics ===\")\n",
    "    stability = evaluation_results.get('error_stability', {})\n",
    "    if stability:\n",
    "        print(f\"Daily Error Std Dev: {stability['daily_error_std']:.4f}\")\n",
    "        print(f\"Listing Error Std Dev: {stability['listing_error_std']:.4f}\")\n",
    "        print(f\"Coefficient of Variation of Error: {stability['cv_error']:.4f}\")\n",
    "        print(f\"Temporal Stability (autocorr): {stability['temporal_stability']:.4f}\")\n",
    "        print(f\"MAE Stability (day-to-day): {stability.get('mae_stability', float('nan')):.4f}\")\n",
    "        print(\"\\nPrice Segment Error Std Dev:\")\n",
    "        for bucket, std_val in stability['price_segment_error_std'].items():\n",
    "            price_segment = ['Very Low', 'Low', 'Medium', 'High', 'Very High'][bucket]\n",
    "            print(f\"  {price_segment}: {std_val:.4f}\")\n",
    "\n",
    "    # Calculate overall feature importance by averaging across splits\n",
    "    avg_importance = feature_importance.groupby('feature')['importance'].mean().reset_index()\n",
    "    top_features = avg_importance.sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\n=== Top 10 Most Important Features (Simplified Model) ===\")\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"{i}. {row['feature']}: {row['importance']:.6f}\")\n",
    "\n",
    "def plot_results(evaluation_results):\n",
    "    \"\"\"Plot the results from the improved model\"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Extract data\n",
    "    daily_metrics = evaluation_results['daily_metrics']\n",
    "    all_results = evaluation_results['all_results']\n",
    "    splits = evaluation_results['split_metrics']\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Add title showing we're using the improved model\n",
    "    fig.suptitle('Improved XGBoost Model with Simplified Features', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Daily MAE\n",
    "    sns.lineplot(\n",
    "        x=pd.to_datetime(daily_metrics['date']),\n",
    "        y=daily_metrics['mae'],\n",
    "        marker='o',\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Mean Absolute Error by Day')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Cross-validation splits performance\n",
    "    splits_x = splits['split']\n",
    "    metrics_to_plot = ['rmse', 'mae']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        sns.lineplot(\n",
    "            x=splits_x,\n",
    "            y=splits[metric],\n",
    "            marker='o',\n",
    "            label=metric.upper(),\n",
    "            ax=axes[0, 1]\n",
    "        )\n",
    "    \n",
    "    axes[0, 1].set_title('Performance Across CV Splits')\n",
    "    axes[0, 1].set_xlabel('CV Split')\n",
    "    axes[0, 1].set_ylabel('Error Metric')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted (colored by split)\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        all_results['price'],\n",
    "        all_results['predicted'],\n",
    "        c=all_results['split'],\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    min_val = min(all_results['price'].min(), all_results['predicted'].min())\n",
    "    max_val = max(all_results['price'].max(), all_results['predicted'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    axes[1, 0].set_title('Actual vs Predicted (Colored by CV Split)')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('CV Split')\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    sns.histplot(all_results['error'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].axvline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_title('Error Distribution')\n",
    "    axes[1, 1].set_xlabel('Error (Actual - Predicted)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Create plot for feature importance\n",
    "    feature_importance = evaluation_results['feature_importance']\n",
    "    top_features = feature_importance.groupby('feature')['importance'].mean().nlargest(10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        x=top_features.values,\n",
    "        y=top_features.index,\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Top 10 Features (Improved Model)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot error autocorrelation\n",
    "    error_acf = np.concatenate([[1], evaluation_results['error_autocorrelation']])\n",
    "    lags = range(len(error_acf))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(lags, error_acf, alpha=0.7)\n",
    "    plt.axhline(y=0, linestyle='--', color='gray')\n",
    "    \n",
    "    # Add confidence intervals (95%)\n",
    "    conf_interval = 1.96 / np.sqrt(len(all_results['error']))\n",
    "    plt.axhline(y=conf_interval, linestyle='--', color='red', alpha=0.5)\n",
    "    plt.axhline(y=-conf_interval, linestyle='--', color='red', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title('Error Autocorrelation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: Performance by listing ID count\n",
    "    # Group by listing ID and calculate average absolute error for each listing\n",
    "    listing_errors = all_results.groupby('listing_id')['abs_error'].mean().reset_index()\n",
    "    listing_errors = listing_errors.sort_values('abs_error')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(listing_errors)), listing_errors['abs_error'], alpha=0.6)\n",
    "    plt.axhline(y=listing_errors['abs_error'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {listing_errors[\"abs_error\"].mean():.2f}')\n",
    "    plt.title('Average Absolute Error by Listing')\n",
    "    plt.xlabel('Listing Index (sorted by error)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\"\n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \".\\output\\improved_xgboost\"\n",
    "    \n",
    "    # Run the model\n",
    "    results = run_improved_xgboost(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        output_dir=output_dir,\n",
    "        window_size=35,  # 5 weeks\n",
    "        n_splits=5,\n",
    "        sample_size=None  # Set to a number like 1000 for testing, None to use all data\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
