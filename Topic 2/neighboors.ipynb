{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import faiss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def gpu_neighbor_selection(train_data, train_ids, test_ids, k=5, \n",
    "                          output_path=None, use_embedding=True, batch_size=1024,\n",
    "                          similarity_threshold=None, min_neighbors=1):\n",
    "    \"\"\"\n",
    "    GPU-accelerated neighbor selection using FAISS and PyTorch,\n",
    "    with filtering to keep only sufficiently similar neighbors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pandas.DataFrame\n",
    "        The complete dataset containing both training and testing listings\n",
    "    train_ids : list\n",
    "        List of listing IDs to use as potential neighbors (training set)\n",
    "    test_ids : list\n",
    "        List of listing IDs that need neighbors (test set)\n",
    "    k : int\n",
    "        Number of neighbors to find for each listing\n",
    "    output_path : str, optional\n",
    "        Path to save the neighbor dictionary\n",
    "    use_embedding : bool, default=True\n",
    "        Whether to use neural embeddings for listings\n",
    "    batch_size : int, default=1024\n",
    "        Batch size for GPU processing\n",
    "    similarity_threshold : float, optional\n",
    "        Maximum distance threshold for keeping neighbors (None=auto-determine)\n",
    "    min_neighbors : int, default=1\n",
    "        Minimum number of neighbors to keep per listing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping test listing IDs to lists of their most similar train listing IDs\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available and ensure we use GPU\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This function requires a GPU.\")\n",
    "    \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Filter data to create train and test datasets from the same source\n",
    "    train_df = train_data[train_data['listing_id'].isin(train_ids)].copy()\n",
    "    test_df = train_data[train_data['listing_id'].isin(test_ids)].copy()\n",
    "    \n",
    "    # Get one representative record per listing (most recent)\n",
    "    train_df = train_df.sort_values('date', ascending=False).drop_duplicates('listing_id')\n",
    "    test_df = test_df.sort_values('date', ascending=False).drop_duplicates('listing_id')\n",
    "    \n",
    "    print(f\"Processing {len(train_df)} train and {len(test_df)} test listings\")\n",
    "    \n",
    "    # Define feature groups\n",
    "    property_features = ['accommodates', 'bedrooms', 'bathrooms', 'amenity_count', \n",
    "                         'luxury_score', 'essential_score', 'bedroom_ratio']\n",
    "    amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    location_features = ['latitude', 'longitude', 'neighbourhood_cleansed_encoded']\n",
    "    \n",
    "    # Get available features\n",
    "    available_property = [f for f in property_features if f in train_df.columns]\n",
    "    available_amenity = [f for f in amenity_features if f in train_df.columns]\n",
    "    available_location = [f for f in location_features if f in train_df.columns]\n",
    "    \n",
    "    # Fill missing values\n",
    "    for df in [train_df, test_df]:\n",
    "        for col in available_property:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        for col in available_amenity:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(0)\n",
    "        for col in available_location:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                if col in ['latitude', 'longitude']:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Normalize features\n",
    "    all_features = available_property + available_amenity + available_location\n",
    "    if not all_features:\n",
    "        raise ValueError(\"No features available for similarity calculation\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_df[all_features].values)\n",
    "    test_features = scaler.transform(test_df[all_features].values)\n",
    "    \n",
    "    # Define feature weights\n",
    "    feature_weights = np.ones(len(all_features), dtype=np.float32)\n",
    "    \n",
    "    # Assign weights to different feature types\n",
    "    for i, feature in enumerate(all_features):\n",
    "        if feature in available_location:\n",
    "            # Higher weight for location features\n",
    "            feature_weights[i] = 2.0\n",
    "        elif feature in ['accommodates', 'bedrooms', 'bathrooms']:\n",
    "            # Medium weight for key property features\n",
    "            feature_weights[i] = 1.5\n",
    "    \n",
    "    # Normalize weights\n",
    "    feature_weights = feature_weights / feature_weights.sum()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_tensor = torch.FloatTensor(train_features).cuda()\n",
    "    test_tensor = torch.FloatTensor(test_features).cuda()\n",
    "    weights_tensor = torch.FloatTensor(feature_weights).cuda()\n",
    "    \n",
    "    # Option to use neural embeddings to improve similarity calculation\n",
    "    if use_embedding:\n",
    "        class EmbeddingNetwork(torch.nn.Module):\n",
    "            def __init__(self, input_dim, embedding_dim=32):\n",
    "                super(EmbeddingNetwork, self).__init__()\n",
    "                self.fc1 = torch.nn.Linear(input_dim, 64)\n",
    "                self.bn1 = torch.nn.BatchNorm1d(64)\n",
    "                self.fc2 = torch.nn.Linear(64, embedding_dim)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = torch.relu(self.bn1(self.fc1(x)))\n",
    "                x = self.fc2(x)\n",
    "                # Normalize embeddings to unit length\n",
    "                x = torch.nn.functional.normalize(x, p=2, dim=1)\n",
    "                return x\n",
    "        \n",
    "        # Train a simple embedding network\n",
    "        print(\"Training embedding network for improved similarity...\")\n",
    "        embedding_dim = 32\n",
    "        model = EmbeddingNetwork(len(all_features), embedding_dim).cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Fixed contrastive loss that works with different batch sizes\n",
    "        def batch_contrastive_loss(embeddings):\n",
    "            batch_size = embeddings.size(0)\n",
    "            \n",
    "            # If batch size is too small, use a simpler loss\n",
    "            if batch_size < 4:\n",
    "                # Simple regularization loss\n",
    "                return torch.mean(torch.norm(embeddings, dim=1))\n",
    "            \n",
    "            # Matrix of pairwise distances within the batch only\n",
    "            dists = torch.cdist(embeddings, embeddings)\n",
    "            \n",
    "            # For each embedding, find closest and furthest embeddings within batch\n",
    "            # Exclude self comparisons (diagonal elements)\n",
    "            mask = torch.ones_like(dists, dtype=torch.bool)\n",
    "            mask.fill_diagonal_(0)\n",
    "            \n",
    "            # Use masked_select to get non-diagonal elements and reshape\n",
    "            non_diag_dists = torch.masked_select(dists, mask).reshape(batch_size, batch_size - 1)\n",
    "            \n",
    "            # Find closest and furthest for each point\n",
    "            closest = non_diag_dists.min(dim=1)[0]   # Closest point to each embedding\n",
    "            furthest = non_diag_dists.max(dim=1)[0]  # Furthest point from each embedding\n",
    "            \n",
    "            # Triplet-like loss: minimize closest distance, maximize furthest distance\n",
    "            # This encourages compact, well-separated clusters\n",
    "            loss = closest.mean() - furthest.mean() * 0.1\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        # Train embedding model with batching for memory efficiency\n",
    "        model.train()\n",
    "        num_train = train_tensor.shape[0]\n",
    "        num_epochs = 20\n",
    "        \n",
    "        # Adjust batch size if needed\n",
    "        actual_batch_size = min(batch_size, num_train)\n",
    "        print(f\"Using batch size: {actual_batch_size}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Shuffle data each epoch\n",
    "            indices = torch.randperm(num_train)\n",
    "            train_tensor_shuffled = train_tensor[indices]\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, num_train, actual_batch_size):\n",
    "                end_idx = min(i + actual_batch_size, num_train)\n",
    "                curr_batch_size = end_idx - i\n",
    "                \n",
    "                # Skip tiny batches\n",
    "                if curr_batch_size < 2:\n",
    "                    continue\n",
    "                \n",
    "                batch = train_tensor_shuffled[i:end_idx]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                embeddings = model(batch)\n",
    "                loss = batch_contrastive_loss(embeddings)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / max(1, num_batches)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Generate embeddings for all listings\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Process train data in batches\n",
    "            train_embeddings = []\n",
    "            for i in range(0, train_tensor.shape[0], actual_batch_size):\n",
    "                end_idx = min(i + actual_batch_size, train_tensor.shape[0])\n",
    "                batch = train_tensor[i:end_idx]\n",
    "                emb = model(batch).cpu().numpy()\n",
    "                train_embeddings.append(emb)\n",
    "            train_embeddings = np.vstack(train_embeddings)\n",
    "            \n",
    "            # Process test data in batches\n",
    "            test_embeddings = []\n",
    "            for i in range(0, test_tensor.shape[0], actual_batch_size):\n",
    "                end_idx = min(i + actual_batch_size, test_tensor.shape[0])\n",
    "                batch = test_tensor[i:end_idx]\n",
    "                emb = model(batch).cpu().numpy()\n",
    "                test_embeddings.append(emb)\n",
    "            test_embeddings = np.vstack(test_embeddings)\n",
    "    else:\n",
    "        # Without embedding model, just use weighted features\n",
    "        train_weighted = train_tensor * weights_tensor\n",
    "        test_weighted = test_tensor * weights_tensor\n",
    "        \n",
    "        # Normalize vectors\n",
    "        train_embeddings = torch.nn.functional.normalize(train_weighted, p=2, dim=1).cpu().numpy()\n",
    "        test_embeddings = torch.nn.functional.normalize(test_weighted, p=2, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Use FAISS for efficient GPU-accelerated nearest neighbor search\n",
    "    print(\"Building FAISS index for fast similarity search...\")\n",
    "    dimension = train_embeddings.shape[1]\n",
    "    \n",
    "    # Choose appropriate index type based on data size\n",
    "    if len(train_embeddings) < 10000:\n",
    "        # For smaller datasets, exact search is fine\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "    else:\n",
    "        # For larger datasets, use approximate search\n",
    "        nlist = min(4096, int(len(train_embeddings) / 30))  # Rule of thumb\n",
    "        quantizer = faiss.IndexFlatL2(dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
    "        # Need to train the index\n",
    "        index.train(train_embeddings)\n",
    "    \n",
    "    # Use GPU for the index - no fallback\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    print(\"Using GPU FAISS index\")\n",
    "    \n",
    "    # Add vectors to the index\n",
    "    index.add(train_embeddings)\n",
    "    \n",
    "    # If using IVF index, set the number of cells to probe\n",
    "    if hasattr(index, 'nprobe'):\n",
    "        index.nprobe = min(50, nlist)  # Trade-off between speed and accuracy\n",
    "    \n",
    "    # Retrieve more neighbors than needed to allow for filtering\n",
    "    search_k = min(k * 3, len(train_embeddings))  # Get 3x more candidates for filtering\n",
    "    search_k = max(search_k, k + 5)  # Ensure we have at least k+5 candidates\n",
    "    \n",
    "    # Perform the search\n",
    "    print(f\"Searching for {search_k} candidate neighbors per test listing...\")\n",
    "    distances, indices = index.search(test_embeddings, search_k)\n",
    "    \n",
    "    # Auto-determine similarity threshold if not provided\n",
    "    if similarity_threshold is None:\n",
    "        # Use statistics from the distance distribution to set a threshold\n",
    "        # Take median of the first nearest neighbor distance and multiply by a factor\n",
    "        first_nn_distances = distances[:, 0]  # First nearest neighbor for each test listing\n",
    "        median_first_nn = np.median(first_nn_distances)\n",
    "        similarity_threshold = median_first_nn * 2.5  # Allow distances up to 2.5x the median first-nn distance\n",
    "        print(f\"Auto-determined similarity threshold: {similarity_threshold:.4f}\")\n",
    "    \n",
    "    # Create neighbor dictionary with similarity filtering\n",
    "    neighbor_dict = {}\n",
    "    total_filtered_out = 0\n",
    "    listings_with_few_neighbors = 0\n",
    "    \n",
    "    for i, test_id in enumerate(test_df['listing_id'].values):\n",
    "        # Get distances and indices for this test listing\n",
    "        listing_distances = distances[i]\n",
    "        listing_indices = indices[i]\n",
    "        \n",
    "        # Filter by similarity threshold\n",
    "        valid_mask = listing_distances < similarity_threshold\n",
    "        valid_indices = listing_indices[valid_mask]\n",
    "        valid_distances = listing_distances[valid_mask]\n",
    "        \n",
    "        # Ensure we have at least min_neighbors\n",
    "        if len(valid_indices) < min_neighbors and len(listing_indices) > 0:\n",
    "            # If we don't have enough neighbors after filtering, take the closest ones\n",
    "            # regardless of threshold (up to min_neighbors)\n",
    "            listings_with_few_neighbors += 1\n",
    "            valid_indices = listing_indices[:min_neighbors]\n",
    "            valid_distances = listing_distances[:min_neighbors]\n",
    "        \n",
    "        # Convert indices to listing IDs and track the associated distances\n",
    "        neighbors_with_distances = [\n",
    "            (train_df['listing_id'].iloc[idx], float(dist)) \n",
    "            for idx, dist in zip(valid_indices, valid_distances)\n",
    "            if idx >= 0 and idx < len(train_df)\n",
    "        ]\n",
    "        \n",
    "        # Sort by distance (most similar first)\n",
    "        neighbors_with_distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Limit to k neighbors and separate IDs and distances\n",
    "        neighbors_with_distances = neighbors_with_distances[:k]\n",
    "        neighbors = [nwd[0] for nwd in neighbors_with_distances]\n",
    "        filtered_distances = [nwd[1] for nwd in neighbors_with_distances]\n",
    "        \n",
    "        # Count filtered neighbors\n",
    "        total_filtered_out += min(search_k, len(listing_indices)) - len(neighbors)\n",
    "        \n",
    "        # Store in dictionary with distance information\n",
    "        neighbor_dict[test_id] = {\n",
    "            'neighbors': neighbors,\n",
    "            'distances': filtered_distances\n",
    "        }\n",
    "    \n",
    "    print(f\"Found neighbors for {len(neighbor_dict)} test listings\")\n",
    "    print(f\"Filtered out {total_filtered_out} neighbors in total due to similarity threshold\")\n",
    "    print(f\"{listings_with_few_neighbors} listings had fewer than requested neighbors after filtering\")\n",
    "    \n",
    "    # Get average number of neighbors per listing after filtering\n",
    "    avg_neighbors = np.mean([len(info['neighbors']) for info in neighbor_dict.values()])\n",
    "    print(f\"Average number of neighbors per listing after filtering: {avg_neighbors:.2f}\")\n",
    "    \n",
    "    # Save results if path provided\n",
    "    if output_path:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(neighbor_dict, f)\n",
    "        print(f\"Saved neighbor dictionary to {output_path}\")\n",
    "        \n",
    "        # Save a readable version with distances\n",
    "        neighbor_rows = []\n",
    "        for test_id, info in neighbor_dict.items():\n",
    "            for i, (neighbor_id, distance) in enumerate(zip(info['neighbors'], info['distances'])):\n",
    "                neighbor_rows.append({\n",
    "                    'test_listing_id': test_id,\n",
    "                    'neighbor_listing_id': neighbor_id,\n",
    "                    'rank': i+1,\n",
    "                    'distance': distance\n",
    "                })\n",
    "        \n",
    "        neighbor_df = pd.DataFrame(neighbor_rows)\n",
    "        \n",
    "        # Only save if we have neighbors\n",
    "        if not neighbor_df.empty:\n",
    "            neighbor_df.to_csv(f\"{os.path.splitext(output_path)[0]}.csv\", index=False)\n",
    "            print(f\"Saved human-readable neighbor list to {os.path.splitext(output_path)[0]}.csv\")\n",
    "    \n",
    "    # Generate analysis of neighbor quality\n",
    "    if 'latitude' in train_df.columns and 'longitude' in train_df.columns:\n",
    "        # Create a mapping from listing ID to coordinates\n",
    "        locations = {}\n",
    "        for _, row in pd.concat([train_df, test_df])[['listing_id', 'latitude', 'longitude']].iterrows():\n",
    "            locations[row['listing_id']] = (row['latitude'], row['longitude'])\n",
    "        \n",
    "        # Calculate average geographic distance to neighbors\n",
    "        geo_distances = []\n",
    "        for test_id, info in neighbor_dict.items():\n",
    "            if test_id in locations:\n",
    "                test_loc = locations[test_id]\n",
    "                for neighbor_id in info['neighbors']:\n",
    "                    if neighbor_id in locations:\n",
    "                        neighbor_loc = locations[neighbor_id]\n",
    "                        # Simple Euclidean distance (not haversine, but sufficient for analysis)\n",
    "                        dist = np.sqrt((test_loc[0] - neighbor_loc[0])**2 + \n",
    "                                       (test_loc[1] - neighbor_loc[1])**2)\n",
    "                        geo_distances.append(dist)\n",
    "        \n",
    "        if geo_distances:\n",
    "            avg_dist = np.mean(geo_distances)\n",
    "            median_dist = np.median(geo_distances)\n",
    "            print(f\"Average geographic distance to neighbors: {avg_dist:.6f}\")\n",
    "            print(f\"Median geographic distance to neighbors: {median_dist:.6f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"GPU-accelerated neighbor selection completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Return a simplified version of the neighbor dictionary for easier use\n",
    "    simplified_neighbor_dict = {test_id: info['neighbors'] for test_id, info in neighbor_dict.items()}\n",
    "    return simplified_neighbor_dict\n",
    "\n",
    "\n",
    "def evaluate_neighbor_quality(train_data, train_ids, test_ids, neighbor_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the selected neighbors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pandas.DataFrame\n",
    "        The complete dataset containing both training and testing listings\n",
    "    train_ids : list\n",
    "        List of listing IDs used as potential neighbors (training set)\n",
    "    test_ids : list\n",
    "        List of listing IDs that need neighbors (test set)\n",
    "    neighbor_dict : dict\n",
    "        Dictionary mapping test listing IDs to neighbor listing IDs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Filter data to create train and test datasets\n",
    "    train_df = train_data[train_data['listing_id'].isin(train_ids)].copy()\n",
    "    test_df = train_data[train_data['listing_id'].isin(test_ids)].copy()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Convert neighbor_dict to standard format if it's the detailed version\n",
    "    simplified_dict = {}\n",
    "    for test_id, value in neighbor_dict.items():\n",
    "        if isinstance(value, dict) and 'neighbors' in value:\n",
    "            simplified_dict[test_id] = value['neighbors']\n",
    "        else:\n",
    "            simplified_dict[test_id] = value\n",
    "    \n",
    "    neighbor_dict = simplified_dict\n",
    "    \n",
    "    # 1. Evaluate geographic proximity\n",
    "    if 'latitude' in train_data.columns and 'longitude' in train_data.columns:\n",
    "        # Create location lookup\n",
    "        locations = {}\n",
    "        for _, row in pd.concat([train_df, test_df])[['listing_id', 'latitude', 'longitude']].iterrows():\n",
    "            locations[row['listing_id']] = (row['latitude'], row['longitude'])\n",
    "        \n",
    "        geo_distances = []\n",
    "        for test_id, neighbors in neighbor_dict.items():\n",
    "            if test_id in locations:\n",
    "                test_loc = locations[test_id]\n",
    "                for neighbor_id in neighbors:\n",
    "                    if neighbor_id in locations:\n",
    "                        neighbor_loc = locations[neighbor_id]\n",
    "                        # Convert to radians for haversine\n",
    "                        lat1, lon1 = np.radians(test_loc)\n",
    "                        lat2, lon2 = np.radians(neighbor_loc)\n",
    "                        \n",
    "                        # Haversine formula\n",
    "                        dlon = lon2 - lon1\n",
    "                        dlat = lat2 - lat1\n",
    "                        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "                        c = 2 * np.arcsin(np.sqrt(a))\n",
    "                        r = 6371  # Radius of earth in km\n",
    "                        dist = c * r\n",
    "                        \n",
    "                        geo_distances.append(dist)\n",
    "        \n",
    "        results['avg_geo_distance_km'] = np.mean(geo_distances) if geo_distances else float('nan')\n",
    "        results['median_geo_distance_km'] = np.median(geo_distances) if geo_distances else float('nan')\n",
    "    \n",
    "    # 2. Evaluate amenity similarity\n",
    "    amenity_cols = [col for col in train_data.columns if col.startswith('has_')]\n",
    "    if amenity_cols:\n",
    "        train_amenities = {}\n",
    "        for _, row in train_df.drop_duplicates('listing_id')[['listing_id'] + amenity_cols].iterrows():\n",
    "            train_amenities[row['listing_id']] = row[amenity_cols].values\n",
    "            \n",
    "        test_amenities = {}\n",
    "        for _, row in test_df.drop_duplicates('listing_id')[['listing_id'] + amenity_cols].iterrows():\n",
    "            test_amenities[row['listing_id']] = row[amenity_cols].values\n",
    "        \n",
    "        jaccard_sims = []\n",
    "        for test_id, neighbors in neighbor_dict.items():\n",
    "            if test_id in test_amenities:\n",
    "                test_amn = test_amenities[test_id]\n",
    "                for neighbor_id in neighbors:\n",
    "                    if neighbor_id in train_amenities:\n",
    "                        neighbor_amn = train_amenities[neighbor_id]\n",
    "                        \n",
    "                        # Jaccard similarity\n",
    "                        intersection = np.sum(np.minimum(test_amn, neighbor_amn))\n",
    "                        union = np.sum(np.maximum(test_amn, neighbor_amn))\n",
    "                        if union > 0:\n",
    "                            sim = intersection / union\n",
    "                            jaccard_sims.append(sim)\n",
    "        \n",
    "        results['avg_amenity_similarity'] = np.mean(jaccard_sims) if jaccard_sims else float('nan')\n",
    "    \n",
    "    # 3. Evaluate price prediction quality\n",
    "    # Create lookup with date and price\n",
    "    train_prices = {}\n",
    "    price_col = 'original_price' if 'original_price' in train_df.columns else 'price'\n",
    "    \n",
    "    for _, row in train_df[['listing_id', 'date', price_col]].iterrows():\n",
    "        lid = row['listing_id']\n",
    "        date = pd.to_datetime(row['date']).date()\n",
    "        price = row[price_col]\n",
    "        \n",
    "        if lid not in train_prices:\n",
    "            train_prices[lid] = {}\n",
    "        train_prices[lid][date] = price\n",
    "    \n",
    "    # Evaluate how well neighbors predict test prices\n",
    "    price_errors = []\n",
    "    for _, row in test_df[['listing_id', 'date', price_col]].iterrows():\n",
    "        test_id = row['listing_id']\n",
    "        date = pd.to_datetime(row['date']).date()\n",
    "        actual_price = row[price_col]\n",
    "        \n",
    "        if test_id in neighbor_dict:\n",
    "            neighbor_prices = []\n",
    "            for neighbor_id in neighbor_dict[test_id]:\n",
    "                if neighbor_id in train_prices and date in train_prices[neighbor_id]:\n",
    "                    neighbor_prices.append(train_prices[neighbor_id][date])\n",
    "            \n",
    "            if neighbor_prices:\n",
    "                predicted_price = np.mean(neighbor_prices)\n",
    "                error = abs(predicted_price - actual_price)\n",
    "                price_errors.append(error)\n",
    "    \n",
    "    if price_errors:\n",
    "        results['avg_price_error'] = np.mean(price_errors)\n",
    "        results['median_price_error'] = np.median(price_errors)\n",
    "        results['rmse'] = np.sqrt(np.mean(np.square(price_errors)))\n",
    "    \n",
    "    # 4. Add neighbor count statistics\n",
    "    neighbor_counts = [len(neighbors) for neighbors in neighbor_dict.values()]\n",
    "    if neighbor_counts:\n",
    "        results['avg_neighbor_count'] = np.mean(neighbor_counts)\n",
    "        results['min_neighbor_count'] = np.min(neighbor_counts)\n",
    "        results['max_neighbor_count'] = np.max(neighbor_counts)\n",
    "        results['listings_with_neighbors_pct'] = 100 * len([c for c in neighbor_counts if c > 0]) / len(neighbor_counts)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_neighbor_selection(train_path, train_ids_path, test_ids_path, output_dir=\"./output\", \n",
    "                           k=5, use_embedding=True, similarity_threshold=None, min_neighbors=1):\n",
    "    \"\"\"\n",
    "    Run the neighbor selection process and save the results.\n",
    "    Uses GPU-only implementation without CPU fallback.\n",
    "    Only keeps sufficiently similar neighbors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_path : str\n",
    "        Path to the training data CSV\n",
    "    train_ids_path : str\n",
    "        Path to file with training listing IDs\n",
    "    test_ids_path : str\n",
    "        Path to file with test listing IDs\n",
    "    output_dir : str\n",
    "        Directory to save output files\n",
    "    k : int\n",
    "        Number of neighbors to find for each test listing\n",
    "    use_embedding : bool\n",
    "        Whether to use neural embeddings for similarity\n",
    "    similarity_threshold : float, optional\n",
    "        Maximum distance threshold for keeping neighbors (None=auto-determine)\n",
    "    min_neighbors : int, default=1\n",
    "        Minimum number of neighbors to keep per listing\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Verify CUDA is available\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This implementation requires a GPU.\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading data from {train_path}\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    \n",
    "    # Load listing IDs\n",
    "    print(f\"Loading train IDs from {train_ids_path}\")\n",
    "    with open(train_ids_path, 'r') as f:\n",
    "        train_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loading test IDs from {test_ids_path}\")\n",
    "    with open(test_ids_path, 'r') as f:\n",
    "        test_ids = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded {len(train_ids)} train IDs and {len(test_ids)} test IDs\")\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    \n",
    "    # Create calculated features if not already present\n",
    "    if 'amenity_count' not in train_data.columns:\n",
    "        amenity_cols = [col for col in train_data.columns if col.startswith('has_')]\n",
    "        if amenity_cols:\n",
    "            train_data['amenity_count'] = train_data[amenity_cols].sum(axis=1)\n",
    "    \n",
    "    if 'bedroom_ratio' not in train_data.columns and 'bedrooms' in train_data.columns and 'accommodates' in train_data.columns:\n",
    "        train_data['bedroom_ratio'] = train_data['bedrooms'] / train_data['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Output path for the neighbor dictionary\n",
    "    output_path = os.path.join(output_dir, \"neighbor_dict.pkl\")\n",
    "    \n",
    "    # Run GPU-accelerated neighbor selection with similarity filtering\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    neighbor_type = \"neural embeddings\" if use_embedding else \"weighted features\"\n",
    "    threshold_type = \"auto-determined\" if similarity_threshold is None else f\"user-specified ({similarity_threshold:.4f})\"\n",
    "    \n",
    "    print(f\"Finding neighbors using {neighbor_type} with {threshold_type} similarity threshold\")\n",
    "    print(f\"Will keep a minimum of {min_neighbors} neighbors per listing\")\n",
    "    \n",
    "    neighbors = gpu_neighbor_selection(\n",
    "        train_data, train_ids, test_ids, k=k,\n",
    "        output_path=output_path, use_embedding=use_embedding,\n",
    "        similarity_threshold=similarity_threshold, min_neighbors=min_neighbors\n",
    "    )\n",
    "    \n",
    "    # Evaluate neighbor quality\n",
    "    print(\"\\nEvaluating neighbor quality...\")\n",
    "    quality_metrics = evaluate_neighbor_quality(train_data, train_ids, test_ids, neighbors)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nNeighbor Quality Evaluation:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    # Save quality metrics\n",
    "    with open(os.path.join(output_dir, \"neighbor_quality.txt\"), 'w') as f:\n",
    "        f.write(\"Neighbor Quality Evaluation:\\n\")\n",
    "        f.write(f\"Number of neighbors requested per listing: {k}\\n\")\n",
    "        f.write(f\"Using similarity filtering: Yes\\n\")\n",
    "        f.write(f\"Similarity threshold: {threshold_type}\\n\")\n",
    "        f.write(f\"Minimum neighbors per listing: {min_neighbors}\\n\")\n",
    "        f.write(f\"Total test listings: {len(test_ids)}\\n\")\n",
    "        f.write(f\"Total train listings: {len(train_ids)}\\n\\n\")\n",
    "        for metric, value in quality_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                f.write(f\"{metric}: {value:.6f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: {value}\\n\")\n",
    "    \n",
    "    print(f\"\\nNeighbor selection complete. Results saved to {output_dir}\")\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\n",
      "Loading train IDs from C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\n",
      "Loading test IDs from C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Finding neighbors using neural embeddings with auto-determined similarity threshold\n",
      "Will keep a minimum of 1 neighbors per listing\n",
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Processing 6291 train and 1573 test listings\n",
      "Training embedding network for improved similarity...\n",
      "Using batch size: 1024\n",
      "Epoch 5/20, Avg Loss: 0.021838\n",
      "Epoch 10/20, Avg Loss: -0.085147\n",
      "Epoch 15/20, Avg Loss: -0.132964\n",
      "Epoch 20/20, Avg Loss: -0.157095\n",
      "Building FAISS index for fast similarity search...\n",
      "Using GPU FAISS index\n",
      "Searching for 15 candidate neighbors per test listing...\n",
      "Auto-determined similarity threshold: 0.0004\n",
      "Found neighbors for 1573 test listings\n",
      "Filtered out 18557 neighbors in total due to similarity threshold\n",
      "584 listings had fewer than requested neighbors after filtering\n",
      "Average number of neighbors per listing after filtering: 3.20\n",
      "Saved neighbor dictionary to ./neighbor_data\\neighbor_dict.pkl\n",
      "Saved human-readable neighbor list to ./neighbor_data\\neighbor_dict.csv\n",
      "Average geographic distance to neighbors: 0.037008\n",
      "Median geographic distance to neighbors: 0.032583\n",
      "GPU-accelerated neighbor selection completed in 1.89 seconds\n",
      "\n",
      "Evaluating neighbor quality...\n",
      "\n",
      "Neighbor Quality Evaluation:\n",
      "  avg_geo_distance_km: 3.1423\n",
      "  median_geo_distance_km: 2.9257\n",
      "  avg_amenity_similarity: 0.8938\n",
      "  avg_price_error: 89.9616\n",
      "  median_price_error: 50.0000\n",
      "  rmse: 151.0868\n",
      "  avg_neighbor_count: 3.2028\n",
      "  min_neighbor_count: 1\n",
      "  max_neighbor_count: 5\n",
      "  listings_with_neighbors_pct: 100.0000\n",
      "\n",
      "Neighbor selection complete. Results saved to ./neighbor_data\n"
     ]
    }
   ],
   "source": [
    "neighbors = run_neighbor_selection(\n",
    "    train_path=r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_up3.csv\",\n",
    "    train_ids_path=r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\",\n",
    "    test_ids_path=r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\",\n",
    "    output_dir=\"./neighbor_data\",\n",
    "    k=5,                         # Maximum neighbors per listing\n",
    "    use_embedding=True,          # Use neural embeddings for better similarity\n",
    "    similarity_threshold=None,   # None = auto-determine the threshold\n",
    "    min_neighbors=1              # Ensure at least this many neighbors per listing\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
