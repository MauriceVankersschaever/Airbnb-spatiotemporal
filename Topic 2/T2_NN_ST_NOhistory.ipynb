{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Creating calculated features...\n",
      "Train data: 1123327 rows, 6291 unique listings\n",
      "Test data: 281142 rows, 1573 unique listings\n",
      "Using 2 spatial features, 5 temporal features, and 26 amenity features\n",
      "Applying log transformation to price data\n",
      "Applying log transformation to price data\n",
      "Train subset: 898711 rows, 5032 listings\n",
      "Validation subset: 224616 rows, 1259 listings\n",
      "Using device: cuda\n",
      "Preparing data for neighbor-based price prediction...\n",
      "Selecting 5 neighbors for each listing...\n",
      "Selected neighbors for 1123327 listings\n",
      "Building spatial graph with 224616 test listings and 5 nearest neighbors...\n",
      "Created graph with 2246160 edges\n",
      "Collecting price histories from neighbors...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "# ====================== Utility Functions ======================\n",
    "# Price transformation function\n",
    "def apply_price_transformation(data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Create calculated features\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Create calculated features for the dataset\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                          'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RÂ²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Function to plot results\n",
    "def plot_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot prediction results\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    plt.annotate(f'Correlation: {corr:.4f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add mean, median error\n",
    "    mean_error = np.mean(errors)\n",
    "    median_error = np.median(errors)\n",
    "    plt.axvline(mean_error, color='g', linestyle='-', label=f'Mean: {mean_error:.2f}')\n",
    "    plt.axvline(median_error, color='b', linestyle='-', label=f'Median: {median_error:.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    \n",
    "    # Mark median and mean\n",
    "    median_pct = np.median(pct_errors)\n",
    "    mean_pct = np.mean(pct_errors)\n",
    "    plt.axvline(median_pct, color='r', linestyle='--', label=f'Median: {median_pct:.2f}%')\n",
    "    plt.axvline(mean_pct, color='g', linestyle='--', label=f'Mean: {mean_pct:.2f}%')\n",
    "    \n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'prediction_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'prediction_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ====================== Neighbor Selection Functions ======================\n",
    "def select_neighbors(data, k=5, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Select k nearest neighbors for each listing based on geographic and feature similarity\n",
    "    \"\"\"\n",
    "    print(f\"Selecting {k} neighbors for each listing...\")\n",
    "    \n",
    "    # Extract coordinates\n",
    "    coords = data[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Extract and normalize key features for similarity\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        feature_values = scaler.fit_transform(data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        feature_values = np.ones((len(coords), 1))\n",
    "    \n",
    "    # Find k+1 nearest neighbors for each listing (including itself)\n",
    "    nn = NearestNeighbors(n_neighbors=min(k+1, len(coords)))\n",
    "    nn.fit(coords)\n",
    "    distances, indices = nn.kneighbors(coords)\n",
    "    \n",
    "    # For each listing, store its neighbors (excluding itself)\n",
    "    neighbors_dict = {}\n",
    "    \n",
    "    for i, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        # Skip the first index if it's the listing itself (distance = 0)\n",
    "        if neighbor_distances[0] < 1e-8:\n",
    "            neighbor_indices = neighbor_indices[1:k+1]\n",
    "            neighbor_distances = neighbor_distances[1:k+1]\n",
    "        else:\n",
    "            # Take first k neighbors\n",
    "            neighbor_indices = neighbor_indices[:k]\n",
    "            neighbor_distances = neighbor_distances[:k]\n",
    "            \n",
    "        # Calculate feature similarity for each neighbor\n",
    "        neighbors_with_similarity = []\n",
    "        \n",
    "        for j, neighbor_idx in enumerate(neighbor_indices):\n",
    "            if j >= len(neighbor_distances):\n",
    "                break\n",
    "                \n",
    "            geo_distance = neighbor_distances[j]\n",
    "            \n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            listing_feat = feature_values[i]\n",
    "            neighbor_feat = feature_values[neighbor_idx]\n",
    "            \n",
    "            feat_norm_product = np.linalg.norm(listing_feat) * np.linalg.norm(neighbor_feat)\n",
    "            if feat_norm_product > 1e-8:\n",
    "                feat_sim = np.dot(listing_feat, neighbor_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "                \n",
    "            # Combined weight\n",
    "            geo_weight = 1.0 / (geo_distance + 1e-6)  # Inverse distance\n",
    "            combined_score = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            neighbors_with_similarity.append((neighbor_idx, combined_score))\n",
    "        \n",
    "        # Sort by combined similarity and take top k\n",
    "        neighbors_with_similarity.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_neighbors = [n[0] for n in neighbors_with_similarity[:k]]\n",
    "        \n",
    "        # Store in dictionary\n",
    "        neighbors_dict[i] = selected_neighbors\n",
    "    \n",
    "    print(f\"Selected neighbors for {len(neighbors_dict)} listings\")\n",
    "    return neighbors_dict\n",
    "\n",
    "\n",
    "def collect_neighbor_price_history(data, listing_ids, neighbors_dict, seq_length=30):\n",
    "    \"\"\"\n",
    "    Collect historical price data from neighbors for each listing\n",
    "    Returns a dictionary mapping listing indices to arrays of neighbor price histories\n",
    "    \"\"\"\n",
    "    print(f\"Collecting price history for neighbors (sequence length: {seq_length})...\")\n",
    "    \n",
    "    # Create mapping from listing ID to index\n",
    "    listing_id_to_idx = {id: idx for idx, id in enumerate(listing_ids)}\n",
    "    idx_to_listing_id = {idx: id for idx, id in enumerate(listing_ids)}\n",
    "    \n",
    "    # Initialize dictionary to store neighbor histories\n",
    "    neighbor_histories = {}\n",
    "    \n",
    "    # For each listing\n",
    "    for idx, listing_id in enumerate(listing_ids):\n",
    "        # Get neighbors of this listing\n",
    "        if idx not in neighbors_dict:\n",
    "            continue\n",
    "            \n",
    "        neighbors = neighbors_dict[idx]\n",
    "        \n",
    "        # For each date this listing has data for\n",
    "        listing_dates = data[data['listing_id'] == listing_id]['date'].sort_values().unique()\n",
    "        \n",
    "        for date in listing_dates:\n",
    "            # For each neighbor, collect price history up to this date\n",
    "            neighbor_prices = []\n",
    "            \n",
    "            for neighbor_idx in neighbors:\n",
    "                # Get neighbor's listing_id\n",
    "                neighbor_id = idx_to_listing_id.get(neighbor_idx)\n",
    "                if neighbor_id is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get neighbor's historical data up to this date\n",
    "                neighbor_history = data[\n",
    "                    (data['listing_id'] == neighbor_id) & \n",
    "                    (data['date'] < date)\n",
    "                ].sort_values('date', ascending=False).head(seq_length)\n",
    "                \n",
    "                # Extract prices\n",
    "                if len(neighbor_history) > 0:\n",
    "                    prices = neighbor_history['price'].values\n",
    "                    # Pad if shorter than seq_length\n",
    "                    padded = np.pad(prices, (0, max(0, seq_length - len(prices))), \n",
    "                                    'constant', constant_values=prices[0] if len(prices) > 0 else 0)\n",
    "                    neighbor_prices.append(padded[:seq_length])\n",
    "                else:\n",
    "                    # If no history, use zeros\n",
    "                    neighbor_prices.append(np.zeros(seq_length))\n",
    "            \n",
    "            # Store in dictionary\n",
    "            key = (listing_id, date)\n",
    "            neighbor_histories[key] = np.array(neighbor_prices)\n",
    "    \n",
    "    print(f\"Collected price histories for {len(neighbor_histories)} listing-date combinations\")\n",
    "    return neighbor_histories\n",
    "\n",
    "\n",
    "# Build enhanced spatial graph for GNN\n",
    "def build_spatial_graph(train_data, test_data, k=5, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-Î±) * geo_weight + Î± * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (for smaller datasets)\n",
    "    if len(train_coords) <= 5000:\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "\n",
    "# ====================== Model Definition ======================\n",
    "class LightweightLSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight LSTM encoder for time series with reduced dimensionality\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=8, output_dim=64, bidirectional=True):\n",
    "        super(LightweightLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM with small hidden dimension\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,  # Single feature per timestep (price)\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Projection from LSTM output to desired output dimension\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.projection = nn.Linear(lstm_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input if needed: [batch_size, seq_len] -> [batch_size, seq_len, 1]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final hidden states from both directions\n",
    "            hidden_forward = hidden[-2, :, :]\n",
    "            hidden_backward = hidden[-1, :, :]\n",
    "            combined = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            # Use only the final hidden state\n",
    "            combined = hidden[-1, :, :]\n",
    "            \n",
    "        # Project to output dimension\n",
    "        projected = self.projection(combined)\n",
    "        return projected\n",
    "\n",
    "\n",
    "class NeighborPricePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN model that predicts listing prices using only neighboring listings' histories\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 num_neighbors=5,\n",
    "                 seq_length=30,\n",
    "                 lstm_hidden_dim=8,  # Lightweight LSTM dimension\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(NeighborPricePredictor, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.seq_length = seq_length\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # For multi-head attention\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # GAT layers for spatial features\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Temporal feature processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Amenity feature processing\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Lightweight LSTM for neighbor price history\n",
    "        self.neighbor_lstm = LightweightLSTMEncoder(\n",
    "            hidden_dim=lstm_hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Neighbor attention mechanism\n",
    "        self.neighbor_attention = nn.Parameter(torch.ones(num_neighbors, 1))\n",
    "        \n",
    "        # Feature fusion mechanism\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))  # spatial, temporal, amenity, neighbor\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, neighbor_histories = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.neighbor_histories\n",
    "        )\n",
    "        \n",
    "        # Process spatial features with GAT\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res\n",
    "        \n",
    "        # Process amenity features\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res\n",
    "        \n",
    "        # Process neighbor histories\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Initialize neighbor embeddings for each neighbor in the batch\n",
    "        neighbor_embeddings = []\n",
    "        \n",
    "        # For each neighbor position\n",
    "        for n_idx in range(self.num_neighbors):\n",
    "            # Get this neighbor's price history for all listings in batch\n",
    "            # Shape: [batch_size, seq_length]\n",
    "            neighbor_history = neighbor_histories[:, n_idx, :]\n",
    "            \n",
    "            # Pass through lightweight LSTM encoder\n",
    "            neighbor_embedding = self.neighbor_lstm(neighbor_history)\n",
    "            neighbor_embeddings.append(neighbor_embedding)\n",
    "        \n",
    "        # Apply attention to weight different neighbors\n",
    "        normalized_attention = F.softmax(self.neighbor_attention, dim=0)\n",
    "        \n",
    "        # Weighted sum of neighbor embeddings\n",
    "        neighbor_features = torch.zeros_like(neighbor_embeddings[0])\n",
    "        for n_idx in range(self.num_neighbors):\n",
    "            neighbor_features += neighbor_embeddings[n_idx] * normalized_attention[n_idx]\n",
    "        \n",
    "        # Feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            neighbor_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "\n",
    "# ====================== Data Preparation Functions ======================\n",
    "def prepare_neighbor_data(train_data, val_data, spatial_features, temporal_features, \n",
    "                         amenity_features, num_neighbors=5, seq_length=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare data for the neighbor-based price prediction model\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for neighbor-based price prediction...\")\n",
    "    \n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Transform features\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features]).astype(np.float32)\n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features]).astype(np.float32)\n",
    "    \n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features]).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features]).astype(np.float32)\n",
    "    \n",
    "    # Transform targets\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val for graph construction\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    \n",
    "    # Get unique listing IDs\n",
    "    train_listings = train_data['listing_id'].unique()\n",
    "    val_listings = val_data['listing_id'].unique()\n",
    "    \n",
    "    # Create combined dataset for neighbor selection\n",
    "    combined_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "    \n",
    "    # Select neighbors for each listing\n",
    "    neighbors_dict = select_neighbors(combined_data, k=num_neighbors)\n",
    "    \n",
    "    # Build spatial graph\n",
    "    edge_index, edge_attr = build_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=num_neighbors\n",
    "    )\n",
    "    \n",
    "    # Collect neighbor price histories\n",
    "    all_listings = np.concatenate([train_listings, val_listings])\n",
    "    \n",
    "    # Initialize neighbor histories tensor: [num_nodes, num_neighbors, seq_length]\n",
    "    X_neighbor_histories = np.zeros((len(combined_data), num_neighbors, seq_length), dtype=np.float32)\n",
    "    \n",
    "    # Get price histories from neighbors\n",
    "    print(\"Collecting price histories from neighbors...\")\n",
    "    for i, listing_id in enumerate(combined_data['listing_id'].unique()):\n",
    "        # Get data for this listing\n",
    "        listing_data = combined_data[combined_data['listing_id'] == listing_id]\n",
    "        \n",
    "        # For each row (date) for this listing\n",
    "        for _, row in listing_data.iterrows():\n",
    "            # Get index in combined data\n",
    "            idx = combined_data[(combined_data['listing_id'] == listing_id) & \n",
    "                                (combined_data['date'] == row['date'])].index[0]\n",
    "            \n",
    "            # Internal index for neighbors dictionary\n",
    "            internal_idx = np.where(all_listings == listing_id)[0][0] if listing_id in all_listings else None\n",
    "            \n",
    "            if internal_idx is not None and internal_idx in neighbors_dict:\n",
    "                # Get neighbors\n",
    "                neighbors = neighbors_dict[internal_idx]\n",
    "                \n",
    "                # For each neighbor\n",
    "                for n_idx, neighbor_idx in enumerate(neighbors[:num_neighbors]):\n",
    "                    if n_idx >= num_neighbors:\n",
    "                        break\n",
    "                        \n",
    "                    # Get neighbor listing ID\n",
    "                    if neighbor_idx < len(all_listings):\n",
    "                        neighbor_id = all_listings[neighbor_idx]\n",
    "                        \n",
    "                        # Get neighbor's price history prior to this date\n",
    "                        neighbor_history = combined_data[\n",
    "                            (combined_data['listing_id'] == neighbor_id) & \n",
    "                            (combined_data['date'] < row['date'])\n",
    "                        ].sort_values('date', ascending=False).head(seq_length)\n",
    "                        \n",
    "                        # Store neighbor's prices\n",
    "                        if len(neighbor_history) > 0:\n",
    "                            prices = neighbor_history['price'].values\n",
    "                            # Pad if needed\n",
    "                            X_neighbor_histories[idx, n_idx, :len(prices)] = prices[:seq_length]\n",
    "                            \n",
    "                            # If fewer than seq_length days of history, pad with last known price\n",
    "                            if len(prices) < seq_length:\n",
    "                                X_neighbor_histories[idx, n_idx, len(prices):] = prices[-1]\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Normalize neighbor histories\n",
    "    # Reshape to 2D for scaling\n",
    "    orig_shape = X_neighbor_histories.shape\n",
    "    X_neighbor_histories_2d = X_neighbor_histories.reshape(-1, seq_length)\n",
    "    \n",
    "    # Create scaler\n",
    "    neighbor_scaler = StandardScaler()\n",
    "    X_neighbor_histories_scaled = neighbor_scaler.fit_transform(X_neighbor_histories_2d)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_neighbor_histories = X_neighbor_histories_scaled.reshape(orig_shape)\n",
    "    \n",
    "    # Create PyG data object\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        neighbor_histories=torch.FloatTensor(X_neighbor_histories).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    \n",
    "    print(f\"Prepared data with {len(X_combined_spatial)} nodes\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, neighbors_dict\n",
    "\n",
    "\n",
    "# ====================== Training and Evaluation Functions ======================\n",
    "def train_model(data, model, epochs=50, lr=0.001, patience=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the neighbor-based price prediction model\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Training Neighbor-Based Price Prediction Model =====\")\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    counter = 0\n",
    "    \n",
    "    # Store history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[data.train_mask]\n",
    "        train_y = data.y[data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(data)[data.val_mask]\n",
    "            val_y = data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Calculate metrics on scaled data\n",
    "            val_rmse = torch.sqrt(F.mse_loss(val_out, val_y)).item()\n",
    "            val_mae = F.l1_loss(val_out, val_y).item()\n",
    "            \n",
    "        # Store history\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory management\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def predict(model, data, target_scaler, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        predictions = model(data)[data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation if applied\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "\n",
    "def evaluate_model(model, data, true_prices, target_scaler, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = predict(model, data, target_scaler, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_predictions(true_prices, predictions.flatten())\n",
    "    \n",
    "    return predictions, metrics\n",
    "\n",
    "\n",
    "# ====================== Main Function ======================\n",
    "def run_neighbor_based_prediction(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                 sample_size=None, num_neighbors=5, seq_length=30,\n",
    "                                 lstm_hidden_dim=8, hidden_dim=64, epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Complete pipeline for training and evaluating the neighbor-based price prediction model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if not exists\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "\n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.75), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.25), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime if needed\n",
    "        if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "            train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Split data into train and test based on listing IDs\n",
    "        train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "        test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        train_df = train_data[train_mask].copy()\n",
    "        test_df = train_data[test_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Define feature groups based on your dataset columns\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Temporal features - using your DTF prefixed features\n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "        amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "        basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', \n",
    "                                   'luxury_score', 'amenity_count']\n",
    "        available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "        amenity_features.extend(available_basic_features)\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "        amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "        \n",
    "        # If any feature group is empty, create dummy features\n",
    "        if not amenity_features:\n",
    "            print(\"No amenity features found, creating dummy feature\")\n",
    "            train_df['dummy_amenity'] = 1\n",
    "            test_df['dummy_amenity'] = 1\n",
    "            amenity_features = ['dummy_amenity']\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"and {len(amenity_features)} amenity features\")\n",
    "        \n",
    "        # Apply log transformation to prices\n",
    "        train_df = apply_price_transformation(train_df)\n",
    "        test_df = apply_price_transformation(test_df)\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = train_df['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "        print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Memory management before training\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Prepare data with neighbor price histories\n",
    "        data, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, neighbors_dict = prepare_neighbor_data(\n",
    "            train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "            num_neighbors=num_neighbors, seq_length=seq_length, device=device\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = NeighborPricePredictor(\n",
    "            spatial_features_dim=len(spatial_features),\n",
    "            temporal_features_dim=len(temporal_features),\n",
    "            amenity_features_dim=len(amenity_features),\n",
    "            num_neighbors=num_neighbors,\n",
    "            seq_length=seq_length,\n",
    "            lstm_hidden_dim=lstm_hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=0.3,\n",
    "            heads=4,\n",
    "            edge_dim=1\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train model\n",
    "        model, history = train_model(\n",
    "            data, model, epochs=epochs, lr=lr, patience=10, device=device\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation RMSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['val_rmse'], label='Validation RMSE')\n",
    "        plt.title('Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        \n",
    "        # Plot validation MAE\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        # Plot learning rate\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['lr'], label='Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('LR')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on validation data\n",
    "        val_predictions, val_metrics = evaluate_model(\n",
    "            model, data, val_subset['original_price'].values, target_scaler, device\n",
    "        )\n",
    "        \n",
    "        # Prepare test data for final evaluation\n",
    "        test_data, _, _, _, _, _, _ = prepare_neighbor_data(\n",
    "            train_subset, test_df, spatial_features, temporal_features, amenity_features,\n",
    "            num_neighbors=num_neighbors, seq_length=seq_length, device=device\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        test_predictions, test_metrics = evaluate_model(\n",
    "            model, test_data, test_df['original_price'].values, target_scaler, device\n",
    "        )\n",
    "        \n",
    "        # Plot results\n",
    "        plot_results(test_df['original_price'].values, test_predictions.flatten(), output_dir=output_dir)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        if output_dir:\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'neighbor_model.pt'))\n",
    "            torch.save({\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'amenity_scaler': amenity_scaler,\n",
    "                'neighbor_scaler': neighbor_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'num_neighbors': num_neighbors,\n",
    "                'seq_length': seq_length,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim\n",
    "            }, os.path.join(output_dir, 'scalers.pt'))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "            # Save test predictions\n",
    "            test_results = pd.DataFrame({\n",
    "                'listing_id': test_df['listing_id'].values,\n",
    "                'date': test_df['date'].values,\n",
    "                'actual': test_df['original_price'].values,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_df['original_price'].values - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_df['original_price'].values - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_df['original_price'].values - test_predictions.flatten()) / \n",
    "                                   (test_df['original_price'].values + 1e-8)) * 100\n",
    "            })\n",
    "            test_results.to_csv(os.path.join(output_dir, 'test_predictions.csv'), index=False)\n",
    "            print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.csv')}\")\n",
    "            \n",
    "            # Save model parameters and results\n",
    "            with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:\n",
    "                f.write(\"Neighbor-Based Price Prediction Model Summary\\n\")\n",
    "                f.write(\"===========================================\\n\\n\")\n",
    "                f.write(f\"Number of neighbors: {num_neighbors}\\n\")\n",
    "                f.write(f\"Sequence length: {seq_length}\\n\")\n",
    "                f.write(f\"LSTM hidden dimension: {lstm_hidden_dim}\\n\")\n",
    "                f.write(f\"Hidden dimension: {hidden_dim}\\n\")\n",
    "                f.write(f\"Learning rate: {lr}\\n\")\n",
    "                f.write(f\"Epochs: {epochs}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Feature counts:\\n\")\n",
    "                f.write(f\"  Spatial features: {len(spatial_features)}\\n\")\n",
    "                f.write(f\"  Temporal features: {len(temporal_features)}\\n\")\n",
    "                f.write(f\"  Amenity features: {len(amenity_features)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Test Results:\\n\")\n",
    "                for metric, value in test_metrics.items():\n",
    "                    f.write(f\"  {metric}: {value:.6f}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== NEIGHBOR-BASED PRICE PREDICTION SUMMARY =====\")\n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"and {len(amenity_features)} amenity features\")\n",
    "        print(f\"Number of neighbors: {num_neighbors}\")\n",
    "        print(f\"LSTM hidden dimension: {lstm_hidden_dim}\")\n",
    "        \n",
    "        print(\"\\n=== Test Metrics ===\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Return model and results\n",
    "        return model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, test_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in neighbor-based price prediction: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def visualize_neighbor_attention(model, output_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for neighboring listings\n",
    "    \"\"\"\n",
    "    # Extract neighbor attention weights\n",
    "    attention_weights = F.softmax(model.neighbor_attention, dim=0).cpu().detach().numpy()\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot attention weights\n",
    "    plt.bar(range(len(attention_weights)), attention_weights.flatten())\n",
    "    plt.xticks(range(len(attention_weights)), [f\"Neighbor {i+1}\" for i in range(len(attention_weights))])\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.title('Neighbor Attention Weights')\n",
    "    \n",
    "    # Add weight values as text\n",
    "    for i, weight in enumerate(attention_weights.flatten()):\n",
    "        plt.text(i, weight + 0.01, f\"{weight:.3f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if output_dir provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'neighbor_attention.png'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_feature_importance(model, output_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize feature fusion weights\n",
    "    \"\"\"\n",
    "    # Extract feature fusion weights\n",
    "    fusion_weights = F.softmax(model.fusion_weights, dim=0).cpu().detach().numpy()\n",
    "    \n",
    "    # Feature types\n",
    "    feature_types = ['Spatial', 'Temporal', 'Amenity', 'Neighbor']\n",
    "    \n",
    "    # Calculate average weight for each feature type\n",
    "    avg_weights = [np.mean(fusion_weights[i]) for i in range(len(feature_types))]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot feature weights\n",
    "    bars = plt.bar(range(len(feature_types)), avg_weights)\n",
    "    plt.xticks(range(len(feature_types)), feature_types)\n",
    "    plt.ylabel('Average Weight')\n",
    "    plt.title('Feature Type Importance')\n",
    "    \n",
    "    # Add weight values as text\n",
    "    for i, weight in enumerate(avg_weights):\n",
    "        plt.text(i, weight + 0.01, f\"{weight:.3f}\", ha='center')\n",
    "    \n",
    "    # Highlight neighbor features\n",
    "    bars[3].set_color('orange')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if output_dir provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'feature_importance.png'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ====================== Main Execution ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\topic2_transformed.csv\"\n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"./output/neighbor_price_prediction\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters\n",
    "    num_neighbors = 5          # Number of neighbors to use\n",
    "    seq_length = 30            # Sequence length for price history\n",
    "    lstm_hidden_dim = 16       # Lightweight LSTM hidden dimension (8 or 16)\n",
    "    hidden_dim = 64            # Hidden dimension for rest of model\n",
    "    epochs = 50                # Maximum number of epochs\n",
    "    lr = 0.001                 # Learning rate\n",
    "    \n",
    "    # Run full pipeline\n",
    "    results = run_neighbor_based_prediction(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        output_dir=output_dir,\n",
    "        num_neighbors=num_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        sample_size=None  # Set to a number for testing or None for full dataset\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        model, *_, test_metrics = results\n",
    "        \n",
    "        # Visualize attention weights\n",
    "        visualize_neighbor_attention(model, output_dir)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        visualize_feature_importance(model, output_dir)\n",
    "        \n",
    "        # Calculate parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        lstm_params = sum(p.numel() for name, p in model.named_parameters() if 'lstm' in name)\n",
    "        \n",
    "        print(\"\\n===== Model Size Information =====\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"LSTM parameters: {lstm_params:,} ({lstm_params/total_params*100:.2f}% of total)\")\n",
    "        print(f\"Model is using {lstm_hidden_dim}-dimensional LSTM\")\n",
    "        \n",
    "        # Save parameter count\n",
    "        with open(os.path.join(output_dir, 'model_size.txt'), 'w') as f:\n",
    "            f.write(f\"Total parameters: {total_params:,}\\n\")\n",
    "            f.write(f\"LSTM parameters: {lstm_params:,} ({lstm_params/total_params*100:.2f}% of total)\\n\")\n",
    "            f.write(f\"Model is using {lstm_hidden_dim}-dimensional LSTM\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
