{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Define the NeighborBasedLSTM model\n",
    "class NeighborBasedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, temporal_features_dim, property_features_dim, \n",
    "                 max_neighbors=5, lstm_hidden_dim=16, hidden_dim=64, dropout=0.3):\n",
    "        super(NeighborBasedLSTM, self).__init__()\n",
    "        \n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # LSTM for processing neighbor price histories\n",
    "        self.neighbor_lstm = nn.LSTM(\n",
    "            input_size=1,  # Single feature per timestep (price)\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism to weight neighbor contributions\n",
    "        self.attention = nn.Linear(lstm_hidden_dim * 2, 1)\n",
    "        \n",
    "        # Processing for property features\n",
    "        self.property_layer1 = nn.Linear(property_features_dim, hidden_dim)\n",
    "        self.property_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.property_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.property_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Processing for temporal features\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion layer\n",
    "        combined_dim = hidden_dim * 2 + lstm_hidden_dim * 2\n",
    "        self.fusion_layer = nn.Linear(combined_dim, hidden_dim)\n",
    "        self.fusion_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the data\n",
    "        property_features = data.property_features\n",
    "        temporal_features = data.temporal_features\n",
    "        neighbor_histories = data.neighbor_histories\n",
    "        neighbor_mask = data.neighbor_mask\n",
    "        \n",
    "        batch_size = property_features.shape[0]\n",
    "        \n",
    "        # Process property features with residual connection\n",
    "        prop_out = F.relu(self.property_layer1(property_features))\n",
    "        prop_out = self.property_bn1(prop_out)\n",
    "        prop_out = self.dropout(prop_out)\n",
    "        prop_out_res = prop_out\n",
    "        prop_out = F.relu(self.property_layer2(prop_out))\n",
    "        prop_out = self.property_bn2(prop_out)\n",
    "        prop_out = prop_out + prop_out_res  # Residual connection\n",
    "        \n",
    "        # Process temporal features with residual connection\n",
    "        temp_out = F.relu(self.temporal_layer1(temporal_features))\n",
    "        temp_out = self.temporal_bn1(temp_out)\n",
    "        temp_out = self.dropout(temp_out)\n",
    "        temp_out_res = temp_out\n",
    "        temp_out = F.relu(self.temporal_layer2(temp_out))\n",
    "        temp_out = self.temporal_bn2(temp_out)\n",
    "        temp_out = temp_out + temp_out_res  # Residual connection\n",
    "        \n",
    "        # Process neighbor histories with LSTM\n",
    "        # Reshape: [batch, max_neighbors, seq_len] -> [batch*max_neighbors, seq_len, 1]\n",
    "        seq_len = neighbor_histories.size(2)\n",
    "        reshaped_histories = neighbor_histories.view(batch_size * self.max_neighbors, seq_len, 1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, (h_n, _) = self.neighbor_lstm(reshaped_histories)\n",
    "        \n",
    "        # Get final hidden states from both directions\n",
    "        h_forward = h_n[0]  # [batch*max_neighbors, lstm_hidden_dim]\n",
    "        h_backward = h_n[1]  # [batch*max_neighbors, lstm_hidden_dim]\n",
    "        h_combined = torch.cat([h_forward, h_backward], dim=1)  # [batch*max_neighbors, lstm_hidden_dim*2]\n",
    "        \n",
    "        # Reshape to [batch, max_neighbors, lstm_hidden_dim*2]\n",
    "        h_combined = h_combined.view(batch_size, self.max_neighbors, -1)\n",
    "        \n",
    "        # Apply attention to weight the neighbors' contributions\n",
    "        attention_scores = self.attention(h_combined)  # [batch, max_neighbors, 1]\n",
    "        attention_scores = attention_scores.squeeze(-1)  # [batch, max_neighbors]\n",
    "        \n",
    "        # Apply mask to ignore padding (non-existent neighbors)\n",
    "        attention_scores = attention_scores.masked_fill(~neighbor_mask, -1e9)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(-1)  # [batch, max_neighbors, 1]\n",
    "        \n",
    "        # Apply attention weights to neighbor features\n",
    "        weighted_features = h_combined * attention_weights  # [batch, max_neighbors, lstm_hidden_dim*2]\n",
    "        \n",
    "        # Sum across all neighbors\n",
    "        neighbor_context = weighted_features.sum(dim=1)  # [batch, lstm_hidden_dim*2]\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([prop_out, temp_out, neighbor_context], dim=1)\n",
    "        combined = F.relu(self.fusion_layer(combined))\n",
    "        combined = self.fusion_bn(combined)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        out = F.relu(self.fc1(combined))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        price_prediction = self.fc2(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "def load_neighbor_data(neighbor_csv_path):\n",
    "    \"\"\"\n",
    "    Load neighbor relationships from CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Loading neighbor data from {neighbor_csv_path}\")\n",
    "    neighbor_df = pd.read_csv(neighbor_csv_path)\n",
    "    \n",
    "    # Create a dictionary mapping test_listing_id to its neighbors\n",
    "    neighbor_dict = {}\n",
    "    \n",
    "    for _, row in neighbor_df.iterrows():\n",
    "        test_id = row['test_listing_id']\n",
    "        neighbor_id = row['neighbor_listing_id']\n",
    "        rank = row['rank']\n",
    "        distance = row['distance']\n",
    "        \n",
    "        if test_id not in neighbor_dict:\n",
    "            neighbor_dict[test_id] = []\n",
    "        \n",
    "        neighbor_dict[test_id].append({\n",
    "            'neighbor_id': neighbor_id,\n",
    "            'rank': rank,\n",
    "            'distance': distance\n",
    "        })\n",
    "    \n",
    "    # Sort neighbors by rank for each test listing\n",
    "    for test_id in neighbor_dict:\n",
    "        neighbor_dict[test_id] = sorted(neighbor_dict[test_id], key=lambda x: x['rank'])\n",
    "    \n",
    "    print(f\"Loaded neighbor data for {len(neighbor_dict)} test listings\")\n",
    "    return neighbor_dict\n",
    "\n",
    "def extract_price_history(listing_data, date, seq_length=30):\n",
    "    \"\"\"\n",
    "    Extract price history for a listing up to a specific date\n",
    "    \"\"\"\n",
    "    previous_data = listing_data[listing_data['date'] < date].sort_values('date', ascending=False)\n",
    "    \n",
    "    # Extract prices\n",
    "    price_history = []\n",
    "    for _, row in previous_data.head(seq_length).iterrows():\n",
    "        price_history.append(row['price'])\n",
    "    \n",
    "    # Pad if needed\n",
    "    if len(price_history) < seq_length:\n",
    "        padding = [price_history[-1] if price_history else 0] * (seq_length - len(price_history))\n",
    "        price_history.extend(padding)\n",
    "    \n",
    "    # Keep only the most recent seq_length prices and reverse to chronological order\n",
    "    price_history = price_history[:seq_length]\n",
    "    price_history.reverse()\n",
    "    \n",
    "    return price_history\n",
    "\n",
    "def prepare_neighbor_data_batch(test_data, train_data, neighbor_dict, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare batched neighbor data for all test instances\n",
    "    \"\"\"\n",
    "    print(\"Preparing neighbor data batch...\")\n",
    "    \n",
    "    # Create lookup dictionary for train listings\n",
    "    train_listings_dict = {}\n",
    "    for listing_id in train_data['listing_id'].unique():\n",
    "        listing_data = train_data[train_data['listing_id'] == listing_id].sort_values('date')\n",
    "        train_listings_dict[listing_id] = listing_data\n",
    "    \n",
    "    # Initialize arrays to store neighbor histories and masks\n",
    "    test_size = len(test_data)\n",
    "    neighbor_histories = np.zeros((test_size, max_neighbors, seq_length), dtype=np.float32)\n",
    "    neighbor_masks = np.zeros((test_size, max_neighbors), dtype=bool)\n",
    "    \n",
    "    # Process each test instance\n",
    "    for idx, (_, test_row) in enumerate(test_data.iterrows()):\n",
    "        test_id = test_row['listing_id']\n",
    "        test_date = test_row['date']\n",
    "        \n",
    "        if test_id not in neighbor_dict:\n",
    "            continue  # Skip if no neighbors found\n",
    "        \n",
    "        # Get neighbors for this test listing\n",
    "        neighbors = neighbor_dict[test_id][:max_neighbors]\n",
    "        \n",
    "        # Process each neighbor\n",
    "        for n_idx, neighbor in enumerate(neighbors):\n",
    "            if n_idx >= max_neighbors:\n",
    "                break\n",
    "                \n",
    "            neighbor_id = neighbor['neighbor_id']\n",
    "            \n",
    "            # Only use neighbors from the training set\n",
    "            if neighbor_id in train_listings_dict:\n",
    "                neighbor_data = train_listings_dict[neighbor_id]\n",
    "                \n",
    "                # Extract price history\n",
    "                price_history = extract_price_history(neighbor_data, test_date, seq_length)\n",
    "                \n",
    "                # Store data\n",
    "                neighbor_histories[idx, n_idx] = price_history\n",
    "                neighbor_masks[idx, n_idx] = True\n",
    "    \n",
    "    print(f\"Prepared neighbor data for {test_size} test instances\")\n",
    "    return neighbor_histories, neighbor_masks\n",
    "\n",
    "def prepare_data_for_neighbor_lstm(train_data, test_data, neighbor_dict, property_features, \n",
    "                                 temporal_features, property_scaler=None, temporal_scaler=None, \n",
    "                                 target_scaler=None, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare data for the NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for NeighborBasedLSTM...\")\n",
    "    \n",
    "    # Initialize or use provided scalers\n",
    "    if property_scaler is None:\n",
    "        property_scaler = StandardScaler()\n",
    "        property_scaler.fit(train_data[property_features])\n",
    "    \n",
    "    if temporal_scaler is None:\n",
    "        temporal_scaler = StandardScaler()\n",
    "        temporal_scaler.fit(train_data[temporal_features])\n",
    "    \n",
    "    if target_scaler is None:\n",
    "        target_scaler = StandardScaler()\n",
    "        target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Scale property features\n",
    "    X_train_property = property_scaler.transform(train_data[property_features]).astype(np.float32)\n",
    "    X_test_property = property_scaler.transform(test_data[property_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale temporal features\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_test_temporal = temporal_scaler.transform(test_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    # Prepare neighbor histories for test data\n",
    "    neighbor_histories, neighbor_masks = prepare_neighbor_data_batch(\n",
    "        test_data, train_data, neighbor_dict, max_neighbors, seq_length\n",
    "    )\n",
    "    \n",
    "    # Scale the target variable\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    y_test = target_scaler.transform(test_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Create data objects\n",
    "    train_data_obj = Data(\n",
    "        property_features=torch.FloatTensor(X_train_property),\n",
    "        temporal_features=torch.FloatTensor(X_train_temporal),\n",
    "        y=torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    )\n",
    "    \n",
    "    test_data_obj = Data(\n",
    "        property_features=torch.FloatTensor(X_test_property),\n",
    "        temporal_features=torch.FloatTensor(X_test_temporal),\n",
    "        neighbor_histories=torch.FloatTensor(neighbor_histories),\n",
    "        neighbor_mask=torch.BoolTensor(neighbor_masks),\n",
    "        y=torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "    )\n",
    "    \n",
    "    return train_data_obj, test_data_obj, property_scaler, temporal_scaler, target_scaler\n",
    "\n",
    "def train_neighbor_lstm_model(train_data, val_data, neighbor_dict, property_features, temporal_features,\n",
    "                            max_neighbors=5, seq_length=30, lstm_hidden_dim=16, hidden_dim=64,\n",
    "                            epochs=50, lr=0.001, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Training NeighborBasedLSTM Model =====\")\n",
    "    print(f\"LSTM hidden dimension: {lstm_hidden_dim}, Max neighbors: {max_neighbors}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    _, val_data_obj, property_scaler, temporal_scaler, target_scaler = prepare_data_for_neighbor_lstm(\n",
    "        train_data, val_data, neighbor_dict, property_features, temporal_features,\n",
    "        max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Move validation data to device\n",
    "    val_data_obj = val_data_obj.to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = NeighborBasedLSTM(\n",
    "        input_dim=1,  # Single price feature\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        property_features_dim=len(property_features),\n",
    "        max_neighbors=max_neighbors,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(val_data_obj)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(out, val_data_obj.y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data_obj)\n",
    "            val_loss = criterion(val_out, val_data_obj.y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_data_obj.y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Store history\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory management\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, property_scaler, temporal_scaler, target_scaler, history\n",
    "\n",
    "def predict_with_neighbor_lstm(model, test_data, train_data, neighbor_dict, property_features, \n",
    "                             temporal_features, property_scaler, temporal_scaler, target_scaler,\n",
    "                             max_neighbors=5, seq_length=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Predicting with NeighborBasedLSTM Model =====\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    _, test_data_obj, _, _, _ = prepare_data_for_neighbor_lstm(\n",
    "        train_data, test_data, neighbor_dict, property_features, temporal_features,\n",
    "        property_scaler, temporal_scaler, target_scaler,\n",
    "        max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    test_data_obj = test_data_obj.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_data_obj)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # If log-transformed, apply inverse\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "    \n",
    "    return predictions_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create calculated features\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def run_neighbor_lstm_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, neighbor_csv_path,\n",
    "                                          output_dir=None, window_size=35, n_splits=5,\n",
    "                                          max_neighbors=5, seq_length=30, lstm_hidden_dim=8,\n",
    "                                          hidden_dim=64, epochs=50, lr=0.001, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run NeighborBasedLSTM model with rolling window cross-validation\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        \n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        # Load neighbor data\n",
    "        neighbor_dict = load_neighbor_data(neighbor_csv_path)\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.8), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.2), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Filter data to include only dates in the desired range\n",
    "        start_date = pd.to_datetime('2023-07-08')\n",
    "        end_date = pd.to_datetime('2024-02-08')\n",
    "        train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "        \n",
    "        # Apply log transformation to price\n",
    "        train_data = apply_price_transformation(train_data)\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Define feature groups\n",
    "        property_features = [\n",
    "            'latitude', 'longitude', 'accommodates', 'bedrooms', 'bathrooms',\n",
    "            'amenity_count', 'luxury_score', 'essential_score'\n",
    "        ]\n",
    "        \n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        property_features = [f for f in property_features if f in train_data.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "        \n",
    "        print(f\"Using {len(property_features)} property features and {len(temporal_features)} temporal features\")\n",
    "        \n",
    "        # Get unique dates and ensure they're properly sorted\n",
    "        unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "        \n",
    "        # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "        last_35_days = unique_dates[-window_size:]\n",
    "        \n",
    "        # Define explicit test periods - each 7 days\n",
    "        test_periods = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = i * (window_size // n_splits)\n",
    "            end_idx = start_idx + (window_size // n_splits)\n",
    "            # Make sure we don't go beyond the available data\n",
    "            if end_idx <= len(last_35_days):\n",
    "                test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "        \n",
    "        # Adjust n_splits if we couldn't create enough test periods\n",
    "        n_splits = len(test_periods)\n",
    "        \n",
    "        print(f\"Created {n_splits} test periods:\")\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Storage for results\n",
    "        cv_results = []\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        split_metrics = []\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Run time series cross-validation using our explicit test periods\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "            \n",
    "            # Define training period: everything before test_start\n",
    "            train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "            train_end_date = train_end.date()\n",
    "            \n",
    "            print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "            print(f\"Testing period: {test_start} to {test_end}\")\n",
    "            \n",
    "            # Split by date first\n",
    "            train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "            test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "            \n",
    "            date_filtered_train = train_data[train_date_mask]\n",
    "            date_filtered_test = train_data[test_date_mask]\n",
    "            \n",
    "            # Now further split by listing IDs\n",
    "            train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "            test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "            \n",
    "            split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "            split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "            \n",
    "            print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "            print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "            \n",
    "            # Check if we have enough data for this split\n",
    "            if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "                print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Split train data into train and validation\n",
    "            unique_train_listings = split_train_data['listing_id'].unique()\n",
    "            train_listings, val_listings = train_test_split(\n",
    "                unique_train_listings, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "            val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "            \n",
    "            # Manage memory before training\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train model for this split\n",
    "            try:\n",
    "                print(f\"\\n----- Training NeighborBasedLSTM Model (Split {i+1}) -----\")\n",
    "                model, property_scaler, temporal_scaler, target_scaler, _ = train_neighbor_lstm_model(\n",
    "                    train_subset, val_subset, neighbor_dict, property_features, temporal_features,\n",
    "                    max_neighbors=max_neighbors, seq_length=seq_length, lstm_hidden_dim=lstm_hidden_dim,\n",
    "                    hidden_dim=hidden_dim, epochs=epochs, lr=lr, device=device\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test data\n",
    "                print(f\"\\n----- Evaluating NeighborBasedLSTM on Test Data (Split {i+1}) -----\")\n",
    "                test_predictions = predict_with_neighbor_lstm(\n",
    "                    model, split_test_data, train_subset, neighbor_dict, property_features, temporal_features,\n",
    "                    property_scaler, temporal_scaler, target_scaler,\n",
    "                    max_neighbors=max_neighbors, seq_length=seq_length, device=device\n",
    "                )\n",
    "                \n",
    "                # Get actual test values (original scale)\n",
    "                test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "                \n",
    "                # Evaluate predictions\n",
    "                metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "                \n",
    "                print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "                \n",
    "                # Store results for this split\n",
    "                split_results = pd.DataFrame({\n",
    "                    'split': i,\n",
    "                    'date': split_test_data['date'],\n",
    "                    'listing_id': split_test_data['listing_id'],\n",
    "                    'price': test_actuals,\n",
    "                    'predicted': test_predictions.flatten(),\n",
    "                    'error': test_actuals - test_predictions.flatten(),\n",
    "                    'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                    'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "                })\n",
    "                \n",
    "                cv_results.append(split_results)\n",
    "                all_predictions.extend(test_predictions.flatten())\n",
    "                all_targets.extend(test_actuals)\n",
    "                \n",
    "                # Save model for this split if output_dir is provided\n",
    "                if output_dir:\n",
    "                    model_path = os.path.join(output_dir, f'neighbor_lstm_model_split_{i+1}.pt')\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "                \n",
    "                # Store metrics for this split\n",
    "                split_metrics.append({\n",
    "                    'split': i,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mae': metrics['mae'],\n",
    "                    'r2': metrics['r2'],\n",
    "                    'mape': metrics['mape'],\n",
    "                    'n_samples': len(test_actuals)\n",
    "                })\n",
    "                \n",
    "                # Memory management after each split\n",
    "                del model, property_scaler, temporal_scaler, target_scaler\n",
    "                del test_predictions, split_train_data, split_test_data, train_subset, val_subset\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in split {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if not cv_results:\n",
    "            print(\"No valid splits completed. Check your data and parameters.\")\n",
    "            return None\n",
    "                \n",
    "        all_results = pd.concat(cv_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        all_targets_array = np.array(all_targets)\n",
    "        all_predictions_array = np.array(all_predictions)\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "            'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "            'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "            'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "        }\n",
    "        \n",
    "        # Calculate daily metrics\n",
    "        all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        daily_metrics = []\n",
    "        for day, group in all_results.groupby('date_str'):\n",
    "            y_true_day = group['price']\n",
    "            y_pred_day = group['predicted']\n",
    "            \n",
    "            daily_metrics.append({\n",
    "                'date': day,\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "                'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "                'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "                'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "                'n_samples': len(y_true_day)\n",
    "            })\n",
    "        \n",
    "        daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "        daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "        daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "        \n",
    "        split_metrics_df = pd.DataFrame(split_metrics)\n",
    "        \n",
    "        # Create a results dictionary\n",
    "        evaluation_results = {\n",
    "            'overall_metrics': overall_metrics,\n",
    "            'split_metrics': split_metrics_df,\n",
    "            'daily_metrics': daily_metrics_df,\n",
    "            'all_results': all_results,\n",
    "            'train_listings': len(train_listing_ids),\n",
    "            'test_listings': len(test_listing_ids)\n",
    "        }\n",
    "        \n",
    "        # Save results if output directory is provided\n",
    "        if output_dir:\n",
    "            # Save all results\n",
    "            results_file = os.path.join(output_dir, 'neighbor_lstm_rolling_window_results.csv')\n",
    "            all_results.to_csv(results_file, index=False)\n",
    "            print(f\"Results saved to {results_file}\")\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics_file = os.path.join(output_dir, 'neighbor_lstm_rolling_window_metrics.csv')\n",
    "            daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "            print(f\"Daily metrics saved to {metrics_file}\")\n",
    "            \n",
    "            # Save summary\n",
    "            with open(os.path.join(output_dir, 'neighbor_lstm_cv_summary.txt'), 'w') as f:\n",
    "                f.write(f\"NeighborBasedLSTM Rolling Window CV Model Summary\\n\")\n",
    "                f.write(f\"=================================\\n\\n\")\n",
    "                f.write(f\"Window size: {window_size} days\\n\")\n",
    "                f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "                f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "                f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "                f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "                f.write(f\"LSTM hidden dimension: {lstm_hidden_dim}\\n\")\n",
    "                f.write(f\"Maximum neighbors per listing: {max_neighbors}\\n\\n\")\n",
    "                f.write(f\"Overall Metrics:\\n\")\n",
    "                for k, v in overall_metrics.items():\n",
    "                    f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== NeighborBasedLSTM ROLLING WINDOW CV SUMMARY =====\")\n",
    "        print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "        \n",
    "        print(\"\\n=== Overall Metrics ===\")\n",
    "        print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "        print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "        \n",
    "        print(\"\\n=== Split Performance ===\")\n",
    "        print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "        \n",
    "        # Return evaluation results\n",
    "        return evaluation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in rolling window CV: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_neighbor_lstm_model(train_path, train_ids_path, test_ids_path, neighbor_csv_path, \n",
    "                          output_dir=None, lstm_hidden_dim=8, hidden_dim=64, max_neighbors=5,\n",
    "                          seq_length=30, epochs=50, lr=0.001, sample_size=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate the NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if not exists\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        \n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # Load neighbor data\n",
    "        neighbor_dict = load_neighbor_data(neighbor_csv_path)\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.8), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.2), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime if needed\n",
    "        if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "            train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "        else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Split data into train and test based on listing IDs\n",
    "        train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "        test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        train_df = train_data[train_mask].copy()\n",
    "        test_df = train_data[test_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Define feature groups\n",
    "        property_features = [\n",
    "            'latitude', 'longitude', 'accommodates', 'bedrooms', 'bathrooms',\n",
    "            'amenity_count', 'luxury_score', 'essential_score', 'bedroom_ratio'\n",
    "        ]\n",
    "        \n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        property_features = [f for f in property_features if f in train_df.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "        \n",
    "        print(f\"Using {len(property_features)} property features and {len(temporal_features)} temporal features\")\n",
    "        \n",
    "        # Apply log transformation to prices\n",
    "        train_df = apply_price_transformation(train_df)\n",
    "        test_df = apply_price_transformation(test_df)\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = train_df['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "        print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Memory management before training\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train NeighborBasedLSTM model\n",
    "        print(\"\\n===== Training NeighborBasedLSTM Model =====\")\n",
    "        model, property_scaler, temporal_scaler, target_scaler, history = train_neighbor_lstm_model(\n",
    "            train_subset, val_subset, neighbor_dict, property_features, temporal_features,\n",
    "            max_neighbors=max_neighbors, seq_length=seq_length, lstm_hidden_dim=lstm_hidden_dim,\n",
    "            hidden_dim=hidden_dim, epochs=epochs, lr=lr, device=device\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation RMSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['val_rmse'], label='Validation RMSE')\n",
    "        plt.title('Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        \n",
    "        # Plot validation MAE\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        # Plot learning rate\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['lr'], label='Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('LR')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'neighbor_lstm_training_history.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        print(\"\\n===== Evaluating NeighborBasedLSTM on Test Data =====\")\n",
    "        test_predictions = predict_with_neighbor_lstm(\n",
    "            model, test_df, train_subset, neighbor_dict, property_features, temporal_features,\n",
    "            property_scaler, temporal_scaler, target_scaler,\n",
    "            max_neighbors=max_neighbors, seq_length=seq_length, device=device\n",
    "        )\n",
    "        \n",
    "        # Get actual test values (original scale)\n",
    "        test_actuals = test_df['original_price'].values if 'original_price' in test_df.columns else test_df['price'].values\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        test_metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_gnn_results(test_actuals, test_predictions.flatten(), history, output_dir)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        if output_dir:\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'neighbor_lstm_model.pt'))\n",
    "            torch.save({\n",
    "                'property_scaler': property_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'max_neighbors': max_neighbors,\n",
    "                'seq_length': seq_length,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim\n",
    "            }, os.path.join(output_dir, 'neighbor_lstm_scalers.pt'))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "            # Save test predictions\n",
    "            test_results = pd.DataFrame({\n",
    "                'listing_id': test_df['listing_id'].values,\n",
    "                'date': test_df['date'].values,\n",
    "                'actual': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            test_results.to_csv(os.path.join(output_dir, 'neighbor_lstm_test_predictions.csv'), index=False)\n",
    "            print(f\"Test predictions saved to {os.path.join(output_dir, 'neighbor_lstm_test_predictions.csv')}\")\n",
    "        \n",
    "        # Return model and metrics\n",
    "        return model, property_scaler, temporal_scaler, target_scaler, test_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in NeighborBasedLSTM model training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def compare_lstm_dimensions(train_path, train_ids_path, test_ids_path, neighbor_csv_path,\n",
    "                          output_dir=None, max_neighbors=5, seq_length=30, sample_size=None):\n",
    "    \"\"\"\n",
    "    Compare LSTM models with different hidden dimensions (8 vs 16)\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set parameters for each model\n",
    "    lstm_hidden_dim_1 = 8   # First LSTM hidden dimension\n",
    "    lstm_hidden_dim_2 = 16  # Second LSTM hidden dimension\n",
    "    \n",
    "    # Run first model\n",
    "    print(\"\\n===== Running NeighborBasedLSTM Model (8 hidden dim) =====\")\n",
    "    result_1 = run_neighbor_lstm_model(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        neighbor_csv_path=neighbor_csv_path,\n",
    "        output_dir=os.path.join(output_dir, 'lstm_dim_8') if output_dir else None,\n",
    "        lstm_hidden_dim=lstm_hidden_dim_1,\n",
    "        max_neighbors=max_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    # Run second model\n",
    "    print(\"\\n===== Running NeighborBasedLSTM Model (16 hidden dim) =====\")\n",
    "    result_2 = run_neighbor_lstm_model(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        neighbor_csv_path=neighbor_csv_path,\n",
    "        output_dir=os.path.join(output_dir, 'lstm_dim_16') if output_dir else None,\n",
    "        lstm_hidden_dim=lstm_hidden_dim_2,\n",
    "        max_neighbors=max_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    # Check if both models ran successfully\n",
    "    if result_1 and result_2:\n",
    "        # Extract model performance\n",
    "        _, _, _, _, metrics_1 = result_1\n",
    "        _, _, _, _, metrics_2 = result_2\n",
    "        \n",
    "        # Compare metrics\n",
    "        print(\"\\n===== Model Comparison =====\")\n",
    "        metrics = ['rmse', 'mae', 'r2', 'mape']\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Metric': metrics,\n",
    "            f'LSTM (dim={lstm_hidden_dim_1})': [metrics_1[m] for m in metrics],\n",
    "            f'LSTM (dim={lstm_hidden_dim_2})': [metrics_2[m] for m in metrics]\n",
    "        })\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        comparison_df['Improvement %'] = [\n",
    "            ((metrics_2[m] - metrics_1[m]) / metrics_1[m] * 100) if m not in ['r2'] else\n",
    "            ((metrics_2[m] - metrics_1[m]) * 100) for m in metrics\n",
    "        ]\n",
    "        \n",
    "        print(comparison_df)\n",
    "        \n",
    "        # Save comparison if output_dir is provided\n",
    "        if output_dir:\n",
    "            comparison_df.to_csv(os.path.join(output_dir, 'lstm_dim_comparison.csv'), index=False)\n",
    "            print(f\"Model comparison saved to {os.path.join(output_dir, 'lstm_dim_comparison.csv')}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    else:\n",
    "        print(\"One or both models failed to run. Check error logs.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loading neighbor data from ./neighbor_data/neighbor_dict.csv\n",
      "Loaded neighbor data for 1573 test listings\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Limiting to 4500 random listings for testing\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Using 8 property features and 5 temporal features\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 517955 rows, 3600 unique listings\n",
      "Test data: 6300 rows, 900 unique listings\n",
      "\n",
      "----- Training NeighborBasedLSTM Model (Split 1) -----\n",
      "\n",
      "===== Training NeighborBasedLSTM Model =====\n",
      "LSTM hidden dimension: 8, Max neighbors: 5\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 103452 test instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5953, Val Loss: 0.4114, RMSE: 186.47, MAE: 116.30\n",
      "Epoch 2/125 - Loss: 0.5187, Val Loss: 0.4062, RMSE: 185.67, MAE: 115.57\n",
      "Epoch 3/125 - Loss: 0.4644, Val Loss: 0.3998, RMSE: 184.77, MAE: 114.66\n",
      "Epoch 4/125 - Loss: 0.4289, Val Loss: 0.3923, RMSE: 183.74, MAE: 113.59\n",
      "Epoch 5/125 - Loss: 0.4016, Val Loss: 0.3841, RMSE: 182.57, MAE: 112.40\n",
      "Epoch 6/125 - Loss: 0.3853, Val Loss: 0.3755, RMSE: 181.30, MAE: 111.14\n",
      "Epoch 7/125 - Loss: 0.3711, Val Loss: 0.3666, RMSE: 179.92, MAE: 109.85\n",
      "Epoch 8/125 - Loss: 0.3596, Val Loss: 0.3577, RMSE: 178.45, MAE: 108.52\n",
      "Epoch 9/125 - Loss: 0.3497, Val Loss: 0.3489, RMSE: 176.92, MAE: 107.19\n",
      "Epoch 10/125 - Loss: 0.3418, Val Loss: 0.3400, RMSE: 175.26, MAE: 105.83\n",
      "Epoch 11/125 - Loss: 0.3335, Val Loss: 0.3312, RMSE: 173.50, MAE: 104.42\n",
      "Epoch 12/125 - Loss: 0.3268, Val Loss: 0.3223, RMSE: 171.61, MAE: 102.96\n",
      "Epoch 13/125 - Loss: 0.3203, Val Loss: 0.3132, RMSE: 169.56, MAE: 101.41\n",
      "Epoch 14/125 - Loss: 0.3159, Val Loss: 0.3038, RMSE: 167.29, MAE: 99.71\n",
      "Epoch 15/125 - Loss: 0.3102, Val Loss: 0.2942, RMSE: 164.83, MAE: 97.90\n",
      "Epoch 16/125 - Loss: 0.3071, Val Loss: 0.2847, RMSE: 162.20, MAE: 95.99\n",
      "Epoch 17/125 - Loss: 0.3013, Val Loss: 0.2753, RMSE: 159.47, MAE: 94.00\n",
      "Epoch 18/125 - Loss: 0.2986, Val Loss: 0.2665, RMSE: 156.71, MAE: 92.05\n",
      "Epoch 19/125 - Loss: 0.2942, Val Loss: 0.2583, RMSE: 154.00, MAE: 90.23\n",
      "Epoch 20/125 - Loss: 0.2892, Val Loss: 0.2508, RMSE: 151.42, MAE: 88.55\n",
      "Epoch 21/125 - Loss: 0.2874, Val Loss: 0.2442, RMSE: 149.08, MAE: 87.05\n",
      "Epoch 22/125 - Loss: 0.2833, Val Loss: 0.2384, RMSE: 147.01, MAE: 85.74\n",
      "Epoch 23/125 - Loss: 0.2787, Val Loss: 0.2334, RMSE: 145.20, MAE: 84.58\n",
      "Epoch 24/125 - Loss: 0.2771, Val Loss: 0.2289, RMSE: 143.57, MAE: 83.55\n",
      "Epoch 25/125 - Loss: 0.2753, Val Loss: 0.2249, RMSE: 142.10, MAE: 82.62\n",
      "Epoch 26/125 - Loss: 0.2723, Val Loss: 0.2212, RMSE: 140.75, MAE: 81.77\n",
      "Epoch 27/125 - Loss: 0.2694, Val Loss: 0.2178, RMSE: 139.48, MAE: 80.99\n",
      "Epoch 28/125 - Loss: 0.2678, Val Loss: 0.2146, RMSE: 138.25, MAE: 80.27\n",
      "Epoch 29/125 - Loss: 0.2647, Val Loss: 0.2117, RMSE: 137.09, MAE: 79.61\n",
      "Epoch 30/125 - Loss: 0.2631, Val Loss: 0.2090, RMSE: 136.05, MAE: 79.04\n",
      "Epoch 31/125 - Loss: 0.2599, Val Loss: 0.2065, RMSE: 135.12, MAE: 78.52\n",
      "Epoch 32/125 - Loss: 0.2590, Val Loss: 0.2042, RMSE: 134.28, MAE: 78.05\n",
      "Epoch 33/125 - Loss: 0.2581, Val Loss: 0.2021, RMSE: 133.56, MAE: 77.63\n",
      "Epoch 34/125 - Loss: 0.2550, Val Loss: 0.2003, RMSE: 132.93, MAE: 77.26\n",
      "Epoch 35/125 - Loss: 0.2535, Val Loss: 0.1986, RMSE: 132.39, MAE: 76.91\n",
      "Epoch 36/125 - Loss: 0.2514, Val Loss: 0.1970, RMSE: 131.94, MAE: 76.58\n",
      "Epoch 37/125 - Loss: 0.2517, Val Loss: 0.1956, RMSE: 131.57, MAE: 76.28\n",
      "Epoch 38/125 - Loss: 0.2488, Val Loss: 0.1943, RMSE: 131.27, MAE: 75.99\n",
      "Epoch 39/125 - Loss: 0.2481, Val Loss: 0.1931, RMSE: 130.98, MAE: 75.72\n",
      "Epoch 40/125 - Loss: 0.2450, Val Loss: 0.1919, RMSE: 130.66, MAE: 75.46\n",
      "Epoch 41/125 - Loss: 0.2436, Val Loss: 0.1907, RMSE: 130.34, MAE: 75.20\n",
      "Epoch 42/125 - Loss: 0.2431, Val Loss: 0.1895, RMSE: 130.01, MAE: 74.94\n",
      "Epoch 43/125 - Loss: 0.2408, Val Loss: 0.1884, RMSE: 129.69, MAE: 74.70\n",
      "Epoch 44/125 - Loss: 0.2399, Val Loss: 0.1873, RMSE: 129.37, MAE: 74.46\n",
      "Epoch 45/125 - Loss: 0.2375, Val Loss: 0.1862, RMSE: 129.03, MAE: 74.22\n",
      "Epoch 46/125 - Loss: 0.2380, Val Loss: 0.1852, RMSE: 128.71, MAE: 73.98\n",
      "Epoch 47/125 - Loss: 0.2360, Val Loss: 0.1843, RMSE: 128.43, MAE: 73.77\n",
      "Epoch 48/125 - Loss: 0.2351, Val Loss: 0.1834, RMSE: 128.16, MAE: 73.56\n",
      "Epoch 49/125 - Loss: 0.2333, Val Loss: 0.1825, RMSE: 127.90, MAE: 73.37\n",
      "Epoch 50/125 - Loss: 0.2326, Val Loss: 0.1817, RMSE: 127.69, MAE: 73.19\n",
      "Epoch 51/125 - Loss: 0.2315, Val Loss: 0.1810, RMSE: 127.49, MAE: 73.01\n",
      "Epoch 52/125 - Loss: 0.2303, Val Loss: 0.1802, RMSE: 127.32, MAE: 72.85\n",
      "Epoch 53/125 - Loss: 0.2286, Val Loss: 0.1795, RMSE: 127.15, MAE: 72.69\n",
      "Epoch 54/125 - Loss: 0.2288, Val Loss: 0.1789, RMSE: 127.04, MAE: 72.55\n",
      "Epoch 55/125 - Loss: 0.2274, Val Loss: 0.1782, RMSE: 126.96, MAE: 72.41\n",
      "Epoch 56/125 - Loss: 0.2247, Val Loss: 0.1776, RMSE: 126.84, MAE: 72.27\n",
      "Epoch 57/125 - Loss: 0.2251, Val Loss: 0.1770, RMSE: 126.72, MAE: 72.13\n",
      "Epoch 58/125 - Loss: 0.2234, Val Loss: 0.1763, RMSE: 126.52, MAE: 71.98\n",
      "Epoch 59/125 - Loss: 0.2233, Val Loss: 0.1756, RMSE: 126.30, MAE: 71.81\n",
      "Epoch 60/125 - Loss: 0.2219, Val Loss: 0.1749, RMSE: 126.05, MAE: 71.65\n",
      "Epoch 61/125 - Loss: 0.2202, Val Loss: 0.1741, RMSE: 125.83, MAE: 71.48\n",
      "Epoch 62/125 - Loss: 0.2194, Val Loss: 0.1734, RMSE: 125.61, MAE: 71.32\n",
      "Epoch 63/125 - Loss: 0.2192, Val Loss: 0.1728, RMSE: 125.39, MAE: 71.16\n",
      "Epoch 64/125 - Loss: 0.2165, Val Loss: 0.1721, RMSE: 125.19, MAE: 71.01\n",
      "Epoch 65/125 - Loss: 0.2174, Val Loss: 0.1714, RMSE: 125.00, MAE: 70.86\n",
      "Epoch 66/125 - Loss: 0.2162, Val Loss: 0.1708, RMSE: 124.84, MAE: 70.72\n",
      "Epoch 67/125 - Loss: 0.2151, Val Loss: 0.1702, RMSE: 124.69, MAE: 70.59\n",
      "Epoch 68/125 - Loss: 0.2139, Val Loss: 0.1696, RMSE: 124.57, MAE: 70.47\n",
      "Epoch 69/125 - Loss: 0.2137, Val Loss: 0.1690, RMSE: 124.46, MAE: 70.36\n",
      "Epoch 70/125 - Loss: 0.2123, Val Loss: 0.1684, RMSE: 124.32, MAE: 70.24\n",
      "Epoch 71/125 - Loss: 0.2108, Val Loss: 0.1678, RMSE: 124.17, MAE: 70.12\n",
      "Epoch 72/125 - Loss: 0.2096, Val Loss: 0.1672, RMSE: 123.97, MAE: 69.99\n",
      "Epoch 73/125 - Loss: 0.2092, Val Loss: 0.1666, RMSE: 123.74, MAE: 69.85\n",
      "Epoch 74/125 - Loss: 0.2090, Val Loss: 0.1660, RMSE: 123.51, MAE: 69.70\n",
      "Epoch 75/125 - Loss: 0.2079, Val Loss: 0.1653, RMSE: 123.29, MAE: 69.56\n",
      "Epoch 76/125 - Loss: 0.2068, Val Loss: 0.1647, RMSE: 123.07, MAE: 69.42\n",
      "Epoch 77/125 - Loss: 0.2056, Val Loss: 0.1641, RMSE: 122.85, MAE: 69.28\n",
      "Epoch 78/125 - Loss: 0.2056, Val Loss: 0.1634, RMSE: 122.65, MAE: 69.13\n",
      "Epoch 79/125 - Loss: 0.2064, Val Loss: 0.1628, RMSE: 122.48, MAE: 69.00\n",
      "Epoch 80/125 - Loss: 0.2039, Val Loss: 0.1623, RMSE: 122.32, MAE: 68.87\n",
      "Epoch 81/125 - Loss: 0.2040, Val Loss: 0.1617, RMSE: 122.18, MAE: 68.74\n",
      "Epoch 82/125 - Loss: 0.2026, Val Loss: 0.1610, RMSE: 122.01, MAE: 68.60\n",
      "Epoch 83/125 - Loss: 0.2016, Val Loss: 0.1604, RMSE: 121.81, MAE: 68.46\n",
      "Epoch 84/125 - Loss: 0.2004, Val Loss: 0.1598, RMSE: 121.63, MAE: 68.31\n",
      "Epoch 85/125 - Loss: 0.2005, Val Loss: 0.1591, RMSE: 121.45, MAE: 68.17\n",
      "Epoch 86/125 - Loss: 0.1995, Val Loss: 0.1585, RMSE: 121.25, MAE: 68.02\n",
      "Epoch 87/125 - Loss: 0.1984, Val Loss: 0.1578, RMSE: 121.07, MAE: 67.88\n",
      "Epoch 88/125 - Loss: 0.1980, Val Loss: 0.1572, RMSE: 120.89, MAE: 67.74\n",
      "Epoch 89/125 - Loss: 0.1973, Val Loss: 0.1566, RMSE: 120.72, MAE: 67.60\n",
      "Epoch 90/125 - Loss: 0.1967, Val Loss: 0.1560, RMSE: 120.55, MAE: 67.46\n",
      "Epoch 91/125 - Loss: 0.1966, Val Loss: 0.1554, RMSE: 120.40, MAE: 67.33\n",
      "Epoch 92/125 - Loss: 0.1956, Val Loss: 0.1548, RMSE: 120.27, MAE: 67.21\n",
      "Epoch 93/125 - Loss: 0.1950, Val Loss: 0.1543, RMSE: 120.13, MAE: 67.08\n",
      "Epoch 94/125 - Loss: 0.1943, Val Loss: 0.1537, RMSE: 120.02, MAE: 66.96\n",
      "Epoch 95/125 - Loss: 0.1938, Val Loss: 0.1532, RMSE: 119.89, MAE: 66.84\n",
      "Epoch 96/125 - Loss: 0.1927, Val Loss: 0.1526, RMSE: 119.73, MAE: 66.71\n",
      "Epoch 97/125 - Loss: 0.1922, Val Loss: 0.1520, RMSE: 119.56, MAE: 66.58\n",
      "Epoch 98/125 - Loss: 0.1906, Val Loss: 0.1514, RMSE: 119.36, MAE: 66.44\n",
      "Epoch 99/125 - Loss: 0.1905, Val Loss: 0.1508, RMSE: 119.14, MAE: 66.29\n",
      "Epoch 100/125 - Loss: 0.1895, Val Loss: 0.1502, RMSE: 118.87, MAE: 66.13\n",
      "Epoch 101/125 - Loss: 0.1897, Val Loss: 0.1495, RMSE: 118.62, MAE: 65.98\n",
      "Epoch 102/125 - Loss: 0.1893, Val Loss: 0.1489, RMSE: 118.36, MAE: 65.83\n",
      "Epoch 103/125 - Loss: 0.1887, Val Loss: 0.1483, RMSE: 118.14, MAE: 65.68\n",
      "Epoch 104/125 - Loss: 0.1867, Val Loss: 0.1477, RMSE: 117.95, MAE: 65.55\n",
      "Epoch 105/125 - Loss: 0.1870, Val Loss: 0.1471, RMSE: 117.78, MAE: 65.42\n",
      "Epoch 106/125 - Loss: 0.1868, Val Loss: 0.1465, RMSE: 117.64, MAE: 65.29\n",
      "Epoch 107/125 - Loss: 0.1863, Val Loss: 0.1460, RMSE: 117.50, MAE: 65.17\n",
      "Epoch 108/125 - Loss: 0.1858, Val Loss: 0.1454, RMSE: 117.36, MAE: 65.06\n",
      "Epoch 109/125 - Loss: 0.1843, Val Loss: 0.1449, RMSE: 117.21, MAE: 64.93\n",
      "Epoch 110/125 - Loss: 0.1841, Val Loss: 0.1443, RMSE: 117.02, MAE: 64.80\n",
      "Epoch 111/125 - Loss: 0.1835, Val Loss: 0.1438, RMSE: 116.84, MAE: 64.66\n",
      "Epoch 112/125 - Loss: 0.1827, Val Loss: 0.1432, RMSE: 116.65, MAE: 64.51\n",
      "Epoch 113/125 - Loss: 0.1822, Val Loss: 0.1426, RMSE: 116.48, MAE: 64.37\n",
      "Epoch 114/125 - Loss: 0.1816, Val Loss: 0.1420, RMSE: 116.32, MAE: 64.24\n",
      "Epoch 115/125 - Loss: 0.1807, Val Loss: 0.1415, RMSE: 116.15, MAE: 64.11\n",
      "Epoch 116/125 - Loss: 0.1801, Val Loss: 0.1409, RMSE: 115.96, MAE: 63.96\n",
      "Epoch 117/125 - Loss: 0.1801, Val Loss: 0.1404, RMSE: 115.79, MAE: 63.83\n",
      "Epoch 118/125 - Loss: 0.1795, Val Loss: 0.1398, RMSE: 115.64, MAE: 63.70\n",
      "Epoch 119/125 - Loss: 0.1792, Val Loss: 0.1393, RMSE: 115.54, MAE: 63.58\n",
      "Epoch 120/125 - Loss: 0.1786, Val Loss: 0.1387, RMSE: 115.42, MAE: 63.46\n",
      "Epoch 121/125 - Loss: 0.1783, Val Loss: 0.1382, RMSE: 115.31, MAE: 63.35\n",
      "Epoch 122/125 - Loss: 0.1771, Val Loss: 0.1376, RMSE: 115.17, MAE: 63.22\n",
      "Epoch 123/125 - Loss: 0.1768, Val Loss: 0.1371, RMSE: 114.99, MAE: 63.09\n",
      "Epoch 124/125 - Loss: 0.1767, Val Loss: 0.1365, RMSE: 114.81, MAE: 62.95\n",
      "Epoch 125/125 - Loss: 0.1751, Val Loss: 0.1359, RMSE: 114.58, MAE: 62.80\n",
      "\n",
      "----- Evaluating NeighborBasedLSTM on Test Data (Split 1) -----\n",
      "\n",
      "===== Predicting with NeighborBasedLSTM Model =====\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 6300 test instances\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 119.55\n",
      "MAE: 68.96\n",
      "R²: 0.4382\n",
      "MAPE: 37.55%\n",
      "Split 1 Results - RMSE: 119.5500, MAE: 68.9586, R²: 0.4382\n",
      "Model for split 1 saved to ./output/neighbor_lstm_model/neighbor_lstm_model_split_1.pt\n",
      "\n",
      "===== Split 2/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 543155 rows, 3600 unique listings\n",
      "Test data: 6300 rows, 900 unique listings\n",
      "\n",
      "----- Training NeighborBasedLSTM Model (Split 2) -----\n",
      "\n",
      "===== Training NeighborBasedLSTM Model =====\n",
      "LSTM hidden dimension: 8, Max neighbors: 5\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 108492 test instances\n",
      "Epoch 1/125 - Loss: 0.5135, Val Loss: 0.4032, RMSE: 183.64, MAE: 114.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/125 - Loss: 0.4508, Val Loss: 0.3976, RMSE: 183.40, MAE: 113.55\n",
      "Epoch 3/125 - Loss: 0.4064, Val Loss: 0.3913, RMSE: 182.96, MAE: 112.71\n",
      "Epoch 4/125 - Loss: 0.3781, Val Loss: 0.3842, RMSE: 182.29, MAE: 111.77\n",
      "Epoch 5/125 - Loss: 0.3596, Val Loss: 0.3765, RMSE: 181.40, MAE: 110.72\n",
      "Epoch 6/125 - Loss: 0.3452, Val Loss: 0.3682, RMSE: 180.28, MAE: 109.57\n",
      "Epoch 7/125 - Loss: 0.3346, Val Loss: 0.3596, RMSE: 178.98, MAE: 108.35\n",
      "Epoch 8/125 - Loss: 0.3252, Val Loss: 0.3509, RMSE: 177.51, MAE: 107.08\n",
      "Epoch 9/125 - Loss: 0.3187, Val Loss: 0.3424, RMSE: 175.94, MAE: 105.79\n",
      "Epoch 10/125 - Loss: 0.3104, Val Loss: 0.3342, RMSE: 174.28, MAE: 104.50\n",
      "Epoch 11/125 - Loss: 0.3044, Val Loss: 0.3262, RMSE: 172.56, MAE: 103.20\n",
      "Epoch 12/125 - Loss: 0.2991, Val Loss: 0.3184, RMSE: 170.76, MAE: 101.87\n",
      "Epoch 13/125 - Loss: 0.2946, Val Loss: 0.3107, RMSE: 168.88, MAE: 100.51\n",
      "Epoch 14/125 - Loss: 0.2894, Val Loss: 0.3028, RMSE: 166.89, MAE: 99.08\n",
      "Epoch 15/125 - Loss: 0.2853, Val Loss: 0.2946, RMSE: 164.79, MAE: 97.56\n",
      "Epoch 16/125 - Loss: 0.2809, Val Loss: 0.2861, RMSE: 162.55, MAE: 95.93\n",
      "Epoch 17/125 - Loss: 0.2784, Val Loss: 0.2773, RMSE: 160.15, MAE: 94.18\n",
      "Epoch 18/125 - Loss: 0.2733, Val Loss: 0.2684, RMSE: 157.64, MAE: 92.34\n",
      "Epoch 19/125 - Loss: 0.2702, Val Loss: 0.2595, RMSE: 155.03, MAE: 90.45\n",
      "Epoch 20/125 - Loss: 0.2691, Val Loss: 0.2509, RMSE: 152.38, MAE: 88.55\n",
      "Epoch 21/125 - Loss: 0.2645, Val Loss: 0.2428, RMSE: 149.77, MAE: 86.72\n",
      "Epoch 22/125 - Loss: 0.2627, Val Loss: 0.2355, RMSE: 147.29, MAE: 85.00\n",
      "Epoch 23/125 - Loss: 0.2596, Val Loss: 0.2291, RMSE: 145.01, MAE: 83.44\n",
      "Epoch 24/125 - Loss: 0.2580, Val Loss: 0.2235, RMSE: 142.95, MAE: 82.05\n",
      "Epoch 25/125 - Loss: 0.2561, Val Loss: 0.2189, RMSE: 141.18, MAE: 80.86\n",
      "Epoch 26/125 - Loss: 0.2526, Val Loss: 0.2150, RMSE: 139.67, MAE: 79.86\n",
      "Epoch 27/125 - Loss: 0.2513, Val Loss: 0.2117, RMSE: 138.37, MAE: 79.00\n",
      "Epoch 28/125 - Loss: 0.2489, Val Loss: 0.2088, RMSE: 137.21, MAE: 78.25\n",
      "Epoch 29/125 - Loss: 0.2472, Val Loss: 0.2062, RMSE: 136.19, MAE: 77.58\n",
      "Epoch 30/125 - Loss: 0.2457, Val Loss: 0.2039, RMSE: 135.24, MAE: 76.97\n",
      "Epoch 31/125 - Loss: 0.2443, Val Loss: 0.2016, RMSE: 134.33, MAE: 76.40\n",
      "Epoch 32/125 - Loss: 0.2429, Val Loss: 0.1995, RMSE: 133.47, MAE: 75.86\n",
      "Epoch 33/125 - Loss: 0.2400, Val Loss: 0.1974, RMSE: 132.61, MAE: 75.36\n",
      "Epoch 34/125 - Loss: 0.2410, Val Loss: 0.1953, RMSE: 131.77, MAE: 74.89\n",
      "Epoch 35/125 - Loss: 0.2374, Val Loss: 0.1934, RMSE: 130.95, MAE: 74.45\n",
      "Epoch 36/125 - Loss: 0.2374, Val Loss: 0.1916, RMSE: 130.15, MAE: 74.05\n",
      "Epoch 37/125 - Loss: 0.2356, Val Loss: 0.1898, RMSE: 129.41, MAE: 73.68\n",
      "Epoch 38/125 - Loss: 0.2338, Val Loss: 0.1883, RMSE: 128.73, MAE: 73.35\n",
      "Epoch 39/125 - Loss: 0.2328, Val Loss: 0.1869, RMSE: 128.13, MAE: 73.06\n",
      "Epoch 40/125 - Loss: 0.2312, Val Loss: 0.1856, RMSE: 127.60, MAE: 72.81\n",
      "Epoch 41/125 - Loss: 0.2296, Val Loss: 0.1844, RMSE: 127.14, MAE: 72.58\n",
      "Epoch 42/125 - Loss: 0.2290, Val Loss: 0.1834, RMSE: 126.73, MAE: 72.38\n",
      "Epoch 43/125 - Loss: 0.2282, Val Loss: 0.1825, RMSE: 126.38, MAE: 72.20\n",
      "Epoch 44/125 - Loss: 0.2265, Val Loss: 0.1816, RMSE: 126.08, MAE: 72.03\n",
      "Epoch 45/125 - Loss: 0.2253, Val Loss: 0.1808, RMSE: 125.83, MAE: 71.88\n",
      "Epoch 46/125 - Loss: 0.2247, Val Loss: 0.1801, RMSE: 125.62, MAE: 71.73\n",
      "Epoch 47/125 - Loss: 0.2235, Val Loss: 0.1793, RMSE: 125.42, MAE: 71.59\n",
      "Epoch 48/125 - Loss: 0.2216, Val Loss: 0.1786, RMSE: 125.22, MAE: 71.44\n",
      "Epoch 49/125 - Loss: 0.2213, Val Loss: 0.1778, RMSE: 125.03, MAE: 71.29\n",
      "Epoch 50/125 - Loss: 0.2207, Val Loss: 0.1771, RMSE: 124.82, MAE: 71.13\n",
      "Epoch 51/125 - Loss: 0.2207, Val Loss: 0.1763, RMSE: 124.62, MAE: 70.98\n",
      "Epoch 52/125 - Loss: 0.2183, Val Loss: 0.1756, RMSE: 124.42, MAE: 70.83\n",
      "Epoch 53/125 - Loss: 0.2175, Val Loss: 0.1748, RMSE: 124.22, MAE: 70.68\n",
      "Epoch 54/125 - Loss: 0.2170, Val Loss: 0.1741, RMSE: 124.04, MAE: 70.54\n",
      "Epoch 55/125 - Loss: 0.2156, Val Loss: 0.1735, RMSE: 123.87, MAE: 70.41\n",
      "Epoch 56/125 - Loss: 0.2147, Val Loss: 0.1728, RMSE: 123.72, MAE: 70.29\n",
      "Epoch 57/125 - Loss: 0.2143, Val Loss: 0.1722, RMSE: 123.59, MAE: 70.18\n",
      "Epoch 58/125 - Loss: 0.2136, Val Loss: 0.1716, RMSE: 123.46, MAE: 70.07\n",
      "Epoch 59/125 - Loss: 0.2124, Val Loss: 0.1711, RMSE: 123.33, MAE: 69.96\n",
      "Epoch 60/125 - Loss: 0.2111, Val Loss: 0.1705, RMSE: 123.20, MAE: 69.86\n",
      "Epoch 61/125 - Loss: 0.2115, Val Loss: 0.1700, RMSE: 123.08, MAE: 69.76\n",
      "Epoch 62/125 - Loss: 0.2109, Val Loss: 0.1694, RMSE: 122.95, MAE: 69.66\n",
      "Epoch 63/125 - Loss: 0.2096, Val Loss: 0.1689, RMSE: 122.84, MAE: 69.56\n",
      "Epoch 64/125 - Loss: 0.2094, Val Loss: 0.1683, RMSE: 122.72, MAE: 69.45\n",
      "Epoch 65/125 - Loss: 0.2083, Val Loss: 0.1678, RMSE: 122.58, MAE: 69.34\n",
      "Epoch 66/125 - Loss: 0.2067, Val Loss: 0.1672, RMSE: 122.44, MAE: 69.23\n",
      "Epoch 67/125 - Loss: 0.2069, Val Loss: 0.1667, RMSE: 122.29, MAE: 69.11\n",
      "Epoch 68/125 - Loss: 0.2065, Val Loss: 0.1661, RMSE: 122.14, MAE: 69.00\n",
      "Epoch 69/125 - Loss: 0.2055, Val Loss: 0.1656, RMSE: 121.99, MAE: 68.89\n",
      "Epoch 70/125 - Loss: 0.2050, Val Loss: 0.1651, RMSE: 121.86, MAE: 68.78\n",
      "Epoch 71/125 - Loss: 0.2041, Val Loss: 0.1646, RMSE: 121.74, MAE: 68.67\n",
      "Epoch 72/125 - Loss: 0.2036, Val Loss: 0.1641, RMSE: 121.59, MAE: 68.56\n",
      "Epoch 73/125 - Loss: 0.2028, Val Loss: 0.1636, RMSE: 121.44, MAE: 68.46\n",
      "Epoch 74/125 - Loss: 0.2019, Val Loss: 0.1631, RMSE: 121.30, MAE: 68.37\n",
      "Epoch 75/125 - Loss: 0.2012, Val Loss: 0.1626, RMSE: 121.14, MAE: 68.27\n",
      "Epoch 76/125 - Loss: 0.2012, Val Loss: 0.1622, RMSE: 121.01, MAE: 68.17\n",
      "Epoch 77/125 - Loss: 0.2003, Val Loss: 0.1617, RMSE: 120.89, MAE: 68.08\n",
      "Epoch 78/125 - Loss: 0.1997, Val Loss: 0.1613, RMSE: 120.76, MAE: 67.98\n",
      "Epoch 79/125 - Loss: 0.1986, Val Loss: 0.1608, RMSE: 120.63, MAE: 67.89\n",
      "Epoch 80/125 - Loss: 0.1990, Val Loss: 0.1603, RMSE: 120.51, MAE: 67.79\n",
      "Epoch 81/125 - Loss: 0.1975, Val Loss: 0.1598, RMSE: 120.37, MAE: 67.69\n",
      "Epoch 82/125 - Loss: 0.1975, Val Loss: 0.1594, RMSE: 120.24, MAE: 67.60\n",
      "Epoch 83/125 - Loss: 0.1963, Val Loss: 0.1589, RMSE: 120.14, MAE: 67.51\n",
      "Epoch 84/125 - Loss: 0.1972, Val Loss: 0.1585, RMSE: 120.04, MAE: 67.42\n",
      "Epoch 85/125 - Loss: 0.1960, Val Loss: 0.1581, RMSE: 119.97, MAE: 67.33\n",
      "Epoch 86/125 - Loss: 0.1954, Val Loss: 0.1576, RMSE: 119.88, MAE: 67.24\n",
      "Epoch 87/125 - Loss: 0.1943, Val Loss: 0.1572, RMSE: 119.78, MAE: 67.15\n",
      "Epoch 88/125 - Loss: 0.1945, Val Loss: 0.1568, RMSE: 119.68, MAE: 67.06\n",
      "Epoch 89/125 - Loss: 0.1937, Val Loss: 0.1563, RMSE: 119.58, MAE: 66.97\n",
      "Epoch 90/125 - Loss: 0.1939, Val Loss: 0.1559, RMSE: 119.47, MAE: 66.88\n",
      "Epoch 91/125 - Loss: 0.1928, Val Loss: 0.1554, RMSE: 119.33, MAE: 66.78\n",
      "Epoch 92/125 - Loss: 0.1933, Val Loss: 0.1550, RMSE: 119.21, MAE: 66.69\n",
      "Epoch 93/125 - Loss: 0.1918, Val Loss: 0.1546, RMSE: 119.09, MAE: 66.59\n",
      "Epoch 94/125 - Loss: 0.1908, Val Loss: 0.1541, RMSE: 118.96, MAE: 66.49\n",
      "Epoch 95/125 - Loss: 0.1903, Val Loss: 0.1536, RMSE: 118.82, MAE: 66.40\n",
      "Epoch 96/125 - Loss: 0.1908, Val Loss: 0.1532, RMSE: 118.70, MAE: 66.30\n",
      "Epoch 97/125 - Loss: 0.1897, Val Loss: 0.1527, RMSE: 118.59, MAE: 66.21\n",
      "Epoch 98/125 - Loss: 0.1903, Val Loss: 0.1523, RMSE: 118.51, MAE: 66.12\n",
      "Epoch 99/125 - Loss: 0.1886, Val Loss: 0.1518, RMSE: 118.43, MAE: 66.04\n",
      "Epoch 100/125 - Loss: 0.1874, Val Loss: 0.1514, RMSE: 118.35, MAE: 65.95\n",
      "Epoch 101/125 - Loss: 0.1883, Val Loss: 0.1509, RMSE: 118.26, MAE: 65.87\n",
      "Epoch 102/125 - Loss: 0.1874, Val Loss: 0.1505, RMSE: 118.17, MAE: 65.78\n",
      "Epoch 103/125 - Loss: 0.1874, Val Loss: 0.1500, RMSE: 118.06, MAE: 65.69\n",
      "Epoch 104/125 - Loss: 0.1861, Val Loss: 0.1496, RMSE: 117.96, MAE: 65.59\n",
      "Epoch 105/125 - Loss: 0.1859, Val Loss: 0.1491, RMSE: 117.84, MAE: 65.50\n",
      "Epoch 106/125 - Loss: 0.1858, Val Loss: 0.1486, RMSE: 117.73, MAE: 65.41\n",
      "Epoch 107/125 - Loss: 0.1856, Val Loss: 0.1482, RMSE: 117.59, MAE: 65.31\n",
      "Epoch 108/125 - Loss: 0.1848, Val Loss: 0.1477, RMSE: 117.43, MAE: 65.22\n",
      "Epoch 109/125 - Loss: 0.1842, Val Loss: 0.1473, RMSE: 117.31, MAE: 65.13\n",
      "Epoch 110/125 - Loss: 0.1843, Val Loss: 0.1469, RMSE: 117.18, MAE: 65.05\n",
      "Epoch 111/125 - Loss: 0.1832, Val Loss: 0.1465, RMSE: 117.09, MAE: 64.97\n",
      "Epoch 112/125 - Loss: 0.1830, Val Loss: 0.1461, RMSE: 117.02, MAE: 64.89\n",
      "Epoch 113/125 - Loss: 0.1823, Val Loss: 0.1457, RMSE: 116.95, MAE: 64.81\n",
      "Epoch 114/125 - Loss: 0.1827, Val Loss: 0.1453, RMSE: 116.87, MAE: 64.73\n",
      "Epoch 115/125 - Loss: 0.1822, Val Loss: 0.1449, RMSE: 116.78, MAE: 64.64\n",
      "Epoch 116/125 - Loss: 0.1805, Val Loss: 0.1444, RMSE: 116.65, MAE: 64.54\n",
      "Epoch 117/125 - Loss: 0.1810, Val Loss: 0.1439, RMSE: 116.48, MAE: 64.43\n",
      "Epoch 118/125 - Loss: 0.1802, Val Loss: 0.1433, RMSE: 116.30, MAE: 64.32\n",
      "Epoch 119/125 - Loss: 0.1799, Val Loss: 0.1428, RMSE: 116.12, MAE: 64.20\n",
      "Epoch 120/125 - Loss: 0.1795, Val Loss: 0.1422, RMSE: 115.97, MAE: 64.09\n",
      "Epoch 121/125 - Loss: 0.1796, Val Loss: 0.1417, RMSE: 115.79, MAE: 63.97\n",
      "Epoch 122/125 - Loss: 0.1793, Val Loss: 0.1412, RMSE: 115.63, MAE: 63.86\n",
      "Epoch 123/125 - Loss: 0.1789, Val Loss: 0.1408, RMSE: 115.53, MAE: 63.77\n",
      "Epoch 124/125 - Loss: 0.1785, Val Loss: 0.1404, RMSE: 115.44, MAE: 63.67\n",
      "Epoch 125/125 - Loss: 0.1774, Val Loss: 0.1400, RMSE: 115.35, MAE: 63.58\n",
      "\n",
      "----- Evaluating NeighborBasedLSTM on Test Data (Split 2) -----\n",
      "\n",
      "===== Predicting with NeighborBasedLSTM Model =====\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 6300 test instances\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 118.35\n",
      "MAE: 66.44\n",
      "R²: 0.4301\n",
      "MAPE: 35.56%\n",
      "Split 2 Results - RMSE: 118.3495, MAE: 66.4384, R²: 0.4301\n",
      "Model for split 2 saved to ./output/neighbor_lstm_model/neighbor_lstm_model_split_2.pt\n",
      "\n",
      "===== Split 3/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-18\n",
      "Testing period: 2024-01-19 to 2024-01-25\n",
      "Train data: 568355 rows, 3600 unique listings\n",
      "Test data: 6300 rows, 900 unique listings\n",
      "\n",
      "----- Training NeighborBasedLSTM Model (Split 3) -----\n",
      "\n",
      "===== Training NeighborBasedLSTM Model =====\n",
      "LSTM hidden dimension: 8, Max neighbors: 5\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 113532 test instances\n",
      "Epoch 1/125 - Loss: 0.6790, Val Loss: 0.4092, RMSE: 181.65, MAE: 114.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/125 - Loss: 0.5700, Val Loss: 0.3992, RMSE: 179.35, MAE: 112.94\n",
      "Epoch 3/125 - Loss: 0.4963, Val Loss: 0.3884, RMSE: 177.20, MAE: 111.27\n",
      "Epoch 4/125 - Loss: 0.4507, Val Loss: 0.3778, RMSE: 175.22, MAE: 109.59\n",
      "Epoch 5/125 - Loss: 0.4218, Val Loss: 0.3681, RMSE: 173.48, MAE: 108.04\n",
      "Epoch 6/125 - Loss: 0.4016, Val Loss: 0.3598, RMSE: 171.96, MAE: 106.68\n",
      "Epoch 7/125 - Loss: 0.3875, Val Loss: 0.3528, RMSE: 170.67, MAE: 105.53\n",
      "Epoch 8/125 - Loss: 0.3733, Val Loss: 0.3469, RMSE: 169.56, MAE: 104.56\n",
      "Epoch 9/125 - Loss: 0.3600, Val Loss: 0.3418, RMSE: 168.59, MAE: 103.71\n",
      "Epoch 10/125 - Loss: 0.3517, Val Loss: 0.3370, RMSE: 167.69, MAE: 102.92\n",
      "Epoch 11/125 - Loss: 0.3447, Val Loss: 0.3321, RMSE: 166.74, MAE: 102.11\n",
      "Epoch 12/125 - Loss: 0.3388, Val Loss: 0.3265, RMSE: 165.66, MAE: 101.18\n",
      "Epoch 13/125 - Loss: 0.3330, Val Loss: 0.3200, RMSE: 164.35, MAE: 100.08\n",
      "Epoch 14/125 - Loss: 0.3293, Val Loss: 0.3122, RMSE: 162.74, MAE: 98.74\n",
      "Epoch 15/125 - Loss: 0.3240, Val Loss: 0.3034, RMSE: 160.80, MAE: 97.17\n",
      "Epoch 16/125 - Loss: 0.3197, Val Loss: 0.2935, RMSE: 158.54, MAE: 95.38\n",
      "Epoch 17/125 - Loss: 0.3139, Val Loss: 0.2831, RMSE: 155.98, MAE: 93.40\n",
      "Epoch 18/125 - Loss: 0.3091, Val Loss: 0.2726, RMSE: 153.22, MAE: 91.30\n",
      "Epoch 19/125 - Loss: 0.3041, Val Loss: 0.2626, RMSE: 150.42, MAE: 89.20\n",
      "Epoch 20/125 - Loss: 0.3016, Val Loss: 0.2534, RMSE: 147.69, MAE: 87.20\n",
      "Epoch 21/125 - Loss: 0.2982, Val Loss: 0.2456, RMSE: 145.22, MAE: 85.42\n",
      "Epoch 22/125 - Loss: 0.2945, Val Loss: 0.2390, RMSE: 143.07, MAE: 83.92\n",
      "Epoch 23/125 - Loss: 0.2937, Val Loss: 0.2336, RMSE: 141.27, MAE: 82.72\n",
      "Epoch 24/125 - Loss: 0.2890, Val Loss: 0.2292, RMSE: 139.82, MAE: 81.78\n",
      "Epoch 25/125 - Loss: 0.2881, Val Loss: 0.2257, RMSE: 138.66, MAE: 81.03\n",
      "Epoch 26/125 - Loss: 0.2847, Val Loss: 0.2227, RMSE: 137.70, MAE: 80.41\n",
      "Epoch 27/125 - Loss: 0.2811, Val Loss: 0.2201, RMSE: 136.87, MAE: 79.87\n",
      "Epoch 28/125 - Loss: 0.2785, Val Loss: 0.2176, RMSE: 136.08, MAE: 79.35\n",
      "Epoch 29/125 - Loss: 0.2758, Val Loss: 0.2152, RMSE: 135.28, MAE: 78.84\n",
      "Epoch 30/125 - Loss: 0.2749, Val Loss: 0.2127, RMSE: 134.48, MAE: 78.34\n",
      "Epoch 31/125 - Loss: 0.2721, Val Loss: 0.2102, RMSE: 133.62, MAE: 77.83\n",
      "Epoch 32/125 - Loss: 0.2703, Val Loss: 0.2077, RMSE: 132.74, MAE: 77.32\n",
      "Epoch 33/125 - Loss: 0.2683, Val Loss: 0.2052, RMSE: 131.84, MAE: 76.83\n",
      "Epoch 34/125 - Loss: 0.2651, Val Loss: 0.2028, RMSE: 130.97, MAE: 76.38\n",
      "Epoch 35/125 - Loss: 0.2635, Val Loss: 0.2007, RMSE: 130.16, MAE: 75.99\n",
      "Epoch 36/125 - Loss: 0.2617, Val Loss: 0.1988, RMSE: 129.47, MAE: 75.66\n",
      "Epoch 37/125 - Loss: 0.2609, Val Loss: 0.1971, RMSE: 128.86, MAE: 75.39\n",
      "Epoch 38/125 - Loss: 0.2579, Val Loss: 0.1956, RMSE: 128.38, MAE: 75.16\n",
      "Epoch 39/125 - Loss: 0.2578, Val Loss: 0.1944, RMSE: 127.99, MAE: 74.97\n",
      "Epoch 40/125 - Loss: 0.2552, Val Loss: 0.1933, RMSE: 127.68, MAE: 74.80\n",
      "Epoch 41/125 - Loss: 0.2527, Val Loss: 0.1923, RMSE: 127.43, MAE: 74.64\n",
      "Epoch 42/125 - Loss: 0.2514, Val Loss: 0.1915, RMSE: 127.22, MAE: 74.47\n",
      "Epoch 43/125 - Loss: 0.2511, Val Loss: 0.1907, RMSE: 127.03, MAE: 74.31\n",
      "Epoch 44/125 - Loss: 0.2500, Val Loss: 0.1900, RMSE: 126.83, MAE: 74.13\n",
      "Epoch 45/125 - Loss: 0.2468, Val Loss: 0.1893, RMSE: 126.64, MAE: 73.95\n",
      "Epoch 46/125 - Loss: 0.2455, Val Loss: 0.1885, RMSE: 126.46, MAE: 73.76\n",
      "Epoch 47/125 - Loss: 0.2450, Val Loss: 0.1877, RMSE: 126.27, MAE: 73.57\n",
      "Epoch 48/125 - Loss: 0.2441, Val Loss: 0.1869, RMSE: 126.07, MAE: 73.38\n",
      "Epoch 49/125 - Loss: 0.2420, Val Loss: 0.1861, RMSE: 125.87, MAE: 73.19\n",
      "Epoch 50/125 - Loss: 0.2403, Val Loss: 0.1852, RMSE: 125.64, MAE: 73.00\n",
      "Epoch 51/125 - Loss: 0.2403, Val Loss: 0.1844, RMSE: 125.42, MAE: 72.83\n",
      "Epoch 52/125 - Loss: 0.2379, Val Loss: 0.1835, RMSE: 125.19, MAE: 72.66\n",
      "Epoch 53/125 - Loss: 0.2375, Val Loss: 0.1827, RMSE: 124.97, MAE: 72.50\n",
      "Epoch 54/125 - Loss: 0.2351, Val Loss: 0.1819, RMSE: 124.79, MAE: 72.36\n",
      "Epoch 55/125 - Loss: 0.2348, Val Loss: 0.1811, RMSE: 124.63, MAE: 72.23\n",
      "Epoch 56/125 - Loss: 0.2337, Val Loss: 0.1804, RMSE: 124.51, MAE: 72.11\n",
      "Epoch 57/125 - Loss: 0.2320, Val Loss: 0.1798, RMSE: 124.41, MAE: 71.99\n",
      "Epoch 58/125 - Loss: 0.2306, Val Loss: 0.1792, RMSE: 124.34, MAE: 71.88\n",
      "Epoch 59/125 - Loss: 0.2307, Val Loss: 0.1785, RMSE: 124.25, MAE: 71.77\n",
      "Epoch 60/125 - Loss: 0.2281, Val Loss: 0.1779, RMSE: 124.17, MAE: 71.66\n",
      "Epoch 61/125 - Loss: 0.2283, Val Loss: 0.1773, RMSE: 124.09, MAE: 71.55\n",
      "Epoch 62/125 - Loss: 0.2262, Val Loss: 0.1767, RMSE: 123.98, MAE: 71.43\n",
      "Epoch 63/125 - Loss: 0.2259, Val Loss: 0.1761, RMSE: 123.86, MAE: 71.31\n",
      "Epoch 64/125 - Loss: 0.2240, Val Loss: 0.1755, RMSE: 123.71, MAE: 71.18\n",
      "Epoch 65/125 - Loss: 0.2239, Val Loss: 0.1749, RMSE: 123.57, MAE: 71.04\n",
      "Epoch 66/125 - Loss: 0.2230, Val Loss: 0.1743, RMSE: 123.41, MAE: 70.90\n",
      "Epoch 67/125 - Loss: 0.2224, Val Loss: 0.1737, RMSE: 123.25, MAE: 70.76\n",
      "Epoch 68/125 - Loss: 0.2220, Val Loss: 0.1731, RMSE: 123.10, MAE: 70.63\n",
      "Epoch 69/125 - Loss: 0.2205, Val Loss: 0.1726, RMSE: 122.96, MAE: 70.50\n",
      "Epoch 70/125 - Loss: 0.2197, Val Loss: 0.1720, RMSE: 122.85, MAE: 70.37\n",
      "Epoch 71/125 - Loss: 0.2190, Val Loss: 0.1715, RMSE: 122.73, MAE: 70.26\n",
      "Epoch 72/125 - Loss: 0.2173, Val Loss: 0.1710, RMSE: 122.62, MAE: 70.15\n",
      "Epoch 73/125 - Loss: 0.2160, Val Loss: 0.1705, RMSE: 122.54, MAE: 70.06\n",
      "Epoch 74/125 - Loss: 0.2157, Val Loss: 0.1700, RMSE: 122.45, MAE: 69.97\n",
      "Epoch 75/125 - Loss: 0.2163, Val Loss: 0.1695, RMSE: 122.36, MAE: 69.87\n",
      "Epoch 76/125 - Loss: 0.2146, Val Loss: 0.1691, RMSE: 122.26, MAE: 69.78\n",
      "Epoch 77/125 - Loss: 0.2131, Val Loss: 0.1686, RMSE: 122.14, MAE: 69.69\n",
      "Epoch 78/125 - Loss: 0.2127, Val Loss: 0.1681, RMSE: 122.02, MAE: 69.60\n",
      "Epoch 79/125 - Loss: 0.2120, Val Loss: 0.1676, RMSE: 121.90, MAE: 69.50\n",
      "Epoch 80/125 - Loss: 0.2114, Val Loss: 0.1672, RMSE: 121.78, MAE: 69.40\n",
      "Epoch 81/125 - Loss: 0.2106, Val Loss: 0.1667, RMSE: 121.65, MAE: 69.29\n",
      "Epoch 82/125 - Loss: 0.2090, Val Loss: 0.1662, RMSE: 121.52, MAE: 69.19\n",
      "Epoch 83/125 - Loss: 0.2090, Val Loss: 0.1657, RMSE: 121.39, MAE: 69.10\n",
      "Epoch 84/125 - Loss: 0.2082, Val Loss: 0.1653, RMSE: 121.29, MAE: 69.00\n",
      "Epoch 85/125 - Loss: 0.2080, Val Loss: 0.1649, RMSE: 121.21, MAE: 68.91\n",
      "Epoch 86/125 - Loss: 0.2072, Val Loss: 0.1644, RMSE: 121.15, MAE: 68.82\n",
      "Epoch 87/125 - Loss: 0.2065, Val Loss: 0.1640, RMSE: 121.09, MAE: 68.73\n",
      "Epoch 88/125 - Loss: 0.2058, Val Loss: 0.1636, RMSE: 121.02, MAE: 68.65\n",
      "Epoch 89/125 - Loss: 0.2048, Val Loss: 0.1632, RMSE: 120.96, MAE: 68.56\n",
      "Epoch 90/125 - Loss: 0.2050, Val Loss: 0.1628, RMSE: 120.91, MAE: 68.48\n",
      "Epoch 91/125 - Loss: 0.2037, Val Loss: 0.1623, RMSE: 120.85, MAE: 68.39\n",
      "Epoch 92/125 - Loss: 0.2032, Val Loss: 0.1619, RMSE: 120.76, MAE: 68.30\n",
      "Epoch 93/125 - Loss: 0.2026, Val Loss: 0.1614, RMSE: 120.65, MAE: 68.20\n",
      "Epoch 94/125 - Loss: 0.2020, Val Loss: 0.1609, RMSE: 120.51, MAE: 68.10\n",
      "Epoch 95/125 - Loss: 0.2019, Val Loss: 0.1605, RMSE: 120.37, MAE: 68.00\n",
      "Epoch 96/125 - Loss: 0.2007, Val Loss: 0.1600, RMSE: 120.21, MAE: 67.89\n",
      "Epoch 97/125 - Loss: 0.1998, Val Loss: 0.1595, RMSE: 120.06, MAE: 67.79\n",
      "Epoch 98/125 - Loss: 0.1995, Val Loss: 0.1590, RMSE: 119.91, MAE: 67.68\n",
      "Epoch 99/125 - Loss: 0.1985, Val Loss: 0.1586, RMSE: 119.76, MAE: 67.57\n",
      "Epoch 100/125 - Loss: 0.1987, Val Loss: 0.1582, RMSE: 119.62, MAE: 67.47\n",
      "Epoch 101/125 - Loss: 0.1978, Val Loss: 0.1578, RMSE: 119.51, MAE: 67.37\n",
      "Epoch 102/125 - Loss: 0.1970, Val Loss: 0.1574, RMSE: 119.42, MAE: 67.28\n",
      "Epoch 103/125 - Loss: 0.1968, Val Loss: 0.1570, RMSE: 119.33, MAE: 67.18\n",
      "Epoch 104/125 - Loss: 0.1968, Val Loss: 0.1566, RMSE: 119.26, MAE: 67.08\n",
      "Epoch 105/125 - Loss: 0.1965, Val Loss: 0.1563, RMSE: 119.19, MAE: 67.00\n",
      "Epoch 106/125 - Loss: 0.1949, Val Loss: 0.1559, RMSE: 119.11, MAE: 66.91\n",
      "Epoch 107/125 - Loss: 0.1947, Val Loss: 0.1556, RMSE: 119.01, MAE: 66.82\n",
      "Epoch 108/125 - Loss: 0.1943, Val Loss: 0.1552, RMSE: 118.91, MAE: 66.73\n",
      "Epoch 109/125 - Loss: 0.1944, Val Loss: 0.1548, RMSE: 118.80, MAE: 66.64\n",
      "Epoch 110/125 - Loss: 0.1926, Val Loss: 0.1544, RMSE: 118.66, MAE: 66.55\n",
      "Epoch 111/125 - Loss: 0.1932, Val Loss: 0.1540, RMSE: 118.51, MAE: 66.45\n",
      "Epoch 112/125 - Loss: 0.1920, Val Loss: 0.1536, RMSE: 118.35, MAE: 66.36\n",
      "Epoch 113/125 - Loss: 0.1916, Val Loss: 0.1532, RMSE: 118.20, MAE: 66.27\n",
      "Epoch 114/125 - Loss: 0.1915, Val Loss: 0.1528, RMSE: 118.08, MAE: 66.18\n",
      "Epoch 115/125 - Loss: 0.1910, Val Loss: 0.1524, RMSE: 117.97, MAE: 66.09\n",
      "Epoch 116/125 - Loss: 0.1909, Val Loss: 0.1520, RMSE: 117.86, MAE: 66.00\n",
      "Epoch 117/125 - Loss: 0.1905, Val Loss: 0.1516, RMSE: 117.76, MAE: 65.91\n",
      "Epoch 118/125 - Loss: 0.1889, Val Loss: 0.1513, RMSE: 117.67, MAE: 65.82\n",
      "Epoch 119/125 - Loss: 0.1889, Val Loss: 0.1509, RMSE: 117.57, MAE: 65.73\n",
      "Epoch 120/125 - Loss: 0.1883, Val Loss: 0.1505, RMSE: 117.48, MAE: 65.63\n",
      "Epoch 121/125 - Loss: 0.1880, Val Loss: 0.1501, RMSE: 117.40, MAE: 65.54\n",
      "Epoch 122/125 - Loss: 0.1884, Val Loss: 0.1497, RMSE: 117.33, MAE: 65.45\n",
      "Epoch 123/125 - Loss: 0.1873, Val Loss: 0.1494, RMSE: 117.25, MAE: 65.35\n",
      "Epoch 124/125 - Loss: 0.1880, Val Loss: 0.1490, RMSE: 117.16, MAE: 65.25\n",
      "Epoch 125/125 - Loss: 0.1862, Val Loss: 0.1486, RMSE: 117.07, MAE: 65.16\n",
      "\n",
      "----- Evaluating NeighborBasedLSTM on Test Data (Split 3) -----\n",
      "\n",
      "===== Predicting with NeighborBasedLSTM Model =====\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 6300 test instances\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 122.01\n",
      "MAE: 67.53\n",
      "R²: 0.4130\n",
      "MAPE: 34.95%\n",
      "Split 3 Results - RMSE: 122.0122, MAE: 67.5308, R²: 0.4130\n",
      "Model for split 3 saved to ./output/neighbor_lstm_model/neighbor_lstm_model_split_3.pt\n",
      "\n",
      "===== Split 4/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-25\n",
      "Testing period: 2024-01-26 to 2024-02-01\n",
      "Train data: 593555 rows, 3600 unique listings\n",
      "Test data: 6300 rows, 900 unique listings\n",
      "\n",
      "----- Training NeighborBasedLSTM Model (Split 4) -----\n",
      "\n",
      "===== Training NeighborBasedLSTM Model =====\n",
      "LSTM hidden dimension: 8, Max neighbors: 5\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 118572 test instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5189, Val Loss: 0.4003, RMSE: 180.40, MAE: 112.42\n",
      "Epoch 2/125 - Loss: 0.4519, Val Loss: 0.3932, RMSE: 179.93, MAE: 111.45\n",
      "Epoch 3/125 - Loss: 0.4106, Val Loss: 0.3862, RMSE: 179.30, MAE: 110.49\n",
      "Epoch 4/125 - Loss: 0.3856, Val Loss: 0.3792, RMSE: 178.54, MAE: 109.55\n",
      "Epoch 5/125 - Loss: 0.3700, Val Loss: 0.3721, RMSE: 177.65, MAE: 108.60\n",
      "Epoch 6/125 - Loss: 0.3551, Val Loss: 0.3650, RMSE: 176.67, MAE: 107.62\n",
      "Epoch 7/125 - Loss: 0.3460, Val Loss: 0.3578, RMSE: 175.60, MAE: 106.62\n",
      "Epoch 8/125 - Loss: 0.3372, Val Loss: 0.3506, RMSE: 174.48, MAE: 105.60\n",
      "Epoch 9/125 - Loss: 0.3291, Val Loss: 0.3434, RMSE: 173.30, MAE: 104.56\n",
      "Epoch 10/125 - Loss: 0.3214, Val Loss: 0.3361, RMSE: 172.06, MAE: 103.48\n",
      "Epoch 11/125 - Loss: 0.3167, Val Loss: 0.3284, RMSE: 170.72, MAE: 102.32\n",
      "Epoch 12/125 - Loss: 0.3121, Val Loss: 0.3202, RMSE: 169.27, MAE: 101.06\n",
      "Epoch 13/125 - Loss: 0.3053, Val Loss: 0.3115, RMSE: 167.64, MAE: 99.69\n",
      "Epoch 14/125 - Loss: 0.3021, Val Loss: 0.3022, RMSE: 165.80, MAE: 98.16\n",
      "Epoch 15/125 - Loss: 0.2970, Val Loss: 0.2924, RMSE: 163.73, MAE: 96.50\n",
      "Epoch 16/125 - Loss: 0.2924, Val Loss: 0.2825, RMSE: 161.45, MAE: 94.72\n",
      "Epoch 17/125 - Loss: 0.2899, Val Loss: 0.2726, RMSE: 159.01, MAE: 92.86\n",
      "Epoch 18/125 - Loss: 0.2864, Val Loss: 0.2633, RMSE: 156.50, MAE: 91.00\n",
      "Epoch 19/125 - Loss: 0.2839, Val Loss: 0.2546, RMSE: 154.01, MAE: 89.21\n",
      "Epoch 20/125 - Loss: 0.2802, Val Loss: 0.2468, RMSE: 151.62, MAE: 87.55\n",
      "Epoch 21/125 - Loss: 0.2760, Val Loss: 0.2400, RMSE: 149.41, MAE: 86.05\n",
      "Epoch 22/125 - Loss: 0.2738, Val Loss: 0.2339, RMSE: 147.41, MAE: 84.70\n",
      "Epoch 23/125 - Loss: 0.2722, Val Loss: 0.2288, RMSE: 145.68, MAE: 83.53\n",
      "Epoch 24/125 - Loss: 0.2684, Val Loss: 0.2243, RMSE: 144.15, MAE: 82.50\n",
      "Epoch 25/125 - Loss: 0.2657, Val Loss: 0.2204, RMSE: 142.76, MAE: 81.57\n",
      "Epoch 26/125 - Loss: 0.2634, Val Loss: 0.2169, RMSE: 141.52, MAE: 80.74\n",
      "Epoch 27/125 - Loss: 0.2613, Val Loss: 0.2138, RMSE: 140.38, MAE: 79.97\n",
      "Epoch 28/125 - Loss: 0.2608, Val Loss: 0.2109, RMSE: 139.31, MAE: 79.26\n",
      "Epoch 29/125 - Loss: 0.2574, Val Loss: 0.2082, RMSE: 138.23, MAE: 78.56\n",
      "Epoch 30/125 - Loss: 0.2569, Val Loss: 0.2057, RMSE: 137.17, MAE: 77.89\n",
      "Epoch 31/125 - Loss: 0.2538, Val Loss: 0.2033, RMSE: 136.10, MAE: 77.22\n",
      "Epoch 32/125 - Loss: 0.2531, Val Loss: 0.2011, RMSE: 135.05, MAE: 76.59\n",
      "Epoch 33/125 - Loss: 0.2510, Val Loss: 0.1991, RMSE: 134.05, MAE: 76.03\n",
      "Epoch 34/125 - Loss: 0.2503, Val Loss: 0.1973, RMSE: 133.11, MAE: 75.52\n",
      "Epoch 35/125 - Loss: 0.2483, Val Loss: 0.1957, RMSE: 132.26, MAE: 75.08\n",
      "Epoch 36/125 - Loss: 0.2452, Val Loss: 0.1942, RMSE: 131.54, MAE: 74.72\n",
      "Epoch 37/125 - Loss: 0.2449, Val Loss: 0.1930, RMSE: 130.95, MAE: 74.42\n",
      "Epoch 38/125 - Loss: 0.2430, Val Loss: 0.1919, RMSE: 130.46, MAE: 74.15\n",
      "Epoch 39/125 - Loss: 0.2422, Val Loss: 0.1908, RMSE: 130.07, MAE: 73.91\n",
      "Epoch 40/125 - Loss: 0.2413, Val Loss: 0.1898, RMSE: 129.75, MAE: 73.69\n",
      "Epoch 41/125 - Loss: 0.2389, Val Loss: 0.1889, RMSE: 129.51, MAE: 73.48\n",
      "Epoch 42/125 - Loss: 0.2384, Val Loss: 0.1880, RMSE: 129.29, MAE: 73.28\n",
      "Epoch 43/125 - Loss: 0.2368, Val Loss: 0.1871, RMSE: 129.08, MAE: 73.09\n",
      "Epoch 44/125 - Loss: 0.2357, Val Loss: 0.1862, RMSE: 128.88, MAE: 72.89\n",
      "Epoch 45/125 - Loss: 0.2348, Val Loss: 0.1854, RMSE: 128.68, MAE: 72.71\n",
      "Epoch 46/125 - Loss: 0.2339, Val Loss: 0.1845, RMSE: 128.47, MAE: 72.53\n",
      "Epoch 47/125 - Loss: 0.2313, Val Loss: 0.1837, RMSE: 128.22, MAE: 72.34\n",
      "Epoch 48/125 - Loss: 0.2311, Val Loss: 0.1829, RMSE: 127.99, MAE: 72.17\n",
      "Epoch 49/125 - Loss: 0.2303, Val Loss: 0.1821, RMSE: 127.74, MAE: 72.00\n",
      "Epoch 50/125 - Loss: 0.2296, Val Loss: 0.1813, RMSE: 127.48, MAE: 71.83\n",
      "Epoch 51/125 - Loss: 0.2270, Val Loss: 0.1806, RMSE: 127.23, MAE: 71.67\n",
      "Epoch 52/125 - Loss: 0.2275, Val Loss: 0.1798, RMSE: 126.96, MAE: 71.51\n",
      "Epoch 53/125 - Loss: 0.2255, Val Loss: 0.1791, RMSE: 126.72, MAE: 71.36\n",
      "Epoch 54/125 - Loss: 0.2247, Val Loss: 0.1784, RMSE: 126.49, MAE: 71.21\n",
      "Epoch 55/125 - Loss: 0.2237, Val Loss: 0.1777, RMSE: 126.29, MAE: 71.07\n",
      "Epoch 56/125 - Loss: 0.2221, Val Loss: 0.1771, RMSE: 126.12, MAE: 70.94\n",
      "Epoch 57/125 - Loss: 0.2211, Val Loss: 0.1764, RMSE: 125.97, MAE: 70.81\n",
      "Epoch 58/125 - Loss: 0.2202, Val Loss: 0.1758, RMSE: 125.85, MAE: 70.69\n",
      "Epoch 59/125 - Loss: 0.2198, Val Loss: 0.1752, RMSE: 125.74, MAE: 70.57\n",
      "Epoch 60/125 - Loss: 0.2187, Val Loss: 0.1746, RMSE: 125.64, MAE: 70.46\n",
      "Epoch 61/125 - Loss: 0.2171, Val Loss: 0.1740, RMSE: 125.58, MAE: 70.36\n",
      "Epoch 62/125 - Loss: 0.2171, Val Loss: 0.1735, RMSE: 125.50, MAE: 70.25\n",
      "Epoch 63/125 - Loss: 0.2156, Val Loss: 0.1729, RMSE: 125.40, MAE: 70.15\n",
      "Epoch 64/125 - Loss: 0.2145, Val Loss: 0.1723, RMSE: 125.31, MAE: 70.04\n",
      "Epoch 65/125 - Loss: 0.2145, Val Loss: 0.1717, RMSE: 125.21, MAE: 69.94\n",
      "Epoch 66/125 - Loss: 0.2148, Val Loss: 0.1711, RMSE: 125.11, MAE: 69.83\n",
      "Epoch 67/125 - Loss: 0.2124, Val Loss: 0.1705, RMSE: 124.96, MAE: 69.71\n",
      "Epoch 68/125 - Loss: 0.2127, Val Loss: 0.1699, RMSE: 124.82, MAE: 69.59\n",
      "Epoch 69/125 - Loss: 0.2113, Val Loss: 0.1694, RMSE: 124.65, MAE: 69.47\n",
      "Epoch 70/125 - Loss: 0.2094, Val Loss: 0.1688, RMSE: 124.46, MAE: 69.35\n",
      "Epoch 71/125 - Loss: 0.2101, Val Loss: 0.1682, RMSE: 124.29, MAE: 69.22\n",
      "Epoch 72/125 - Loss: 0.2084, Val Loss: 0.1676, RMSE: 124.12, MAE: 69.10\n",
      "Epoch 73/125 - Loss: 0.2082, Val Loss: 0.1670, RMSE: 123.96, MAE: 68.99\n",
      "Epoch 74/125 - Loss: 0.2077, Val Loss: 0.1665, RMSE: 123.84, MAE: 68.88\n",
      "Epoch 75/125 - Loss: 0.2065, Val Loss: 0.1660, RMSE: 123.74, MAE: 68.78\n",
      "Epoch 76/125 - Loss: 0.2060, Val Loss: 0.1654, RMSE: 123.60, MAE: 68.67\n",
      "Epoch 77/125 - Loss: 0.2046, Val Loss: 0.1649, RMSE: 123.46, MAE: 68.56\n",
      "Epoch 78/125 - Loss: 0.2047, Val Loss: 0.1643, RMSE: 123.29, MAE: 68.44\n",
      "Epoch 79/125 - Loss: 0.2037, Val Loss: 0.1638, RMSE: 123.15, MAE: 68.33\n",
      "Epoch 80/125 - Loss: 0.2028, Val Loss: 0.1632, RMSE: 123.01, MAE: 68.22\n",
      "Epoch 81/125 - Loss: 0.2023, Val Loss: 0.1627, RMSE: 122.86, MAE: 68.11\n",
      "Epoch 82/125 - Loss: 0.2017, Val Loss: 0.1621, RMSE: 122.73, MAE: 67.99\n",
      "Epoch 83/125 - Loss: 0.2015, Val Loss: 0.1616, RMSE: 122.62, MAE: 67.88\n",
      "Epoch 84/125 - Loss: 0.2006, Val Loss: 0.1610, RMSE: 122.51, MAE: 67.77\n",
      "Epoch 85/125 - Loss: 0.1992, Val Loss: 0.1605, RMSE: 122.39, MAE: 67.66\n",
      "Epoch 86/125 - Loss: 0.1990, Val Loss: 0.1600, RMSE: 122.27, MAE: 67.55\n",
      "Epoch 87/125 - Loss: 0.1982, Val Loss: 0.1595, RMSE: 122.11, MAE: 67.44\n",
      "Epoch 88/125 - Loss: 0.1983, Val Loss: 0.1589, RMSE: 121.97, MAE: 67.33\n",
      "Epoch 89/125 - Loss: 0.1968, Val Loss: 0.1584, RMSE: 121.80, MAE: 67.22\n",
      "Epoch 90/125 - Loss: 0.1971, Val Loss: 0.1579, RMSE: 121.64, MAE: 67.11\n",
      "Epoch 91/125 - Loss: 0.1958, Val Loss: 0.1574, RMSE: 121.50, MAE: 67.01\n",
      "Epoch 92/125 - Loss: 0.1959, Val Loss: 0.1569, RMSE: 121.38, MAE: 66.90\n",
      "Epoch 93/125 - Loss: 0.1943, Val Loss: 0.1564, RMSE: 121.25, MAE: 66.80\n",
      "Epoch 94/125 - Loss: 0.1953, Val Loss: 0.1559, RMSE: 121.11, MAE: 66.70\n",
      "Epoch 95/125 - Loss: 0.1941, Val Loss: 0.1554, RMSE: 120.99, MAE: 66.60\n",
      "Epoch 96/125 - Loss: 0.1931, Val Loss: 0.1549, RMSE: 120.88, MAE: 66.50\n",
      "Epoch 97/125 - Loss: 0.1932, Val Loss: 0.1544, RMSE: 120.79, MAE: 66.40\n",
      "Epoch 98/125 - Loss: 0.1922, Val Loss: 0.1540, RMSE: 120.72, MAE: 66.31\n",
      "Epoch 99/125 - Loss: 0.1923, Val Loss: 0.1535, RMSE: 120.64, MAE: 66.21\n",
      "Epoch 100/125 - Loss: 0.1905, Val Loss: 0.1530, RMSE: 120.54, MAE: 66.11\n",
      "Epoch 101/125 - Loss: 0.1907, Val Loss: 0.1525, RMSE: 120.44, MAE: 66.02\n",
      "Epoch 102/125 - Loss: 0.1903, Val Loss: 0.1521, RMSE: 120.30, MAE: 65.92\n",
      "Epoch 103/125 - Loss: 0.1901, Val Loss: 0.1516, RMSE: 120.16, MAE: 65.83\n",
      "Epoch 104/125 - Loss: 0.1885, Val Loss: 0.1512, RMSE: 119.98, MAE: 65.74\n",
      "Epoch 105/125 - Loss: 0.1880, Val Loss: 0.1507, RMSE: 119.79, MAE: 65.64\n",
      "Epoch 106/125 - Loss: 0.1879, Val Loss: 0.1503, RMSE: 119.61, MAE: 65.54\n",
      "Epoch 107/125 - Loss: 0.1875, Val Loss: 0.1498, RMSE: 119.45, MAE: 65.44\n",
      "Epoch 108/125 - Loss: 0.1861, Val Loss: 0.1493, RMSE: 119.33, MAE: 65.34\n",
      "Epoch 109/125 - Loss: 0.1867, Val Loss: 0.1488, RMSE: 119.21, MAE: 65.23\n",
      "Epoch 110/125 - Loss: 0.1855, Val Loss: 0.1483, RMSE: 119.13, MAE: 65.13\n",
      "Epoch 111/125 - Loss: 0.1853, Val Loss: 0.1478, RMSE: 119.03, MAE: 65.02\n",
      "Epoch 112/125 - Loss: 0.1842, Val Loss: 0.1473, RMSE: 118.95, MAE: 64.92\n",
      "Epoch 113/125 - Loss: 0.1841, Val Loss: 0.1469, RMSE: 118.86, MAE: 64.82\n",
      "Epoch 114/125 - Loss: 0.1841, Val Loss: 0.1464, RMSE: 118.74, MAE: 64.72\n",
      "Epoch 115/125 - Loss: 0.1834, Val Loss: 0.1459, RMSE: 118.60, MAE: 64.62\n",
      "Epoch 116/125 - Loss: 0.1828, Val Loss: 0.1455, RMSE: 118.45, MAE: 64.52\n",
      "Epoch 117/125 - Loss: 0.1819, Val Loss: 0.1450, RMSE: 118.30, MAE: 64.41\n",
      "Epoch 118/125 - Loss: 0.1817, Val Loss: 0.1445, RMSE: 118.14, MAE: 64.30\n",
      "Epoch 119/125 - Loss: 0.1806, Val Loss: 0.1440, RMSE: 117.96, MAE: 64.18\n",
      "Epoch 120/125 - Loss: 0.1809, Val Loss: 0.1435, RMSE: 117.81, MAE: 64.07\n",
      "Epoch 121/125 - Loss: 0.1800, Val Loss: 0.1429, RMSE: 117.68, MAE: 63.95\n",
      "Epoch 122/125 - Loss: 0.1795, Val Loss: 0.1425, RMSE: 117.54, MAE: 63.84\n",
      "Epoch 123/125 - Loss: 0.1791, Val Loss: 0.1420, RMSE: 117.41, MAE: 63.73\n",
      "Epoch 124/125 - Loss: 0.1789, Val Loss: 0.1415, RMSE: 117.31, MAE: 63.64\n",
      "Epoch 125/125 - Loss: 0.1788, Val Loss: 0.1411, RMSE: 117.22, MAE: 63.54\n",
      "\n",
      "----- Evaluating NeighborBasedLSTM on Test Data (Split 4) -----\n",
      "\n",
      "===== Predicting with NeighborBasedLSTM Model =====\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 6300 test instances\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 127.02\n",
      "MAE: 68.22\n",
      "R²: 0.3893\n",
      "MAPE: 33.85%\n",
      "Split 4 Results - RMSE: 127.0158, MAE: 68.2225, R²: 0.3893\n",
      "Model for split 4 saved to ./output/neighbor_lstm_model/neighbor_lstm_model_split_4.pt\n",
      "\n",
      "===== Split 5/5 =====\n",
      "Training period: 2023-08-07 to 2024-02-01\n",
      "Testing period: 2024-02-02 to 2024-02-08\n",
      "Train data: 618755 rows, 3600 unique listings\n",
      "Test data: 6300 rows, 900 unique listings\n",
      "\n",
      "----- Training NeighborBasedLSTM Model (Split 5) -----\n",
      "\n",
      "===== Training NeighborBasedLSTM Model =====\n",
      "LSTM hidden dimension: 8, Max neighbors: 5\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 123612 test instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5040, Val Loss: 0.4122, RMSE: 178.96, MAE: 113.43\n",
      "Epoch 2/125 - Loss: 0.4414, Val Loss: 0.4049, RMSE: 178.68, MAE: 112.38\n",
      "Epoch 3/125 - Loss: 0.3965, Val Loss: 0.3970, RMSE: 178.20, MAE: 111.28\n",
      "Epoch 4/125 - Loss: 0.3640, Val Loss: 0.3886, RMSE: 177.51, MAE: 110.10\n",
      "Epoch 5/125 - Loss: 0.3462, Val Loss: 0.3797, RMSE: 176.61, MAE: 108.86\n",
      "Epoch 6/125 - Loss: 0.3325, Val Loss: 0.3703, RMSE: 175.54, MAE: 107.59\n",
      "Epoch 7/125 - Loss: 0.3245, Val Loss: 0.3610, RMSE: 174.35, MAE: 106.29\n",
      "Epoch 8/125 - Loss: 0.3180, Val Loss: 0.3518, RMSE: 173.08, MAE: 104.99\n",
      "Epoch 9/125 - Loss: 0.3115, Val Loss: 0.3431, RMSE: 171.78, MAE: 103.73\n",
      "Epoch 10/125 - Loss: 0.3061, Val Loss: 0.3349, RMSE: 170.47, MAE: 102.50\n",
      "Epoch 11/125 - Loss: 0.2992, Val Loss: 0.3272, RMSE: 169.16, MAE: 101.32\n",
      "Epoch 12/125 - Loss: 0.2933, Val Loss: 0.3200, RMSE: 167.87, MAE: 100.19\n",
      "Epoch 13/125 - Loss: 0.2872, Val Loss: 0.3130, RMSE: 166.56, MAE: 99.06\n",
      "Epoch 14/125 - Loss: 0.2843, Val Loss: 0.3060, RMSE: 165.18, MAE: 97.90\n",
      "Epoch 15/125 - Loss: 0.2817, Val Loss: 0.2986, RMSE: 163.66, MAE: 96.67\n",
      "Epoch 16/125 - Loss: 0.2780, Val Loss: 0.2908, RMSE: 161.96, MAE: 95.30\n",
      "Epoch 17/125 - Loss: 0.2753, Val Loss: 0.2823, RMSE: 160.01, MAE: 93.78\n",
      "Epoch 18/125 - Loss: 0.2722, Val Loss: 0.2735, RMSE: 157.80, MAE: 92.10\n",
      "Epoch 19/125 - Loss: 0.2683, Val Loss: 0.2642, RMSE: 155.32, MAE: 90.26\n",
      "Epoch 20/125 - Loss: 0.2664, Val Loss: 0.2550, RMSE: 152.64, MAE: 88.30\n",
      "Epoch 21/125 - Loss: 0.2625, Val Loss: 0.2460, RMSE: 149.81, MAE: 86.32\n",
      "Epoch 22/125 - Loss: 0.2596, Val Loss: 0.2378, RMSE: 146.99, MAE: 84.40\n",
      "Epoch 23/125 - Loss: 0.2575, Val Loss: 0.2305, RMSE: 144.29, MAE: 82.60\n",
      "Epoch 24/125 - Loss: 0.2550, Val Loss: 0.2243, RMSE: 141.82, MAE: 81.03\n",
      "Epoch 25/125 - Loss: 0.2525, Val Loss: 0.2191, RMSE: 139.66, MAE: 79.73\n",
      "Epoch 26/125 - Loss: 0.2517, Val Loss: 0.2149, RMSE: 137.84, MAE: 78.67\n",
      "Epoch 27/125 - Loss: 0.2504, Val Loss: 0.2115, RMSE: 136.36, MAE: 77.83\n",
      "Epoch 28/125 - Loss: 0.2480, Val Loss: 0.2087, RMSE: 135.19, MAE: 77.16\n",
      "Epoch 29/125 - Loss: 0.2470, Val Loss: 0.2064, RMSE: 134.27, MAE: 76.63\n",
      "Epoch 30/125 - Loss: 0.2435, Val Loss: 0.2044, RMSE: 133.53, MAE: 76.18\n",
      "Epoch 31/125 - Loss: 0.2429, Val Loss: 0.2026, RMSE: 132.90, MAE: 75.79\n",
      "Epoch 32/125 - Loss: 0.2410, Val Loss: 0.2010, RMSE: 132.32, MAE: 75.42\n",
      "Epoch 33/125 - Loss: 0.2396, Val Loss: 0.1993, RMSE: 131.73, MAE: 75.03\n",
      "Epoch 34/125 - Loss: 0.2379, Val Loss: 0.1976, RMSE: 131.10, MAE: 74.64\n",
      "Epoch 35/125 - Loss: 0.2369, Val Loss: 0.1958, RMSE: 130.39, MAE: 74.21\n",
      "Epoch 36/125 - Loss: 0.2344, Val Loss: 0.1940, RMSE: 129.62, MAE: 73.77\n",
      "Epoch 37/125 - Loss: 0.2336, Val Loss: 0.1922, RMSE: 128.82, MAE: 73.34\n",
      "Epoch 38/125 - Loss: 0.2322, Val Loss: 0.1904, RMSE: 127.98, MAE: 72.90\n",
      "Epoch 39/125 - Loss: 0.2302, Val Loss: 0.1887, RMSE: 127.14, MAE: 72.50\n",
      "Epoch 40/125 - Loss: 0.2300, Val Loss: 0.1871, RMSE: 126.35, MAE: 72.13\n",
      "Epoch 41/125 - Loss: 0.2284, Val Loss: 0.1857, RMSE: 125.63, MAE: 71.81\n",
      "Epoch 42/125 - Loss: 0.2265, Val Loss: 0.1844, RMSE: 124.99, MAE: 71.53\n",
      "Epoch 43/125 - Loss: 0.2265, Val Loss: 0.1832, RMSE: 124.44, MAE: 71.29\n",
      "Epoch 44/125 - Loss: 0.2260, Val Loss: 0.1821, RMSE: 123.98, MAE: 71.08\n",
      "Epoch 45/125 - Loss: 0.2234, Val Loss: 0.1812, RMSE: 123.62, MAE: 70.88\n",
      "Epoch 46/125 - Loss: 0.2238, Val Loss: 0.1803, RMSE: 123.34, MAE: 70.70\n",
      "Epoch 47/125 - Loss: 0.2228, Val Loss: 0.1794, RMSE: 123.13, MAE: 70.52\n",
      "Epoch 48/125 - Loss: 0.2211, Val Loss: 0.1786, RMSE: 122.96, MAE: 70.35\n",
      "Epoch 49/125 - Loss: 0.2202, Val Loss: 0.1778, RMSE: 122.83, MAE: 70.18\n",
      "Epoch 50/125 - Loss: 0.2187, Val Loss: 0.1770, RMSE: 122.72, MAE: 70.01\n",
      "Epoch 51/125 - Loss: 0.2177, Val Loss: 0.1763, RMSE: 122.62, MAE: 69.85\n",
      "Epoch 52/125 - Loss: 0.2175, Val Loss: 0.1755, RMSE: 122.49, MAE: 69.69\n",
      "Epoch 53/125 - Loss: 0.2155, Val Loss: 0.1748, RMSE: 122.33, MAE: 69.53\n",
      "Epoch 54/125 - Loss: 0.2156, Val Loss: 0.1741, RMSE: 122.17, MAE: 69.38\n",
      "Epoch 55/125 - Loss: 0.2146, Val Loss: 0.1733, RMSE: 121.98, MAE: 69.24\n",
      "Epoch 56/125 - Loss: 0.2141, Val Loss: 0.1726, RMSE: 121.77, MAE: 69.09\n",
      "Epoch 57/125 - Loss: 0.2127, Val Loss: 0.1719, RMSE: 121.55, MAE: 68.95\n",
      "Epoch 58/125 - Loss: 0.2117, Val Loss: 0.1712, RMSE: 121.31, MAE: 68.82\n",
      "Epoch 59/125 - Loss: 0.2105, Val Loss: 0.1704, RMSE: 121.07, MAE: 68.69\n",
      "Epoch 60/125 - Loss: 0.2103, Val Loss: 0.1697, RMSE: 120.82, MAE: 68.56\n",
      "Epoch 61/125 - Loss: 0.2093, Val Loss: 0.1691, RMSE: 120.62, MAE: 68.44\n",
      "Epoch 62/125 - Loss: 0.2090, Val Loss: 0.1684, RMSE: 120.45, MAE: 68.33\n",
      "Epoch 63/125 - Loss: 0.2071, Val Loss: 0.1678, RMSE: 120.30, MAE: 68.22\n",
      "Epoch 64/125 - Loss: 0.2067, Val Loss: 0.1671, RMSE: 120.17, MAE: 68.10\n",
      "Epoch 65/125 - Loss: 0.2063, Val Loss: 0.1665, RMSE: 120.06, MAE: 68.00\n",
      "Epoch 66/125 - Loss: 0.2063, Val Loss: 0.1659, RMSE: 119.97, MAE: 67.88\n",
      "Epoch 67/125 - Loss: 0.2050, Val Loss: 0.1653, RMSE: 119.89, MAE: 67.77\n",
      "Epoch 68/125 - Loss: 0.2044, Val Loss: 0.1647, RMSE: 119.81, MAE: 67.65\n",
      "Epoch 69/125 - Loss: 0.2038, Val Loss: 0.1641, RMSE: 119.73, MAE: 67.52\n",
      "Epoch 70/125 - Loss: 0.2023, Val Loss: 0.1636, RMSE: 119.65, MAE: 67.41\n",
      "Epoch 71/125 - Loss: 0.2012, Val Loss: 0.1630, RMSE: 119.55, MAE: 67.29\n",
      "Epoch 72/125 - Loss: 0.2017, Val Loss: 0.1624, RMSE: 119.42, MAE: 67.17\n",
      "Epoch 73/125 - Loss: 0.2003, Val Loss: 0.1619, RMSE: 119.30, MAE: 67.06\n",
      "Epoch 74/125 - Loss: 0.2002, Val Loss: 0.1613, RMSE: 119.17, MAE: 66.95\n",
      "Epoch 75/125 - Loss: 0.1993, Val Loss: 0.1607, RMSE: 119.03, MAE: 66.84\n",
      "Epoch 76/125 - Loss: 0.1990, Val Loss: 0.1602, RMSE: 118.87, MAE: 66.73\n",
      "Epoch 77/125 - Loss: 0.1986, Val Loss: 0.1596, RMSE: 118.72, MAE: 66.63\n",
      "Epoch 78/125 - Loss: 0.1975, Val Loss: 0.1590, RMSE: 118.57, MAE: 66.53\n",
      "Epoch 79/125 - Loss: 0.1969, Val Loss: 0.1585, RMSE: 118.44, MAE: 66.43\n",
      "Epoch 80/125 - Loss: 0.1962, Val Loss: 0.1579, RMSE: 118.30, MAE: 66.33\n",
      "Epoch 81/125 - Loss: 0.1959, Val Loss: 0.1573, RMSE: 118.17, MAE: 66.23\n",
      "Epoch 82/125 - Loss: 0.1956, Val Loss: 0.1568, RMSE: 118.05, MAE: 66.14\n",
      "Epoch 83/125 - Loss: 0.1950, Val Loss: 0.1562, RMSE: 117.94, MAE: 66.03\n",
      "Epoch 84/125 - Loss: 0.1935, Val Loss: 0.1556, RMSE: 117.84, MAE: 65.93\n",
      "Epoch 85/125 - Loss: 0.1934, Val Loss: 0.1550, RMSE: 117.74, MAE: 65.82\n",
      "Epoch 86/125 - Loss: 0.1934, Val Loss: 0.1545, RMSE: 117.66, MAE: 65.71\n",
      "Epoch 87/125 - Loss: 0.1921, Val Loss: 0.1539, RMSE: 117.55, MAE: 65.61\n",
      "Epoch 88/125 - Loss: 0.1917, Val Loss: 0.1534, RMSE: 117.45, MAE: 65.50\n",
      "Epoch 89/125 - Loss: 0.1906, Val Loss: 0.1528, RMSE: 117.32, MAE: 65.40\n",
      "Epoch 90/125 - Loss: 0.1905, Val Loss: 0.1523, RMSE: 117.18, MAE: 65.29\n",
      "Epoch 91/125 - Loss: 0.1899, Val Loss: 0.1517, RMSE: 117.00, MAE: 65.18\n",
      "Epoch 92/125 - Loss: 0.1893, Val Loss: 0.1512, RMSE: 116.82, MAE: 65.06\n",
      "Epoch 93/125 - Loss: 0.1886, Val Loss: 0.1507, RMSE: 116.63, MAE: 64.95\n",
      "Epoch 94/125 - Loss: 0.1883, Val Loss: 0.1501, RMSE: 116.44, MAE: 64.84\n",
      "Epoch 95/125 - Loss: 0.1880, Val Loss: 0.1496, RMSE: 116.26, MAE: 64.73\n",
      "Epoch 96/125 - Loss: 0.1868, Val Loss: 0.1490, RMSE: 116.07, MAE: 64.62\n",
      "Epoch 97/125 - Loss: 0.1865, Val Loss: 0.1485, RMSE: 115.90, MAE: 64.51\n",
      "Epoch 98/125 - Loss: 0.1858, Val Loss: 0.1479, RMSE: 115.72, MAE: 64.39\n",
      "Epoch 99/125 - Loss: 0.1853, Val Loss: 0.1473, RMSE: 115.57, MAE: 64.27\n",
      "Epoch 100/125 - Loss: 0.1843, Val Loss: 0.1467, RMSE: 115.39, MAE: 64.14\n",
      "Epoch 101/125 - Loss: 0.1853, Val Loss: 0.1460, RMSE: 115.21, MAE: 64.01\n",
      "Epoch 102/125 - Loss: 0.1843, Val Loss: 0.1454, RMSE: 115.05, MAE: 63.88\n",
      "Epoch 103/125 - Loss: 0.1836, Val Loss: 0.1449, RMSE: 114.90, MAE: 63.76\n",
      "Epoch 104/125 - Loss: 0.1830, Val Loss: 0.1443, RMSE: 114.79, MAE: 63.64\n",
      "Epoch 105/125 - Loss: 0.1825, Val Loss: 0.1438, RMSE: 114.65, MAE: 63.52\n",
      "Epoch 106/125 - Loss: 0.1814, Val Loss: 0.1432, RMSE: 114.52, MAE: 63.41\n",
      "Epoch 107/125 - Loss: 0.1808, Val Loss: 0.1427, RMSE: 114.40, MAE: 63.31\n",
      "Epoch 108/125 - Loss: 0.1809, Val Loss: 0.1422, RMSE: 114.30, MAE: 63.21\n",
      "Epoch 109/125 - Loss: 0.1795, Val Loss: 0.1416, RMSE: 114.14, MAE: 63.10\n",
      "Epoch 110/125 - Loss: 0.1801, Val Loss: 0.1411, RMSE: 113.94, MAE: 62.98\n",
      "Epoch 111/125 - Loss: 0.1787, Val Loss: 0.1405, RMSE: 113.72, MAE: 62.84\n",
      "Epoch 112/125 - Loss: 0.1780, Val Loss: 0.1399, RMSE: 113.50, MAE: 62.71\n",
      "Epoch 113/125 - Loss: 0.1773, Val Loss: 0.1392, RMSE: 113.29, MAE: 62.57\n",
      "Epoch 114/125 - Loss: 0.1775, Val Loss: 0.1386, RMSE: 113.08, MAE: 62.42\n",
      "Epoch 115/125 - Loss: 0.1773, Val Loss: 0.1379, RMSE: 112.89, MAE: 62.27\n",
      "Epoch 116/125 - Loss: 0.1766, Val Loss: 0.1373, RMSE: 112.73, MAE: 62.13\n",
      "Epoch 117/125 - Loss: 0.1763, Val Loss: 0.1367, RMSE: 112.58, MAE: 62.00\n",
      "Epoch 118/125 - Loss: 0.1754, Val Loss: 0.1361, RMSE: 112.42, MAE: 61.86\n",
      "Epoch 119/125 - Loss: 0.1750, Val Loss: 0.1355, RMSE: 112.26, MAE: 61.73\n",
      "Epoch 120/125 - Loss: 0.1745, Val Loss: 0.1349, RMSE: 112.09, MAE: 61.60\n",
      "Epoch 121/125 - Loss: 0.1732, Val Loss: 0.1343, RMSE: 111.91, MAE: 61.47\n",
      "Epoch 122/125 - Loss: 0.1732, Val Loss: 0.1337, RMSE: 111.70, MAE: 61.33\n",
      "Epoch 123/125 - Loss: 0.1725, Val Loss: 0.1332, RMSE: 111.51, MAE: 61.20\n",
      "Epoch 124/125 - Loss: 0.1722, Val Loss: 0.1326, RMSE: 111.31, MAE: 61.07\n",
      "Epoch 125/125 - Loss: 0.1715, Val Loss: 0.1320, RMSE: 111.13, MAE: 60.93\n",
      "\n",
      "----- Evaluating NeighborBasedLSTM on Test Data (Split 5) -----\n",
      "\n",
      "===== Predicting with NeighborBasedLSTM Model =====\n",
      "Preparing data for NeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 6300 test instances\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 127.11\n",
      "MAE: 68.42\n",
      "R²: 0.3906\n",
      "MAPE: 33.04%\n",
      "Split 5 Results - RMSE: 127.1081, MAE: 68.4160, R²: 0.3906\n",
      "Model for split 5 saved to ./output/neighbor_lstm_model/neighbor_lstm_model_split_5.pt\n",
      "Results saved to ./output/neighbor_lstm_model/neighbor_lstm_rolling_window_results.csv\n",
      "Daily metrics saved to ./output/neighbor_lstm_model/neighbor_lstm_rolling_window_metrics.csv\n",
      "\n",
      "===== NeighborBasedLSTM ROLLING WINDOW CV SUMMARY =====\n",
      "Using 3600 listings for training and 900 listings for testing\n",
      "\n",
      "=== Overall Metrics ===\n",
      "RMSE: 122.8619\n",
      "MAE: 67.9133\n",
      "R²: 0.4118\n",
      "MAPE: 34.9897%\n",
      "\n",
      "=== Split Performance ===\n",
      " split       rmse       mae       r2  n_samples\n",
      "     0 119.550049 68.958640 0.438206       6300\n",
      "     1 118.349494 66.438435 0.430071       6300\n",
      "     2 122.012198 67.530762 0.412987       6300\n",
      "     3 127.015845 68.222526 0.389274       6300\n",
      "     4 127.108063 68.416046 0.390617       6300\n",
      "NeighborBasedLSTM model with rolling window CV completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = \"train_up3.csv\"\n",
    "    train_ids_path = \"train_ids.txt\"\n",
    "    test_ids_path = \"test_ids.txt\"\n",
    "    neighbor_csv_path = \"./neighbor_data/neighbor_dict.csv\"  # Path to neighbor information CSV\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/neighbor_lstm_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters\n",
    "    lstm_hidden_dim = 8         # Hidden dimension for LSTM (8 or 16)\n",
    "    hidden_dim = 64             # Hidden dimension size for rest of model\n",
    "    max_neighbors = 5           # Maximum number of neighbors to consider per listing\n",
    "    seq_length = 30             # Sequence length for neighbor price history\n",
    "    epochs = 125                 # Maximum number of epochs\n",
    "    lr = 0.001                  # Learning rate\n",
    "    \n",
    "    # Choose between different run modes\n",
    "    run_mode = \"rolling_window\"  # Options: \"single\", \"rolling_window\", \"compare_dims\"\n",
    "    \n",
    "    try:\n",
    "        if run_mode == \"single\":\n",
    "            # Run single model training\n",
    "            result_tuple = run_neighbor_lstm_model(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                neighbor_csv_path=neighbor_csv_path,\n",
    "                output_dir=output_dir,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                epochs=epochs,\n",
    "                lr=lr,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            \n",
    "            if result_tuple:\n",
    "                model, property_scaler, temporal_scaler, target_scaler, test_metrics = result_tuple\n",
    "                print(\"NeighborBasedLSTM model training completed successfully!\")\n",
    "                \n",
    "                # Print summary of model\n",
    "                print(\"\\n===== Model Summary =====\")\n",
    "                total_param = sum(p.numel() for p in model.parameters())\n",
    "                lstm_param = sum(p.numel() for name, p in model.named_parameters() if 'lstm' in name)\n",
    "                print(f\"Total model parameters: {total_param:,}\")\n",
    "                print(f\"LSTM parameters: {lstm_param:,}\")\n",
    "                print(f\"LSTM parameters as % of total: {lstm_param/total_param*100:.2f}%\")\n",
    "                \n",
    "        elif run_mode == \"rolling_window\":\n",
    "            # Run with rolling window cross-validation\n",
    "            results = run_neighbor_lstm_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                neighbor_csv_path=neighbor_csv_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                epochs=epochs,\n",
    "                lr=lr,\n",
    "                sample_size=4500  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            print(\"NeighborBasedLSTM model with rolling window CV completed successfully!\")\n",
    "            \n",
    "        elif run_mode == \"compare_dims\":\n",
    "            # Run comparison between 8-dim and 16-dim LSTM models\n",
    "            comparison = compare_lstm_dimensions(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                neighbor_csv_path=neighbor_csv_path,\n",
    "                output_dir=output_dir,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                sample_size=200  # Use small sample for faster comparison\n",
    "            )\n",
    "            print(\"LSTM dimension comparison completed successfully!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running NeighborBasedLSTM model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
