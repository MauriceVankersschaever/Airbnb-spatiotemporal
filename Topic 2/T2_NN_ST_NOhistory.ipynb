{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: topic2_transformed.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Building neighbor network with k=5 neighbors per listing...\n",
      "Neighbor network built: 7864 listings with neighbors\n",
      "Average neighbors per listing: 5.00\n",
      "Min neighbors: 5, Max neighbors: 5\n",
      "Creating neighbor price history features...\n",
      "Creating neighbor price history with seq_length=30 and max_neighbors=5...\n",
      "Processing row 0/1404469\n",
      "Processing row 1000/1404469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1586\u001b[0m\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM parameters as % of total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_param\u001b[38;5;241m/\u001b[39mtotal_param\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m run_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_window\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;66;03m# Run with rolling window cross-validation\u001b[39;00m\n\u001b[1;32m-> 1586\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_neighbor_aware_gnn_with_rolling_window_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_ids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ids_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 5 weeks\u001b[39;49;00m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlstm_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlstm_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to a number for testing or None for full dataset\u001b[39;49;00m\n\u001b[0;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeighbor-aware GNN with rolling window CV completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m run_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompare\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1602\u001b[0m     \u001b[38;5;66;03m# Run comparison between different neighbor configurations\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 817\u001b[0m, in \u001b[0;36mrun_neighbor_aware_gnn_with_rolling_window_cv\u001b[1;34m(train_path, train_ids_path, test_ids_path, output_dir, window_size, n_splits, sample_size, max_neighbors, seq_length, lstm_hidden_dim, lstm_layers)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Create neighbor price history\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating neighbor price history features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 817\u001b[0m train_data, neighbor_price_features \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_neighbor_price_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbor_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_neighbors\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Define feature groups based on your dataset columns\u001b[39;00m\n\u001b[0;32m    822\u001b[0m spatial_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    824\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[1], line 251\u001b[0m, in \u001b[0;36mcreate_neighbor_price_history\u001b[1;34m(df, neighbor_dict, seq_length, max_neighbors)\u001b[0m\n\u001b[0;32m    249\u001b[0m history_date \u001b[38;5;241m=\u001b[39m all_dates[history_date_idx]\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neighbor_id \u001b[38;5;129;01min\u001b[39;00m price_lookup \u001b[38;5;129;01mand\u001b[39;00m history_date \u001b[38;5;129;01min\u001b[39;00m price_lookup[neighbor_id]:\n\u001b[1;32m--> 251\u001b[0m     \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneighbor_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_price_day_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43md\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m price_lookup[neighbor_id][history_date]\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# No data for this neighbor on this date, use current listing's price\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_price_day_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\indexing.py:2164\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2162\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 2164\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241m.\u001b[39miloc[loc]\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[0;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[0;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   2174\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\generic.py:6460\u001b[0m, in \u001b[0;36mNDFrame.dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6432\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   6433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtypes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   6434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6435\u001b[0m \u001b[38;5;124;03m    Return the dtypes in the DataFrame.\u001b[39;00m\n\u001b[0;32m   6436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6458\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6460\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(data, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n",
      "File \u001b[1;32mc:\\Users\\mvk\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:289\u001b[0m, in \u001b[0;36mBaseBlockManager.get_dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dtypes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[0;32m    288\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([blk\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblknos\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.dates as mdates\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import gc\n",
    "\n",
    "# Lightweight NeighborTimeSeriesEncoderLSTM with reduced dimension\n",
    "class NeighborTimeSeriesEncoderLSTM(nn.Module):\n",
    "    def __init__(self, neighbor_count, hidden_dim, output_dim, num_layers=1, dropout=0.0, bidirectional=True):\n",
    "        super(NeighborTimeSeriesEncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.neighbor_count = neighbor_count\n",
    "        self.hidden_dim = hidden_dim  # Reduced internal dimension (8-16)\n",
    "        self.output_dim = output_dim  # Final output dimension\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM layer with reduced hidden dimension\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=neighbor_count,  # One feature per neighbor listing\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Projection layer to map from LSTM output to desired output dimension\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.projection = nn.Linear(lstm_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, seq_len, neighbor_count]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final hidden states from both directions\n",
    "            hidden_forward = hidden[-2, :, :]\n",
    "            hidden_backward = hidden[-1, :, :]\n",
    "            combined = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            # Use only the final hidden state\n",
    "            combined = hidden[-1, :, :]\n",
    "            \n",
    "        # Project to output dimension\n",
    "        projected = self.projection(combined)\n",
    "        return projected\n",
    "\n",
    "# Price transformation function (reused from original code)\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create calculated features (reused from original code)\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# NEW FUNCTION: Build neighbor network for each listing\n",
    "def build_neighbor_network(df, k=5, max_distance=2.0):\n",
    "    \"\"\"\n",
    "    Build a network of neighbors for each listing\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing listings with latitude and longitude\n",
    "        k: Number of neighbors to find for each listing\n",
    "        max_distance: Maximum distance (in km) to consider a neighbor\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping listing_id to list of neighbor listing_ids\n",
    "    \"\"\"\n",
    "    print(f\"Building neighbor network with k={k} neighbors per listing...\")\n",
    "    \n",
    "    # Extract coordinates and listing IDs\n",
    "    coords = df[['latitude', 'longitude']].values\n",
    "    listing_ids = df['listing_id'].values\n",
    "    \n",
    "    # Find k nearest neighbors for each listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k+1, len(coords)))  # +1 to include self\n",
    "    nn.fit(coords)\n",
    "    distances, indices = nn.kneighbors(coords)\n",
    "    \n",
    "    # Create neighbor dictionary\n",
    "    neighbor_dict = {}\n",
    "    \n",
    "    for i, (listing_id, neighbor_indices, neighbor_distances) in enumerate(zip(listing_ids, indices, distances)):\n",
    "        # Convert distance to kilometers (assuming coordinates are in degrees)\n",
    "        # Simple approximation: 1 degree â‰ˆ 111 km\n",
    "        neighbor_distances_km = neighbor_distances * 111.0\n",
    "        \n",
    "        # Filter neighbors by maximum distance and exclude self\n",
    "        valid_neighbors = []\n",
    "        for idx, dist in zip(neighbor_indices[1:], neighbor_distances_km[1:]):  # Skip first (self)\n",
    "            if dist <= max_distance:\n",
    "                valid_neighbors.append(listing_ids[idx])\n",
    "        \n",
    "        neighbor_dict[listing_id] = valid_neighbors\n",
    "    \n",
    "    # Print statistics\n",
    "    neighbor_counts = [len(neighbors) for neighbors in neighbor_dict.values()]\n",
    "    print(f\"Neighbor network built: {len(neighbor_dict)} listings with neighbors\")\n",
    "    print(f\"Average neighbors per listing: {np.mean(neighbor_counts):.2f}\")\n",
    "    print(f\"Min neighbors: {np.min(neighbor_counts)}, Max neighbors: {np.max(neighbor_counts)}\")\n",
    "    \n",
    "    return neighbor_dict\n",
    "\n",
    "# NEW FUNCTION: Create neighbor price history features\n",
    "def create_neighbor_price_history(df, neighbor_dict, seq_length=30, max_neighbors=5):\n",
    "    \"\"\"\n",
    "    Create price history sequences from neighboring listings\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with listing data\n",
    "        neighbor_dict: Dictionary mapping listing_id to list of neighbor listing_ids\n",
    "        seq_length: Number of days in history sequence\n",
    "        max_neighbors: Maximum number of neighbors to include\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with neighbor price history features\n",
    "    \"\"\"\n",
    "    print(f\"Creating neighbor price history with seq_length={seq_length} and max_neighbors={max_neighbors}...\")\n",
    "    \n",
    "    # Create a copy of the dataframe with reset index\n",
    "    result_df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Get all unique dates\n",
    "    all_dates = sorted(result_df['date'].unique())\n",
    "    date_index = {date: i for i, date in enumerate(all_dates)}\n",
    "    \n",
    "    # Create lookup for prices by listing_id and date\n",
    "    price_lookup = {}\n",
    "    for _, row in result_df.iterrows():\n",
    "        if row['listing_id'] not in price_lookup:\n",
    "            price_lookup[row['listing_id']] = {}\n",
    "        price_lookup[row['listing_id']][row['date']] = row['price']\n",
    "    \n",
    "    # Initialize neighbor price columns\n",
    "    for n in range(max_neighbors):\n",
    "        for d in range(seq_length):\n",
    "            result_df[f'neighbor_{n+1}_price_day_{d+1}'] = np.nan\n",
    "    \n",
    "    # For each row in the dataset\n",
    "    for idx, row in result_df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing row {idx}/{len(result_df)}\")\n",
    "            \n",
    "        listing_id = row['listing_id']\n",
    "        current_date = row['date']\n",
    "        \n",
    "        # Get neighbors for this listing\n",
    "        neighbors = neighbor_dict.get(listing_id, [])[:max_neighbors]\n",
    "        \n",
    "        # Pad with zeros if not enough neighbors\n",
    "        while len(neighbors) < max_neighbors:\n",
    "            neighbors.append(None)\n",
    "        \n",
    "        # For each neighbor\n",
    "        for n, neighbor_id in enumerate(neighbors):\n",
    "            if neighbor_id is None:\n",
    "                # No neighbor, fill with zeros\n",
    "                for d in range(seq_length):\n",
    "                    result_df.loc[idx, f'neighbor_{n+1}_price_day_{d+1}'] = 0\n",
    "            else:\n",
    "                # Get price history for this neighbor\n",
    "                for d in range(seq_length):\n",
    "                    history_date_idx = date_index.get(current_date) - d - 1\n",
    "                    if history_date_idx >= 0:\n",
    "                        history_date = all_dates[history_date_idx]\n",
    "                        if neighbor_id in price_lookup and history_date in price_lookup[neighbor_id]:\n",
    "                            result_df.loc[idx, f'neighbor_{n+1}_price_day_{d+1}'] = price_lookup[neighbor_id][history_date]\n",
    "                        else:\n",
    "                            # No data for this neighbor on this date, use current listing's price\n",
    "                            result_df.loc[idx, f'neighbor_{n+1}_price_day_{d+1}'] = row['price']\n",
    "                    else:\n",
    "                        # Date is before the start of our data\n",
    "                        result_df.loc[idx, f'neighbor_{n+1}_price_day_{d+1}'] = row['price']\n",
    "    \n",
    "    # Fill any remaining NaN values with the current listing's price\n",
    "    for n in range(max_neighbors):\n",
    "        for d in range(seq_length):\n",
    "            col_name = f'neighbor_{n+1}_price_day_{d+1}'\n",
    "            mask = result_df[col_name].isna()\n",
    "            if mask.any():\n",
    "                result_df.loc[mask, col_name] = result_df.loc[mask, 'price']\n",
    "    \n",
    "    # Create list of generated features\n",
    "    neighbor_price_features = [f'neighbor_{n+1}_price_day_{d+1}' for n in range(max_neighbors) for d in range(seq_length)]\n",
    "    \n",
    "    return result_df, neighbor_price_features\n",
    "\n",
    "# NEW FUNCTION: Create time series tensor from neighbor prices\n",
    "def prepare_neighbor_time_series_tensors(df, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare time series tensors from neighbor price data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with neighbor price history\n",
    "        max_neighbors: Maximum number of neighbors used\n",
    "        seq_length: Length of the time series\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array with shape [n_samples, seq_length, max_neighbors]\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    # Initialize tensor\n",
    "    neighbor_tensor = np.zeros((n_samples, seq_length, max_neighbors), dtype=np.float32)\n",
    "    \n",
    "    # Fill tensor\n",
    "    for n in range(max_neighbors):\n",
    "        for d in range(seq_length):\n",
    "            col_name = f'neighbor_{n+1}_price_day_{d+1}'\n",
    "            # For each day and neighbor, we want to get all samples\n",
    "            neighbor_tensor[:, seq_length-d-1, n] = df[col_name].values\n",
    "    \n",
    "    return neighbor_tensor\n",
    "\n",
    "# Modified GNN model with neighbor time series\n",
    "class NeighborAwareGNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 spatial_features_dim,\n",
    "                 temporal_features_dim,\n",
    "                 amenity_features_dim,\n",
    "                 max_neighbors=5,\n",
    "                 seq_length=30,\n",
    "                 hidden_dim=64,\n",
    "                 lstm_hidden_dim=8,  # Reduced LSTM hidden dimension\n",
    "                 lstm_layers=1,\n",
    "                 lstm_bidirectional=True,\n",
    "                 dropout=0.3,\n",
    "                 heads=4,\n",
    "                 edge_dim=1):\n",
    "        super(NeighborAwareGNN, self).__init__()\n",
    "        \n",
    "        # Store sequence parameters as instance attributes\n",
    "        self.seq_length = seq_length\n",
    "        self.max_neighbors = max_neighbors\n",
    "        \n",
    "        # For multi-head attention, ensure hidden_dim is divisible by heads\n",
    "        self.h_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # Ensure consistent output dimension\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        \n",
    "        # Spatial feature extraction with GAT\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization for stable training\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Temporal feature processing\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Amenity feature processing\n",
    "        self.amenity_layer1 = nn.Linear(amenity_features_dim, hidden_dim)\n",
    "        self.amenity_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.amenity_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.amenity_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Neighbor time series LSTM encoding\n",
    "        self.neighbor_lstm = NeighborTimeSeriesEncoderLSTM(\n",
    "            neighbor_count=max_neighbors,\n",
    "            hidden_dim=lstm_hidden_dim,  # Reduced dimension (8-16)\n",
    "            output_dim=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=lstm_bidirectional\n",
    "        )\n",
    "        self.lstm_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion with learned weights\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(4, hidden_dim))\n",
    "        self.fusion_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        # Final prediction layers with residual connections\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the PyG data object\n",
    "        x, edge_index, edge_attr, temporal_x, amenity_x, neighbor_time_series = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.temporal_x, \n",
    "            data.amenity_x, data.neighbor_time_series\n",
    "        )\n",
    "        \n",
    "        # First GAT layer with batch normalization and residual\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Process temporal features\n",
    "        temporal_features = F.elu(self.temporal_layer1(temporal_x))\n",
    "        temporal_features = self.temporal_bn1(temporal_features)\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features_res = temporal_features\n",
    "        temporal_features = F.elu(self.temporal_layer2(temporal_features))\n",
    "        temporal_features = self.temporal_bn2(temporal_features)\n",
    "        temporal_features = temporal_features + temporal_features_res  # Residual connection\n",
    "        \n",
    "        # Process amenity features\n",
    "        amenity_features = F.elu(self.amenity_layer1(amenity_x))\n",
    "        amenity_features = self.amenity_bn1(amenity_features)\n",
    "        amenity_features = self.dropout(amenity_features)\n",
    "        amenity_features_res = amenity_features\n",
    "        amenity_features = F.elu(self.amenity_layer2(amenity_features))\n",
    "        amenity_features = self.amenity_bn2(amenity_features)\n",
    "        amenity_features = amenity_features + amenity_features_res  # Residual connection\n",
    "        \n",
    "        # Process neighbor time series with LSTM\n",
    "        # Reshape to [batch_size, seq_length, n_neighbors]\n",
    "        batch_size = neighbor_time_series.shape[0]\n",
    "        neighbor_seq = neighbor_time_series.view(batch_size, self.seq_length, self.max_neighbors)\n",
    "        \n",
    "        # Pass through LSTM encoder\n",
    "        neighbor_features = self.neighbor_lstm(neighbor_seq)\n",
    "        neighbor_features = self.lstm_bn(neighbor_features)\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        normalized_weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        \n",
    "        # Apply weights to each feature type\n",
    "        fused_features = (\n",
    "            spatial_features * normalized_weights[0] +\n",
    "            temporal_features * normalized_weights[1] +\n",
    "            amenity_features * normalized_weights[2] +\n",
    "            neighbor_features * normalized_weights[3] +\n",
    "            self.fusion_bias\n",
    "        )\n",
    "        \n",
    "        # Final prediction with residual connections\n",
    "        out = F.elu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = F.elu(self.fc2(out))\n",
    "        out = self.fc2_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        \n",
    "        price_prediction = self.fc3(out)\n",
    "        \n",
    "        return price_prediction\n",
    "\n",
    "# Function to prepare graph data with neighbor time series\n",
    "def prepare_neighbor_aware_graph(train_data, val_data, spatial_features, temporal_features, \n",
    "                               amenity_features, spatial_scaler, temporal_scaler, amenity_scaler, \n",
    "                               target_scaler, device, k=10, feature_weight=0.3, \n",
    "                               max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare a combined graph with neighbor time series data for prediction\n",
    "    \"\"\"\n",
    "    # Scale the spatial features\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features]).astype(np.float32)\n",
    "    X_val_spatial = spatial_scaler.transform(val_data[spatial_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale the temporal features\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_val_temporal = temporal_scaler.transform(val_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale the amenity features\n",
    "    X_train_amenity = amenity_scaler.transform(train_data[amenity_features]).astype(np.float32)\n",
    "    X_val_amenity = amenity_scaler.transform(val_data[amenity_features]).astype(np.float32)\n",
    "    \n",
    "    # Transform the target variable\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Prepare neighbor time series data\n",
    "    X_train_neighbor = prepare_neighbor_time_series_tensors(\n",
    "        train_data, max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    X_val_neighbor = prepare_neighbor_time_series_tensors(\n",
    "        val_data, max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Scale neighbor time series (using same scaler for all neighbors)\n",
    "    neighbor_scaler = StandardScaler()\n",
    "    orig_shape = X_train_neighbor.shape\n",
    "    X_train_neighbor = neighbor_scaler.fit_transform(X_train_neighbor.reshape(-1, max_neighbors)).reshape(orig_shape)\n",
    "    X_val_neighbor = neighbor_scaler.transform(X_val_neighbor.reshape(-1, max_neighbors)).reshape(X_val_neighbor.shape)\n",
    "    \n",
    "    y_val = target_scaler.transform(val_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Combine train and val features\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_val_spatial])\n",
    "    X_combined_temporal = np.vstack([X_train_temporal, X_val_temporal])\n",
    "    X_combined_amenity = np.vstack([X_train_amenity, X_val_amenity])\n",
    "    X_combined_neighbor = np.vstack([X_train_neighbor, X_val_neighbor])\n",
    "    \n",
    "    # Create combined y with placeholder values for validation\n",
    "    y_combined = np.zeros(len(X_combined_spatial), dtype=np.float32)\n",
    "    y_combined[:len(y_train)] = y_train\n",
    "    \n",
    "    # Build enhanced spatial graph\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph(\n",
    "        train_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in train_data.columns]], \n",
    "        val_data[['latitude', 'longitude'] + [f for f in ['accommodates', 'bedrooms', 'bathrooms'] if f in val_data.columns]], \n",
    "        k=k,\n",
    "        feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Create PyG data object with explicit float32 tensors\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial).to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        edge_attr=edge_attr.to(device),\n",
    "        temporal_x=torch.FloatTensor(X_combined_temporal).to(device),\n",
    "        amenity_x=torch.FloatTensor(X_combined_amenity).to(device),\n",
    "        neighbor_time_series=torch.FloatTensor(X_combined_neighbor.reshape(len(X_combined_spatial), -1)).to(device),\n",
    "        y=torch.FloatTensor(y_combined.reshape(-1, 1)).to(device),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool).to(device),\n",
    "        val_y=torch.FloatTensor(y_val.reshape(-1, 1)).to(device),\n",
    "    )\n",
    "    \n",
    "    # Set masks after creation\n",
    "    data.train_mask[:len(X_train_spatial)] = True\n",
    "    data.val_mask[len(X_train_spatial):] = True\n",
    "    data.val_indices = torch.nonzero(data.val_mask).squeeze().to(device)\n",
    "    \n",
    "    # Store sequence parameters\n",
    "    data.seq_length = seq_length\n",
    "    data.max_neighbors = max_neighbors\n",
    "    \n",
    "    print(f\"Created neighbor-aware graph with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    print(f\"Train nodes: {data.train_mask.sum().item()}, Val nodes: {data.val_mask.sum().item()}\")\n",
    "    \n",
    "    return data, neighbor_scaler\n",
    "\n",
    "# Function to train the neighbor-aware GNN model\n",
    "def train_neighbor_aware_gnn(train_data, val_data, spatial_features, temporal_features, amenity_features, \n",
    "                           max_neighbors=5, seq_length=30, hidden_dim=64, lstm_hidden_dim=8, lstm_layers=1, \n",
    "                           epochs=50, lr=0.001, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GNN model with neighbor time series data\n",
    "    \"\"\"\n",
    "    # Initialize scalers\n",
    "    spatial_scaler = StandardScaler()\n",
    "    temporal_scaler = StandardScaler()\n",
    "    amenity_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "\n",
    "    # Fit scalers on training data\n",
    "    spatial_scaler.fit(train_data[spatial_features])\n",
    "    temporal_scaler.fit(train_data[temporal_features])\n",
    "    amenity_scaler.fit(train_data[amenity_features])\n",
    "    target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare graph data with neighbor time series\n",
    "    graph_data, neighbor_scaler = prepare_neighbor_aware_graph(\n",
    "        train_data, val_data, spatial_features, temporal_features, amenity_features,\n",
    "        spatial_scaler, temporal_scaler, amenity_scaler, target_scaler, device, \n",
    "        max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Initialize model with lightweight LSTM\n",
    "    model = NeighborAwareGNN(\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        amenity_features_dim=len(amenity_features),\n",
    "        max_neighbors=max_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        hidden_dim=hidden_dim,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,  # Lightweight LSTM (8-16 dim)\n",
    "        lstm_layers=lstm_layers,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    # Store history for plotting\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(graph_data)\n",
    "        \n",
    "        # Get outputs for training nodes only\n",
    "        train_out = out[graph_data.train_mask]\n",
    "        train_y = graph_data.y[graph_data.train_mask]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(train_out, train_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            val_out = model(graph_data)[graph_data.val_mask]\n",
    "            val_y = graph_data.val_y\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_out, val_y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(val_y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics on original scale\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Store history\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory management\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, history\n",
    "\n",
    "# Function to make predictions with the neighbor-aware model\n",
    "def predict_with_neighbor_aware_gnn(model, test_data, spatial_features, temporal_features, amenity_features, \n",
    "                                  spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, \n",
    "                                  target_scaler, train_data, device, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained neighbor-aware GNN model\n",
    "    \"\"\"\n",
    "    # Create a copy of test_data to avoid modifying the original\n",
    "    test_data_copy = test_data.copy()\n",
    "    \n",
    "    # Prepare graph data with test data as validation\n",
    "    graph_data, _ = prepare_neighbor_aware_graph(\n",
    "        train_data, test_data_copy, spatial_features, temporal_features, amenity_features,\n",
    "        spatial_scaler, temporal_scaler, amenity_scaler, target_scaler, device, \n",
    "        max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)[graph_data.val_mask]\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # Inverse log transformation\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "    return predictions_orig\n",
    "\n",
    "# Function to evaluate predictions (reused from original code)\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== Neighbor-Aware GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RÂ²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Enhanced run_neighbor_aware_gnn_with_rolling_window_cv function\n",
    "def run_neighbor_aware_gnn_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, output_dir=None, \n",
    "                                                window_size=35, n_splits=5, sample_size=None,\n",
    "                                                max_neighbors=5, seq_length=30, lstm_hidden_dim=8, lstm_layers=1):\n",
    "    \"\"\"\n",
    "    Run neighbor-aware GNN model with rolling window cross-validation\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "\n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "\n",
    "        # Drop legacy price columns if they exist\n",
    "        price_cols_to_remove = ['price_lag_1d', 'simulated_price']\n",
    "        \n",
    "        for col in price_cols_to_remove:\n",
    "            if col in train_data.columns:\n",
    "                print(f\"Dropping {col} column from the dataset\")\n",
    "                train_data = train_data.drop(col, axis=1)\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.75), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.25), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Filter data to include only dates from 7/8/23 till 2/8/24\n",
    "        start_date = pd.to_datetime('2023-07-08')\n",
    "        end_date = pd.to_datetime('2024-02-08')\n",
    "        train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "        \n",
    "        # Apply log transformation to price\n",
    "        train_data = apply_price_transformation(train_data)\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Build neighbor network\n",
    "        neighbor_dict = build_neighbor_network(train_data, k=max_neighbors, max_distance=2.0)\n",
    "        \n",
    "        # Create neighbor price history\n",
    "        print(\"Creating neighbor price history features...\")\n",
    "        train_data, neighbor_price_features = create_neighbor_price_history(\n",
    "            train_data, neighbor_dict, seq_length=seq_length, max_neighbors=max_neighbors\n",
    "        )\n",
    "        \n",
    "        # Define feature groups based on your dataset columns\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Temporal features - using your DTF prefixed features\n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "        amenity_features = [col for col in train_data.columns if col.startswith('has_')]\n",
    "        basic_property_features = ['accommodates', 'bedrooms', 'bathrooms', 'essential_score', 'luxury_score', 'amenity_count', 'bedroom_ratio']\n",
    "        available_basic_features = [f for f in basic_property_features if f in train_data.columns]\n",
    "        amenity_features.extend(available_basic_features)\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "        amenity_features = [f for f in amenity_features if f in train_data.columns]\n",
    "        \n",
    "        # If any feature group is empty, create dummy features\n",
    "        if not amenity_features:\n",
    "            print(\"No amenity features found, creating dummy feature\")\n",
    "            train_data['dummy_amenity'] = 1\n",
    "            amenity_features = ['dummy_amenity']\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"{len(amenity_features)} amenity features, and {len(neighbor_price_features)} neighbor price features\")\n",
    "        \n",
    "        # Get unique dates and ensure they're properly sorted\n",
    "        unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "        \n",
    "        # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "        last_35_days = unique_dates[-window_size:]\n",
    "        \n",
    "        # Define explicit test periods - each 7 days\n",
    "        test_periods = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = i * (window_size // n_splits)\n",
    "            end_idx = start_idx + (window_size // n_splits)\n",
    "            # Make sure we don't go beyond the available data\n",
    "            if end_idx <= len(last_35_days):\n",
    "                test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "        \n",
    "        # Adjust n_splits if we couldn't create enough test periods\n",
    "        n_splits = len(test_periods)\n",
    "        \n",
    "        print(f\"Created {n_splits} test periods:\")\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Storage for results\n",
    "        cv_results = []\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        split_metrics = []\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Run time series cross-validation using our explicit test periods\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "            \n",
    "            # Define training period: everything before test_start\n",
    "            train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "            train_end_date = train_end.date()\n",
    "            \n",
    "            print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "            print(f\"Testing period: {test_start} to {test_end}\")\n",
    "            \n",
    "            # Split by date first\n",
    "            train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "            test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "            \n",
    "            date_filtered_train = train_data[train_date_mask]\n",
    "            date_filtered_test = train_data[test_date_mask]\n",
    "            \n",
    "            # Now further split by listing IDs\n",
    "            train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "            test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "            \n",
    "            split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "            split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "            \n",
    "            print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "            print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "            \n",
    "            # Check if we have enough data for this split\n",
    "            if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "                print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Split train data into train and validation\n",
    "            unique_train_listings = split_train_data['listing_id'].unique()\n",
    "            train_listings, val_listings = train_test_split(\n",
    "                unique_train_listings, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            train_subset = split_train_data[split_train_data['listing_id'].isin(train_listings)].copy()\n",
    "            val_subset = split_train_data[split_train_data['listing_id'].isin(val_listings)].copy()\n",
    "            \n",
    "            # Manage memory before training\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train neighbor-aware GNN model for this split\n",
    "            try:\n",
    "                print(f\"\\n----- Training Neighbor-Aware GNN Model (Split {i+1}) -----\")\n",
    "                gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, _ = train_neighbor_aware_gnn(\n",
    "                    train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "                    max_neighbors=max_neighbors, seq_length=seq_length, hidden_dim=64, \n",
    "                    lstm_hidden_dim=lstm_hidden_dim, lstm_layers=lstm_layers,\n",
    "                    epochs=100, lr=0.001, device=device\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test data\n",
    "                print(f\"\\n----- Evaluating Neighbor-Aware GNN on Test Data (Split {i+1}) -----\")\n",
    "                test_predictions = predict_with_neighbor_aware_gnn(\n",
    "                    gnn_model, split_test_data, spatial_features, temporal_features, amenity_features,\n",
    "                    spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler,\n",
    "                    target_scaler, train_subset, device,\n",
    "                    max_neighbors=max_neighbors, seq_length=seq_length\n",
    "                )\n",
    "                \n",
    "                # Get actual test values (original scale)\n",
    "                test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "                \n",
    "                # Evaluate predictions\n",
    "                metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "                \n",
    "                print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, RÂ²: {metrics['r2']:.4f}\")\n",
    "                \n",
    "                # Store results for this split\n",
    "                split_results = pd.DataFrame({\n",
    "                    'split': i,\n",
    "                    'date': split_test_data['date'],\n",
    "                    'listing_id': split_test_data['listing_id'],\n",
    "                    'price': test_actuals,\n",
    "                    'predicted': test_predictions.flatten(),\n",
    "                    'error': test_actuals - test_predictions.flatten(),\n",
    "                    'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                    'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "                })\n",
    "                \n",
    "                cv_results.append(split_results)\n",
    "                all_predictions.extend(test_predictions.flatten())\n",
    "                all_targets.extend(test_actuals)\n",
    "                \n",
    "                # Save model for this split if output_dir is provided\n",
    "                if output_dir:\n",
    "                    model_path = os.path.join(output_dir, f'neighbor_gnn_model_split_{i+1}.pt')\n",
    "                    torch.save(gnn_model.state_dict(), model_path)\n",
    "                    print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "                \n",
    "                # Store metrics for this split\n",
    "                split_metrics.append({\n",
    "                    'split': i,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mae': metrics['mae'],\n",
    "                    'r2': metrics['r2'],\n",
    "                    'mape': metrics['mape'],\n",
    "                    'n_samples': len(test_actuals)\n",
    "                })\n",
    "                \n",
    "                # Memory management after each split\n",
    "                del gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler\n",
    "                del test_predictions, split_train_data, split_test_data, train_subset, val_subset\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in split {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if not cv_results:\n",
    "            print(\"No valid splits completed. Check your data and parameters.\")\n",
    "            return None\n",
    "                \n",
    "        all_results = pd.concat(cv_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        all_targets_array = np.array(all_targets)\n",
    "        all_predictions_array = np.array(all_predictions)\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "            'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "            'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "            'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "        }\n",
    "        \n",
    "        # Calculate daily metrics\n",
    "        all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        daily_metrics = []\n",
    "        for day, group in all_results.groupby('date_str'):\n",
    "            y_true_day = group['price']\n",
    "            y_pred_day = group['predicted']\n",
    "            \n",
    "            daily_metrics.append({\n",
    "                'date': day,\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "                'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "                'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "                'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "                'n_samples': len(y_true_day)\n",
    "            })\n",
    "        \n",
    "        daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "        daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "        daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "        \n",
    "        split_metrics_df = pd.DataFrame(split_metrics)\n",
    "        \n",
    "        # Create a results dictionary\n",
    "        evaluation_results = {\n",
    "            'overall_metrics': overall_metrics,\n",
    "            'split_metrics': split_metrics_df,\n",
    "            'daily_metrics': daily_metrics_df,\n",
    "            'all_results': all_results,\n",
    "            'train_listings': len(train_listing_ids),\n",
    "            'test_listings': len(test_listing_ids)\n",
    "        }\n",
    "        \n",
    "        # Save results if output directory is provided\n",
    "        if output_dir:\n",
    "            # Save all results\n",
    "            results_file = os.path.join(output_dir, 'neighbor_gnn_rolling_window_results.csv')\n",
    "            all_results.to_csv(results_file, index=False)\n",
    "            print(f\"Results saved to {results_file}\")\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics_file = os.path.join(output_dir, 'neighbor_gnn_rolling_window_metrics.csv')\n",
    "            daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "            print(f\"Daily metrics saved to {metrics_file}\")\n",
    "            \n",
    "            # Save summary\n",
    "            with open(os.path.join(output_dir, 'neighbor_gnn_cv_summary.txt'), 'w') as f:\n",
    "                f.write(f\"Neighbor-Aware GNN Rolling Window CV Model Summary\\n\")\n",
    "                f.write(f\"==================================================\\n\\n\")\n",
    "                f.write(f\"Window size: {window_size} days\\n\")\n",
    "                f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "                f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "                f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "                f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "                f.write(f\"Max neighbors: {max_neighbors}\\n\")\n",
    "                f.write(f\"Sequence length: {seq_length}\\n\")\n",
    "                f.write(f\"LSTM hidden dimension: {lstm_hidden_dim}\\n\")\n",
    "                f.write(f\"LSTM layers: {lstm_layers}\\n\\n\")\n",
    "                f.write(f\"Overall Metrics:\\n\")\n",
    "                for k, v in overall_metrics.items():\n",
    "                    f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== NEIGHBOR-AWARE GNN ROLLING WINDOW CV SUMMARY =====\")\n",
    "        print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "        \n",
    "        print(\"\\n=== Overall Metrics ===\")\n",
    "        print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "        print(f\"RÂ²: {overall_metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "        \n",
    "        print(\"\\n=== Split Performance ===\")\n",
    "        print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "        \n",
    "        # Return evaluation results\n",
    "        return evaluation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in rolling window CV: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to plot results (reused from original code)\n",
    "def plot_gnn_results(y_true, y_pred, history=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot GNN prediction results\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    plt.annotate(f'Correlation: {corr:.4f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Error Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_true - y_pred\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add mean, median error\n",
    "    mean_error = np.mean(errors)\n",
    "    median_error = np.median(errors)\n",
    "    plt.axvline(mean_error, color='g', linestyle='-', label=f'Mean: {mean_error:.2f}')\n",
    "    plt.axvline(median_error, color='b', linestyle='-', label=f'Median: {median_error:.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: Error vs Actual Price\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_true, errors, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title('Error vs Actual Price')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    # Add lowess trend line if statsmodels is available\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        lowess = sm.nonparametric.lowess(errors, y_true, frac=0.3)\n",
    "        plt.plot(lowess[:, 0], lowess[:, 1], 'r-', linewidth=2, label='Trend')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Plot 4: Percentage Error Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pct_errors = np.abs(errors / (y_true + 1e-8)) * 100\n",
    "    plt.hist(pct_errors, bins=50, alpha=0.7)\n",
    "    \n",
    "    # Mark median and mean\n",
    "    median_pct = np.median(pct_errors)\n",
    "    mean_pct = np.mean(pct_errors)\n",
    "    plt.axvline(median_pct, color='r', linestyle='--', label=f'Median: {median_pct:.2f}%')\n",
    "    plt.axvline(mean_pct, color='g', linestyle='--', label=f'Mean: {mean_pct:.2f}%')\n",
    "    \n",
    "    plt.title('Percentage Error Distribution')\n",
    "    plt.xlabel('Percentage Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'neighbor_gnn_results.png'))\n",
    "        print(f\"Plot saved to {os.path.join(output_dir, 'neighbor_gnn_results.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Main function to run the neighbor-aware model\n",
    "def run_neighbor_aware_gnn(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None, \n",
    "                         max_neighbors=5, seq_length=30, lstm_hidden_dim=8, lstm_layers=1, \n",
    "                         hidden_dim=64, lr=0.001, epochs=50):\n",
    "    \"\"\"\n",
    "    Run neighbor-aware GNN model without using the listing's own price history\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if not exists\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "\n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.8), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.2), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime if needed\n",
    "        if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "            train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Split data into train and test based on listing IDs\n",
    "        train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "        test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        train_df = train_data[train_mask].copy()\n",
    "        test_df = train_data[test_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Build neighbor network\n",
    "        print(\"Building neighbor network...\")\n",
    "        neighbor_dict = build_neighbor_network(train_df, k=max_neighbors, max_distance=2.0)\n",
    "        \n",
    "        # Create neighbor price history features\n",
    "        print(\"Creating neighbor price history features...\")\n",
    "        train_df, neighbor_price_features = create_neighbor_price_history(\n",
    "            train_df, neighbor_dict, seq_length=seq_length, max_neighbors=max_neighbors\n",
    "        )\n",
    "        test_df, _ = create_neighbor_price_history(\n",
    "            test_df, neighbor_dict, seq_length=seq_length, max_neighbors=max_neighbors\n",
    "        )\n",
    "\n",
    "        # Define feature groups based on dataset columns\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        # Temporal features - using DTF prefixed features\n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Amenity features - all has_* columns plus accommodates, bedrooms, bathrooms\n",
    "        amenity_features = [col for col in train_df.columns if col.startswith('has_')]\n",
    "        basic_property_features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "        available_basic_features = [f for f in basic_property_features if f in train_df.columns]\n",
    "        amenity_features.extend(available_basic_features)\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "        amenity_features = [f for f in amenity_features if f in train_df.columns]\n",
    "        \n",
    "        # If any feature group is empty, create dummy features\n",
    "        if not amenity_features:\n",
    "            print(\"No amenity features found, creating dummy feature\")\n",
    "            train_df['dummy_amenity'] = 1\n",
    "            test_df['dummy_amenity'] = 1\n",
    "            amenity_features = ['dummy_amenity']\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(temporal_features)} temporal features, \"\n",
    "              f\"{len(amenity_features)} amenity features, and {max_neighbors} neighbors with {seq_length} days of history\")\n",
    "        \n",
    "        # Apply log transformation to prices\n",
    "        train_df = apply_price_transformation(train_df)\n",
    "        test_df = apply_price_transformation(test_df)\n",
    "        \n",
    "        # Split train data into train and validation\n",
    "        unique_train_listings = train_df['listing_id'].unique()\n",
    "        train_listings, val_listings = train_test_split(\n",
    "            unique_train_listings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_subset = train_df[train_df['listing_id'].isin(train_listings)].copy()\n",
    "        val_subset = train_df[train_df['listing_id'].isin(val_listings)].copy()\n",
    "        \n",
    "        print(f\"Train subset: {len(train_subset)} rows, {len(train_listings)} listings\")\n",
    "        print(f\"Validation subset: {len(val_subset)} rows, {len(val_listings)} listings\")\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Memory management before training\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train neighbor-aware GNN model\n",
    "        print(\"\\n===== Training Neighbor-Aware GNN Model =====\")\n",
    "        gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, history = train_neighbor_aware_gnn(\n",
    "            train_subset, val_subset, spatial_features, temporal_features, amenity_features,\n",
    "            max_neighbors=max_neighbors, seq_length=seq_length, hidden_dim=hidden_dim, \n",
    "            lstm_hidden_dim=lstm_hidden_dim, lstm_layers=lstm_layers, \n",
    "            epochs=epochs, lr=lr, device=device\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation RMSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['val_rmse'], label='Validation RMSE')\n",
    "        plt.title('Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        \n",
    "        # Plot validation MAE\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        # Plot learning rate\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['lr'], label='Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('LR')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'neighbor_training_history.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        print(\"\\n===== Evaluating Neighbor-Aware GNN on Test Data =====\")\n",
    "        test_predictions = predict_with_neighbor_aware_gnn(\n",
    "            gnn_model, test_df, spatial_features, temporal_features, amenity_features,\n",
    "            spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler,\n",
    "            target_scaler, train_subset, device,\n",
    "            max_neighbors=max_neighbors, seq_length=seq_length\n",
    "        )\n",
    "        \n",
    "        # Get actual test values (original scale)\n",
    "        test_actuals = test_df['original_price'].values if 'original_price' in test_df.columns else test_df['price'].values\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        test_metrics = evaluate_gnn_predictions(test_actuals, test_predictions.flatten(), print_results=True)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_gnn_results(test_actuals, test_predictions.flatten(), history, output_dir)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        if output_dir:\n",
    "            torch.save(gnn_model.state_dict(), os.path.join(output_dir, 'neighbor_gnn_model.pt'))\n",
    "            torch.save({\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'amenity_scaler': amenity_scaler,\n",
    "                'neighbor_scaler': neighbor_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'max_neighbors': max_neighbors,\n",
    "                'seq_length': seq_length,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim\n",
    "            }, os.path.join(output_dir, 'neighbor_scalers.pt'))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "            # Save test predictions\n",
    "            test_results = pd.DataFrame({\n",
    "                'listing_id': test_df['listing_id'].values,\n",
    "                'date': test_df['date'].values,\n",
    "                'actual': test_actuals,\n",
    "                'predicted': test_predictions.flatten(),\n",
    "                'error': test_actuals - test_predictions.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - test_predictions.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - test_predictions.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            test_results.to_csv(os.path.join(output_dir, 'neighbor_test_predictions.csv'), index=False)\n",
    "            print(f\"Test predictions saved to {os.path.join(output_dir, 'neighbor_test_predictions.csv')}\")\n",
    "        \n",
    "        # Return model, scalers and metrics\n",
    "        return gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, test_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in neighbor-aware GNN model training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to compare different neighbor configurations\n",
    "def compare_neighbor_configurations(train_path, train_ids_path, test_ids_path, output_dir=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Compare different neighbor configurations for the neighbor-aware GNN\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define configurations to test\n",
    "    configs = [\n",
    "        {\"max_neighbors\": 3, \"lstm_hidden_dim\": 8, \"name\": \"3_neighbors_8dim\"},\n",
    "        {\"max_neighbors\": 5, \"lstm_hidden_dim\": 8, \"name\": \"5_neighbors_8dim\"},\n",
    "        {\"max_neighbors\": 5, \"lstm_hidden_dim\": 16, \"name\": \"5_neighbors_16dim\"},\n",
    "        {\"max_neighbors\": 10, \"lstm_hidden_dim\": 8, \"name\": \"10_neighbors_8dim\"}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n===== Testing Configuration: {config['name']} =====\")\n",
    "        \n",
    "        # Set parameters for this configuration\n",
    "        max_neighbors = config[\"max_neighbors\"]\n",
    "        lstm_hidden_dim = config[\"lstm_hidden_dim\"]\n",
    "        seq_length = 30  # Keep sequence length constant\n",
    "        \n",
    "        # Create config-specific output directory\n",
    "        config_dir = os.path.join(output_dir, config[\"name\"]) if output_dir else None\n",
    "        if config_dir:\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "        \n",
    "        # Run model with these parameters\n",
    "        model_result = run_neighbor_aware_gnn(\n",
    "            train_path=train_path,\n",
    "            train_ids_path=train_ids_path,\n",
    "            test_ids_path=test_ids_path,\n",
    "            output_dir=config_dir,\n",
    "            sample_size=sample_size,\n",
    "            max_neighbors=max_neighbors,\n",
    "            lstm_hidden_dim=lstm_hidden_dim,\n",
    "            seq_length=seq_length,\n",
    "            epochs=30  # Reduced epochs for faster comparison\n",
    "        )\n",
    "        \n",
    "        # Extract metrics if model ran successfully\n",
    "        if model_result:\n",
    "            _, _, _, _, _, _, metrics = model_result\n",
    "            \n",
    "            # Add configuration details to metrics\n",
    "            config_metrics = {\n",
    "                'config_name': config['name'],\n",
    "                'max_neighbors': max_neighbors,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'r2': metrics['r2'],\n",
    "                'mape': metrics['mape']\n",
    "            }\n",
    "            \n",
    "            results.append(config_metrics)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    if results:\n",
    "        comparison_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save comparison if output_dir is provided\n",
    "        if output_dir:\n",
    "            comparison_df.to_csv(os.path.join(output_dir, 'neighbor_config_comparison.csv'), index=False)\n",
    "            print(f\"Configuration comparison saved to {os.path.join(output_dir, 'neighbor_config_comparison.csv')}\")\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n===== Configuration Comparison =====\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Create comparison plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # RMSE comparison\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.barplot(x='config_name', y='rmse', data=comparison_df)\n",
    "        plt.title('RMSE by Configuration')\n",
    "        plt.xlabel('Configuration')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # RÂ² comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.barplot(x='config_name', y='r2', data=comparison_df)\n",
    "        plt.title('RÂ² by Configuration')\n",
    "        plt.xlabel('Configuration')\n",
    "        plt.ylabel('RÂ²')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'neighbor_config_comparison.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df\n",
    "    else:\n",
    "        print(\"No valid configurations completed. Check your data and parameters.\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\topic2_transformed.csv\"\n",
    "    train_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\train_ids.txt\"\n",
    "    test_ids_path = r\"C:\\Users\\mvk\\Documents\\DATA_school\\thesis\\Subset\\top_price_changers_subset\\test_ids.txt\"\n",
    "    # Output directory\n",
    "    output_dir = \"./output/neighbor_gnn_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters for neighbor-aware model\n",
    "    max_neighbors = 5        # Number of neighbors to use\n",
    "    seq_length = 30          # Sequence length for historical data\n",
    "    lstm_hidden_dim = 8      # Reduced LSTM hidden dimension\n",
    "    lstm_layers = 1          # Number of LSTM layers\n",
    "    hidden_dim = 64          # Hidden dimension size for rest of model\n",
    "    lr = 0.001               # Learning rate \n",
    "    epochs = 20              # Maximum number of epochs\n",
    "    \n",
    "    # Choose between different run modes\n",
    "    run_mode = \"rolling_window\"  # Options: \"single\", \"rolling_window\", \"compare\", \"all\"\n",
    "    \n",
    "    try:\n",
    "        if run_mode == \"single\":\n",
    "            # Run single model training with neighbor-aware GNN\n",
    "            result_tuple = run_neighbor_aware_gnn(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                hidden_dim=hidden_dim,\n",
    "                lr=lr,\n",
    "                epochs=epochs,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            \n",
    "            if result_tuple:\n",
    "                gnn_model, spatial_scaler, temporal_scaler, amenity_scaler, neighbor_scaler, target_scaler, test_metrics = result_tuple\n",
    "                print(\"Neighbor-aware GNN model training completed successfully!\")\n",
    "                \n",
    "                # Print summary of model characteristics\n",
    "                print(\"\\n===== Model Summary =====\")\n",
    "                total_param = sum(p.numel() for p in gnn_model.parameters())\n",
    "                lstm_param = sum(p.numel() for name, p in gnn_model.named_parameters() if 'lstm' in name)\n",
    "                print(f\"Total model parameters: {total_param:,}\")\n",
    "                print(f\"LSTM parameters: {lstm_param:,}\")\n",
    "                print(f\"LSTM parameters as % of total: {lstm_param/total_param*100:.2f}%\")\n",
    "                \n",
    "        elif run_mode == \"rolling_window\":\n",
    "            # Run with rolling window cross-validation\n",
    "            results = run_neighbor_aware_gnn_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            print(\"Neighbor-aware GNN with rolling window CV completed successfully!\")\n",
    "            \n",
    "        elif run_mode == \"compare\":\n",
    "            # Run comparison between different neighbor configurations\n",
    "            comparison = compare_neighbor_configurations(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=output_dir,\n",
    "                sample_size=200  # Use small sample for faster comparison\n",
    "            )\n",
    "            print(\"Configuration comparison completed successfully!\")\n",
    "            \n",
    "        elif run_mode == \"all\":\n",
    "            # Run both single model and cross-validation\n",
    "            print(\"\\n===== RUNNING SINGLE MODEL TRAINING =====\")\n",
    "            result_tuple = run_neighbor_aware_gnn(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=os.path.join(output_dir, \"single_model\"),\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                hidden_dim=hidden_dim,\n",
    "                lr=lr,\n",
    "                epochs=epochs,\n",
    "                sample_size=None\n",
    "            )\n",
    "            \n",
    "            print(\"\\n===== RUNNING ROLLING WINDOW CROSS-VALIDATION =====\")\n",
    "            results = run_neighbor_aware_gnn_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                output_dir=os.path.join(output_dir, \"cv_results\"),\n",
    "                window_size=35,\n",
    "                n_splits=5,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                lstm_layers=lstm_layers,\n",
    "                sample_size=None\n",
    "            )\n",
    "            \n",
    "            print(\"All neighbor-aware GNN experiments completed successfully!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running neighbor-aware GNN model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
