{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_neighbor_data(neighbor_csv_path):\n",
    "    \"\"\"\n",
    "    Load neighbor relationships from CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Loading neighbor data from {neighbor_csv_path}\")\n",
    "    neighbor_df = pd.read_csv(neighbor_csv_path)\n",
    "    \n",
    "    # Create a dictionary mapping test_listing_id to its neighbors\n",
    "    neighbor_dict = {}\n",
    "    \n",
    "    for _, row in neighbor_df.iterrows():\n",
    "        test_id = row['test_listing_id']\n",
    "        neighbor_id = row['neighbor_listing_id']\n",
    "        rank = row['rank']\n",
    "        distance = row['distance']\n",
    "        \n",
    "        if test_id not in neighbor_dict:\n",
    "            neighbor_dict[test_id] = []\n",
    "        \n",
    "        neighbor_dict[test_id].append({\n",
    "            'neighbor_id': neighbor_id,\n",
    "            'rank': rank,\n",
    "            'distance': distance\n",
    "        })\n",
    "    \n",
    "    # Sort neighbors by rank for each test listing\n",
    "    for test_id in neighbor_dict:\n",
    "        neighbor_dict[test_id] = sorted(neighbor_dict[test_id], key=lambda x: x['rank'])\n",
    "    \n",
    "    print(f\"Loaded neighbor data for {len(neighbor_dict)} test listings\")\n",
    "    return neighbor_dict\n",
    "\n",
    "def extract_price_history(listing_data, date, seq_length=30):\n",
    "    \"\"\"\n",
    "    Extract price history for a listing up to a specific date\n",
    "    \"\"\"\n",
    "    previous_data = listing_data[listing_data['date'] < date].sort_values('date', ascending=False)\n",
    "    \n",
    "    # Extract prices\n",
    "    price_history = []\n",
    "    for _, row in previous_data.head(seq_length).iterrows():\n",
    "        price_history.append(row['price'])\n",
    "    \n",
    "    # Pad if needed\n",
    "    if len(price_history) < seq_length:\n",
    "        padding = [price_history[-1] if price_history else 0] * (seq_length - len(price_history))\n",
    "        price_history.extend(padding)\n",
    "    \n",
    "    # Keep only the most recent seq_length prices and reverse to chronological order\n",
    "    price_history = price_history[:seq_length]\n",
    "    price_history.reverse()\n",
    "    \n",
    "    return price_history\n",
    "\n",
    "def prepare_neighbor_data_batch(test_data, train_data, neighbor_dict, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare batched neighbor data for all test instances\n",
    "    \"\"\"\n",
    "    print(\"Preparing neighbor data batch...\")\n",
    "    \n",
    "    # Create lookup dictionary for train listings\n",
    "    train_listings_dict = {}\n",
    "    for listing_id in train_data['listing_id'].unique():\n",
    "        listing_data = train_data[train_data['listing_id'] == listing_id].sort_values('date')\n",
    "        train_listings_dict[listing_id] = listing_data\n",
    "    \n",
    "    # Initialize arrays to store neighbor histories and masks\n",
    "    test_size = len(test_data)\n",
    "    neighbor_histories = np.zeros((test_size, max_neighbors, seq_length), dtype=np.float32)\n",
    "    neighbor_masks = np.zeros((test_size, max_neighbors), dtype=bool)\n",
    "    \n",
    "    # Process each test instance\n",
    "    for idx, (_, test_row) in enumerate(test_data.iterrows()):\n",
    "        test_id = test_row['listing_id']\n",
    "        test_date = test_row['date']\n",
    "        \n",
    "        if test_id not in neighbor_dict:\n",
    "            continue  # Skip if no neighbors found\n",
    "        \n",
    "        # Get neighbors for this test listing\n",
    "        neighbors = neighbor_dict[test_id][:max_neighbors]\n",
    "        \n",
    "        # Process each neighbor\n",
    "        for n_idx, neighbor in enumerate(neighbors):\n",
    "            if n_idx >= max_neighbors:\n",
    "                break\n",
    "                \n",
    "            neighbor_id = neighbor['neighbor_id']\n",
    "            \n",
    "            # Only use neighbors from the training set\n",
    "            if neighbor_id in train_listings_dict:\n",
    "                neighbor_data = train_listings_dict[neighbor_id]\n",
    "                \n",
    "                # Extract price history\n",
    "                price_history = extract_price_history(neighbor_data, test_date, seq_length)\n",
    "                \n",
    "                # Store data\n",
    "                neighbor_histories[idx, n_idx] = price_history\n",
    "                neighbor_masks[idx, n_idx] = True\n",
    "    \n",
    "    print(f\"Prepared neighbor data for {test_size} test instances\")\n",
    "    return neighbor_histories, neighbor_masks\n",
    "\n",
    "def prepare_data_for_neighbor_lstm(train_data, test_data, neighbor_dict, property_features, \n",
    "                                 temporal_features, property_scaler=None, temporal_scaler=None, \n",
    "                                 target_scaler=None, max_neighbors=5, seq_length=30):\n",
    "    \"\"\"\n",
    "    Prepare data for the NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for NeighborBasedLSTM...\")\n",
    "    \n",
    "    # Initialize or use provided scalers\n",
    "    if property_scaler is None:\n",
    "        property_scaler = StandardScaler()\n",
    "        property_scaler.fit(train_data[property_features])\n",
    "    \n",
    "    if temporal_scaler is None:\n",
    "        temporal_scaler = StandardScaler()\n",
    "        temporal_scaler.fit(train_data[temporal_features])\n",
    "    \n",
    "    if target_scaler is None:\n",
    "        target_scaler = StandardScaler()\n",
    "        target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Scale property features\n",
    "    X_train_property = property_scaler.transform(train_data[property_features]).astype(np.float32)\n",
    "    X_test_property = property_scaler.transform(test_data[property_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale temporal features\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_test_temporal = temporal_scaler.transform(test_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    # Prepare neighbor histories for test data\n",
    "    neighbor_histories, neighbor_masks = prepare_neighbor_data_batch(\n",
    "        test_data, train_data, neighbor_dict, max_neighbors, seq_length\n",
    "    )\n",
    "    \n",
    "    # Scale the target variable\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    y_test = target_scaler.transform(test_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Create data objects\n",
    "    train_data_obj = Data(\n",
    "        property_features=torch.FloatTensor(X_train_property),\n",
    "        temporal_features=torch.FloatTensor(X_train_temporal),\n",
    "        y=torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    )\n",
    "    \n",
    "    test_data_obj = Data(\n",
    "        property_features=torch.FloatTensor(X_test_property),\n",
    "        temporal_features=torch.FloatTensor(X_test_temporal),\n",
    "        neighbor_histories=torch.FloatTensor(neighbor_histories),\n",
    "        neighbor_mask=torch.BoolTensor(neighbor_masks),\n",
    "        y=torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "    )\n",
    "    \n",
    "    return train_data_obj, test_data_obj, property_scaler, temporal_scaler, target_scaler\n",
    "\n",
    "def predict_with_neighbor_lstm(model, test_data, train_data, neighbor_dict, property_features, \n",
    "                             temporal_features, property_scaler, temporal_scaler, target_scaler,\n",
    "                             max_neighbors=5, seq_length=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained NeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Predicting with NeighborBasedLSTM Model =====\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    _, test_data_obj, _, _, _ = prepare_data_for_neighbor_lstm(\n",
    "        train_data, test_data, neighbor_dict, property_features, temporal_features,\n",
    "        property_scaler, temporal_scaler, target_scaler,\n",
    "        max_neighbors=max_neighbors, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    test_data_obj = test_data_obj.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_data_obj)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        # If log-transformed, apply inverse\n",
    "        predictions_orig = np.expm1(predictions_np)\n",
    "    \n",
    "    return predictions_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate predictions\n",
    "def evaluate_gnn_predictions(y_true, y_pred, print_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate GNN predictions using multiple metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_results:\n",
    "        print(\"=== GNN Model Evaluation ===\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def apply_price_transformation(train_data, inverse=False):\n",
    "    \"\"\"\n",
    "    Apply log transformation to price data or inverse the transformation\n",
    "    \"\"\"\n",
    "    df = train_data.copy()\n",
    "    \n",
    "    if not inverse:\n",
    "        # Apply log transformation\n",
    "        print(\"Applying log transformation to price data\")\n",
    "        df['original_price'] = df['price']  # Store original price\n",
    "        df['price'] = np.log1p(df['price'])  # log1p to handle zero values\n",
    "    else:\n",
    "        # Inverse transform\n",
    "        print(\"Inverting log transformation for predictions\")\n",
    "        df['price'] = np.expm1(df['price'])  # expm1 is the inverse of log1p\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_calculated_features(df):\n",
    "    \"\"\"\n",
    "    Adapt calculated features to work with provided dataset columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Bedroom ratio\n",
    "    if 'bedrooms' in df_copy.columns and 'accommodates' in df_copy.columns:\n",
    "        df_copy['bedroom_ratio'] = df_copy['bedrooms'] / df_copy['accommodates'].clip(lower=1)\n",
    "    \n",
    "    # Count amenities\n",
    "    amenity_columns = df_copy.filter(like='has_').columns\n",
    "    if len(amenity_columns) > 0:\n",
    "        df_copy['amenity_count'] = df_copy[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Luxury score - use specific amenities from your dataset\n",
    "    luxury_amenities = ['has_hot_water', 'has_hair_dryer', 'has_dedicated_workspace', \n",
    "                         'has_tv', 'has_wifi', 'has_shampoo']\n",
    "    available_luxury = [col for col in luxury_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_luxury:\n",
    "        df_copy['luxury_score'] = df_copy[available_luxury].sum(axis=1) / len(available_luxury)\n",
    "    else:\n",
    "        df_copy['luxury_score'] = 0\n",
    "    \n",
    "    # Essential score - basic amenities that are essential\n",
    "    essential_amenities = ['has_essentials', 'has_bed_linens', 'has_kitchen', \n",
    "                           'has_smoke_alarm', 'has_heating']\n",
    "    available_essential = [col for col in essential_amenities if col in df_copy.columns]\n",
    "    \n",
    "    if available_essential:\n",
    "        df_copy['essential_score'] = df_copy[available_essential].sum(axis=1) / len(available_essential)\n",
    "    else:\n",
    "        df_copy['essential_score'] = 0\n",
    "    \n",
    "    # Price volatility features based on rolling statistics\n",
    "    if all(col in df_copy.columns for col in ['rolling_max_7d', 'rolling_min_7d']):\n",
    "        df_copy['price_range_7d'] = df_copy['rolling_max_7d'] - df_copy['rolling_min_7d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_14d', 'rolling_min_14d']):\n",
    "        df_copy['price_range_14d'] = df_copy['rolling_max_14d'] - df_copy['rolling_min_14d']\n",
    "    \n",
    "    if all(col in df_copy.columns for col in ['rolling_max_30d', 'rolling_min_30d']):\n",
    "        df_copy['price_range_30d'] = df_copy['rolling_max_30d'] - df_copy['rolling_min_30d']\n",
    "    \n",
    "    # Fill any NaN values that might have been created\n",
    "    numeric_cols = df_copy.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().any():\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def compare_lstm_dimensions(train_path, train_ids_path, test_ids_path, neighbor_csv_path,\n",
    "                          output_dir=None, max_neighbors=5, seq_length=30, sample_size=None):\n",
    "    \"\"\"\n",
    "    Compare LSTM models with different hidden dimensions (8 vs 16)\n",
    "    \"\"\"\n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set parameters for each model\n",
    "    lstm_hidden_dim_1 = 8   # First LSTM hidden dimension\n",
    "    lstm_hidden_dim_2 = 16  # Second LSTM hidden dimension\n",
    "    \n",
    "    # Run first model\n",
    "    print(\"\\n===== Running NeighborBasedLSTM Model (8 hidden dim) =====\")\n",
    "    result_1 = run_neighbor_lstm_model(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        neighbor_csv_path=neighbor_csv_path,\n",
    "        output_dir=os.path.join(output_dir, 'lstm_dim_8') if output_dir else None,\n",
    "        lstm_hidden_dim=lstm_hidden_dim_1,\n",
    "        max_neighbors=max_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    # Run second model\n",
    "    print(\"\\n===== Running NeighborBasedLSTM Model (16 hidden dim) =====\")\n",
    "    result_2 = run_neighbor_lstm_model(\n",
    "        train_path=train_path,\n",
    "        train_ids_path=train_ids_path,\n",
    "        test_ids_path=test_ids_path,\n",
    "        neighbor_csv_path=neighbor_csv_path,\n",
    "        output_dir=os.path.join(output_dir, 'lstm_dim_16') if output_dir else None,\n",
    "        lstm_hidden_dim=lstm_hidden_dim_2,\n",
    "        max_neighbors=max_neighbors,\n",
    "        seq_length=seq_length,\n",
    "        epochs=30,  # Reduced epochs for faster comparison\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    # Check if both models ran successfully\n",
    "    if result_1 and result_2:\n",
    "        # Extract model performance\n",
    "        _, _, _, _, metrics_1 = result_1\n",
    "        _, _, _, _, metrics_2 = result_2\n",
    "        \n",
    "        # Compare metrics\n",
    "        print(\"\\n===== Model Comparison =====\")\n",
    "        metrics = ['rmse', 'mae', 'r2', 'mape']\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Metric': metrics,\n",
    "            f'LSTM (dim={lstm_hidden_dim_1})': [metrics_1[m] for m in metrics],\n",
    "            f'LSTM (dim={lstm_hidden_dim_2})': [metrics_2[m] for m in metrics]\n",
    "        })\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        comparison_df['Improvement %'] = [\n",
    "            ((metrics_2[m] - metrics_1[m]) / metrics_1[m] * 100) if m not in ['r2'] else\n",
    "            ((metrics_2[m] - metrics_1[m]) * 100) for m in metrics\n",
    "        ]\n",
    "        \n",
    "        print(comparison_df)\n",
    "        \n",
    "        # Save comparison if output_dir is provided\n",
    "        if output_dir:\n",
    "            comparison_df.to_csv(os.path.join(output_dir, 'lstm_dim_comparison.csv'), index=False)\n",
    "            print(f\"Model comparison saved to {os.path.join(output_dir, 'lstm_dim_comparison.csv')}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    else:\n",
    "        print(\"One or both models failed to run. Check error logs.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: train_up3.csv\n",
      "Loading data...\n",
      "Loading train/test listing IDs...\n",
      "Loading neighbor data from ./neighbor_data/neighbor_dict.csv\n",
      "Loaded neighbor data for 1573 test listings\n",
      "Loaded 6291 train IDs and 1573 test IDs\n",
      "Applying log transformation to price data\n",
      "Creating calculated features...\n",
      "Using 2 spatial features, 6 property features, and 5 temporal features\n",
      "Created 5 test periods:\n",
      "  Period 1: 2024-01-05 to 2024-01-11\n",
      "  Period 2: 2024-01-12 to 2024-01-18\n",
      "  Period 3: 2024-01-19 to 2024-01-25\n",
      "  Period 4: 2024-01-26 to 2024-02-01\n",
      "  Period 5: 2024-02-02 to 2024-02-08\n",
      "Using device: cuda\n",
      "\n",
      "===== Split 1/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-04\n",
      "Testing period: 2024-01-05 to 2024-01-11\n",
      "Train data: 903142 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training EnhancedNeighborBasedLSTM Model (Split 1) -----\n",
      "Preparing data for EnhancedNeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 11011 test instances\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Prepared data with 914153 nodes and 220220 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5653, Val Loss: 0.3517\n",
      "Epoch 2/125 - Loss: 0.4492, Val Loss: 0.3449\n",
      "Epoch 3/125 - Loss: 0.3646, Val Loss: 0.3373\n",
      "Epoch 4/125 - Loss: 0.3261, Val Loss: 0.3290\n",
      "Epoch 5/125 - Loss: 0.3120, Val Loss: 0.3209\n",
      "Epoch 6/125 - Loss: 0.3046, Val Loss: 0.3133\n",
      "Epoch 7/125 - Loss: 0.2941, Val Loss: 0.3068\n",
      "Epoch 8/125 - Loss: 0.2841, Val Loss: 0.3013\n",
      "Epoch 9/125 - Loss: 0.2763, Val Loss: 0.2966\n",
      "Epoch 10/125 - Loss: 0.2658, Val Loss: 0.2922\n",
      "Epoch 11/125 - Loss: 0.2653, Val Loss: 0.2874\n",
      "Epoch 12/125 - Loss: 0.2556, Val Loss: 0.2817\n",
      "Epoch 13/125 - Loss: 0.2537, Val Loss: 0.2749\n",
      "Epoch 14/125 - Loss: 0.2529, Val Loss: 0.2669\n",
      "Epoch 15/125 - Loss: 0.2436, Val Loss: 0.2578\n",
      "Epoch 16/125 - Loss: 0.2428, Val Loss: 0.2480\n",
      "Epoch 17/125 - Loss: 0.2428, Val Loss: 0.2383\n",
      "Epoch 18/125 - Loss: 0.2381, Val Loss: 0.2290\n",
      "Epoch 19/125 - Loss: 0.2292, Val Loss: 0.2203\n",
      "Epoch 20/125 - Loss: 0.2299, Val Loss: 0.2127\n",
      "Epoch 21/125 - Loss: 0.2285, Val Loss: 0.2063\n",
      "Epoch 22/125 - Loss: 0.2269, Val Loss: 0.2011\n",
      "Epoch 23/125 - Loss: 0.2227, Val Loss: 0.1968\n",
      "Epoch 24/125 - Loss: 0.2225, Val Loss: 0.1931\n",
      "Epoch 25/125 - Loss: 0.2208, Val Loss: 0.1898\n",
      "Epoch 26/125 - Loss: 0.2197, Val Loss: 0.1867\n",
      "Epoch 27/125 - Loss: 0.2133, Val Loss: 0.1837\n",
      "Epoch 28/125 - Loss: 0.2141, Val Loss: 0.1805\n",
      "Epoch 29/125 - Loss: 0.2122, Val Loss: 0.1772\n",
      "Epoch 30/125 - Loss: 0.2115, Val Loss: 0.1739\n",
      "Epoch 31/125 - Loss: 0.2097, Val Loss: 0.1704\n",
      "Epoch 32/125 - Loss: 0.2056, Val Loss: 0.1670\n",
      "Epoch 33/125 - Loss: 0.2052, Val Loss: 0.1640\n",
      "Epoch 34/125 - Loss: 0.2031, Val Loss: 0.1614\n",
      "Epoch 35/125 - Loss: 0.2014, Val Loss: 0.1593\n",
      "Epoch 36/125 - Loss: 0.1981, Val Loss: 0.1576\n",
      "Epoch 37/125 - Loss: 0.1973, Val Loss: 0.1562\n",
      "Epoch 38/125 - Loss: 0.1995, Val Loss: 0.1550\n",
      "Epoch 39/125 - Loss: 0.1940, Val Loss: 0.1540\n",
      "Epoch 40/125 - Loss: 0.1979, Val Loss: 0.1531\n",
      "Epoch 41/125 - Loss: 0.1942, Val Loss: 0.1522\n",
      "Epoch 42/125 - Loss: 0.1921, Val Loss: 0.1514\n",
      "Epoch 43/125 - Loss: 0.1894, Val Loss: 0.1506\n",
      "Epoch 44/125 - Loss: 0.1909, Val Loss: 0.1500\n",
      "Epoch 45/125 - Loss: 0.1893, Val Loss: 0.1494\n",
      "Epoch 46/125 - Loss: 0.1887, Val Loss: 0.1488\n",
      "Epoch 47/125 - Loss: 0.1865, Val Loss: 0.1482\n",
      "Epoch 48/125 - Loss: 0.1855, Val Loss: 0.1477\n",
      "Epoch 49/125 - Loss: 0.1835, Val Loss: 0.1472\n",
      "Epoch 50/125 - Loss: 0.1846, Val Loss: 0.1466\n",
      "Epoch 51/125 - Loss: 0.1830, Val Loss: 0.1461\n",
      "Epoch 52/125 - Loss: 0.1813, Val Loss: 0.1456\n",
      "Epoch 53/125 - Loss: 0.1812, Val Loss: 0.1451\n",
      "Epoch 54/125 - Loss: 0.1784, Val Loss: 0.1446\n",
      "Epoch 55/125 - Loss: 0.1797, Val Loss: 0.1443\n",
      "Epoch 56/125 - Loss: 0.1781, Val Loss: 0.1440\n",
      "Epoch 57/125 - Loss: 0.1762, Val Loss: 0.1437\n",
      "Epoch 58/125 - Loss: 0.1763, Val Loss: 0.1434\n",
      "Epoch 59/125 - Loss: 0.1774, Val Loss: 0.1431\n",
      "Epoch 60/125 - Loss: 0.1761, Val Loss: 0.1429\n",
      "Epoch 61/125 - Loss: 0.1742, Val Loss: 0.1427\n",
      "Epoch 62/125 - Loss: 0.1737, Val Loss: 0.1425\n",
      "Epoch 63/125 - Loss: 0.1724, Val Loss: 0.1424\n",
      "Epoch 64/125 - Loss: 0.1721, Val Loss: 0.1421\n",
      "Epoch 65/125 - Loss: 0.1718, Val Loss: 0.1418\n",
      "Epoch 66/125 - Loss: 0.1701, Val Loss: 0.1414\n",
      "Epoch 67/125 - Loss: 0.1697, Val Loss: 0.1411\n",
      "Epoch 68/125 - Loss: 0.1681, Val Loss: 0.1408\n",
      "Epoch 69/125 - Loss: 0.1691, Val Loss: 0.1405\n",
      "Epoch 70/125 - Loss: 0.1684, Val Loss: 0.1402\n",
      "Epoch 71/125 - Loss: 0.1687, Val Loss: 0.1401\n",
      "Epoch 72/125 - Loss: 0.1667, Val Loss: 0.1401\n",
      "Epoch 73/125 - Loss: 0.1672, Val Loss: 0.1402\n",
      "Epoch 74/125 - Loss: 0.1655, Val Loss: 0.1401\n",
      "Epoch 75/125 - Loss: 0.1666, Val Loss: 0.1396\n",
      "Epoch 76/125 - Loss: 0.1652, Val Loss: 0.1393\n",
      "Epoch 77/125 - Loss: 0.1653, Val Loss: 0.1394\n",
      "Epoch 78/125 - Loss: 0.1631, Val Loss: 0.1398\n",
      "Epoch 79/125 - Loss: 0.1623, Val Loss: 0.1396\n",
      "Epoch 80/125 - Loss: 0.1606, Val Loss: 0.1390\n",
      "Epoch 81/125 - Loss: 0.1621, Val Loss: 0.1386\n",
      "Epoch 82/125 - Loss: 0.1621, Val Loss: 0.1384\n",
      "Epoch 83/125 - Loss: 0.1605, Val Loss: 0.1381\n",
      "Epoch 84/125 - Loss: 0.1609, Val Loss: 0.1378\n",
      "Epoch 85/125 - Loss: 0.1580, Val Loss: 0.1376\n",
      "Epoch 86/125 - Loss: 0.1585, Val Loss: 0.1372\n",
      "Epoch 87/125 - Loss: 0.1590, Val Loss: 0.1369\n",
      "Epoch 88/125 - Loss: 0.1584, Val Loss: 0.1367\n",
      "Epoch 89/125 - Loss: 0.1581, Val Loss: 0.1364\n",
      "Epoch 90/125 - Loss: 0.1589, Val Loss: 0.1361\n",
      "Epoch 91/125 - Loss: 0.1563, Val Loss: 0.1358\n",
      "Epoch 92/125 - Loss: 0.1571, Val Loss: 0.1355\n",
      "Epoch 93/125 - Loss: 0.1550, Val Loss: 0.1351\n",
      "Epoch 94/125 - Loss: 0.1546, Val Loss: 0.1347\n",
      "Epoch 95/125 - Loss: 0.1567, Val Loss: 0.1342\n",
      "Epoch 96/125 - Loss: 0.1559, Val Loss: 0.1338\n",
      "Epoch 97/125 - Loss: 0.1530, Val Loss: 0.1334\n",
      "Epoch 98/125 - Loss: 0.1532, Val Loss: 0.1330\n",
      "Epoch 99/125 - Loss: 0.1538, Val Loss: 0.1327\n",
      "Epoch 100/125 - Loss: 0.1514, Val Loss: 0.1323\n",
      "Epoch 101/125 - Loss: 0.1524, Val Loss: 0.1320\n",
      "Epoch 102/125 - Loss: 0.1533, Val Loss: 0.1316\n",
      "Epoch 103/125 - Loss: 0.1533, Val Loss: 0.1312\n",
      "Epoch 104/125 - Loss: 0.1508, Val Loss: 0.1309\n",
      "Epoch 105/125 - Loss: 0.1512, Val Loss: 0.1308\n",
      "Epoch 106/125 - Loss: 0.1518, Val Loss: 0.1306\n",
      "Epoch 107/125 - Loss: 0.1520, Val Loss: 0.1303\n",
      "Epoch 108/125 - Loss: 0.1509, Val Loss: 0.1302\n",
      "Epoch 109/125 - Loss: 0.1484, Val Loss: 0.1299\n",
      "Epoch 110/125 - Loss: 0.1516, Val Loss: 0.1295\n",
      "Epoch 111/125 - Loss: 0.1505, Val Loss: 0.1290\n",
      "Epoch 112/125 - Loss: 0.1496, Val Loss: 0.1285\n",
      "Epoch 113/125 - Loss: 0.1493, Val Loss: 0.1280\n",
      "Epoch 114/125 - Loss: 0.1483, Val Loss: 0.1277\n",
      "Epoch 115/125 - Loss: 0.1482, Val Loss: 0.1275\n",
      "Epoch 116/125 - Loss: 0.1485, Val Loss: 0.1274\n",
      "Epoch 117/125 - Loss: 0.1479, Val Loss: 0.1273\n",
      "Epoch 118/125 - Loss: 0.1477, Val Loss: 0.1272\n",
      "Epoch 119/125 - Loss: 0.1474, Val Loss: 0.1270\n",
      "Epoch 120/125 - Loss: 0.1469, Val Loss: 0.1267\n",
      "Epoch 121/125 - Loss: 0.1462, Val Loss: 0.1263\n",
      "Epoch 122/125 - Loss: 0.1462, Val Loss: 0.1261\n",
      "Epoch 123/125 - Loss: 0.1460, Val Loss: 0.1258\n",
      "Epoch 124/125 - Loss: 0.1450, Val Loss: 0.1256\n",
      "Epoch 125/125 - Loss: 0.1457, Val Loss: 0.1253\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 109.33\n",
      "MAE: 58.20\n",
      "R²: 0.5400\n",
      "MAPE: 27.54%\n",
      "Split 1 Results - RMSE: 109.3280, MAE: 58.2044, R²: 0.5400\n",
      "Model for split 1 saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_model_split_1.pt\n",
      "\n",
      "===== Split 2/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-11\n",
      "Testing period: 2024-01-12 to 2024-01-18\n",
      "Train data: 947179 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training EnhancedNeighborBasedLSTM Model (Split 2) -----\n",
      "Preparing data for EnhancedNeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 11011 test instances\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Prepared data with 958190 nodes and 220220 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5250, Val Loss: 0.3386\n",
      "Epoch 2/125 - Loss: 0.4076, Val Loss: 0.3308\n",
      "Epoch 3/125 - Loss: 0.3449, Val Loss: 0.3231\n",
      "Epoch 4/125 - Loss: 0.3280, Val Loss: 0.3167\n",
      "Epoch 5/125 - Loss: 0.3168, Val Loss: 0.3118\n",
      "Epoch 6/125 - Loss: 0.3028, Val Loss: 0.3081\n",
      "Epoch 7/125 - Loss: 0.2907, Val Loss: 0.3052\n",
      "Epoch 8/125 - Loss: 0.2805, Val Loss: 0.3027\n",
      "Epoch 9/125 - Loss: 0.2761, Val Loss: 0.3000\n",
      "Epoch 10/125 - Loss: 0.2734, Val Loss: 0.2963\n",
      "Epoch 11/125 - Loss: 0.2675, Val Loss: 0.2910\n",
      "Epoch 12/125 - Loss: 0.2648, Val Loss: 0.2844\n",
      "Epoch 13/125 - Loss: 0.2632, Val Loss: 0.2767\n",
      "Epoch 14/125 - Loss: 0.2538, Val Loss: 0.2680\n",
      "Epoch 15/125 - Loss: 0.2502, Val Loss: 0.2585\n",
      "Epoch 16/125 - Loss: 0.2499, Val Loss: 0.2491\n",
      "Epoch 17/125 - Loss: 0.2459, Val Loss: 0.2398\n",
      "Epoch 18/125 - Loss: 0.2415, Val Loss: 0.2309\n",
      "Epoch 19/125 - Loss: 0.2375, Val Loss: 0.2229\n",
      "Epoch 20/125 - Loss: 0.2376, Val Loss: 0.2161\n",
      "Epoch 21/125 - Loss: 0.2323, Val Loss: 0.2103\n",
      "Epoch 22/125 - Loss: 0.2353, Val Loss: 0.2056\n",
      "Epoch 23/125 - Loss: 0.2272, Val Loss: 0.2013\n",
      "Epoch 24/125 - Loss: 0.2250, Val Loss: 0.1975\n",
      "Epoch 25/125 - Loss: 0.2252, Val Loss: 0.1938\n",
      "Epoch 26/125 - Loss: 0.2244, Val Loss: 0.1902\n",
      "Epoch 27/125 - Loss: 0.2214, Val Loss: 0.1863\n",
      "Epoch 28/125 - Loss: 0.2195, Val Loss: 0.1822\n",
      "Epoch 29/125 - Loss: 0.2162, Val Loss: 0.1780\n",
      "Epoch 30/125 - Loss: 0.2125, Val Loss: 0.1736\n",
      "Epoch 31/125 - Loss: 0.2107, Val Loss: 0.1693\n",
      "Epoch 32/125 - Loss: 0.2121, Val Loss: 0.1653\n",
      "Epoch 33/125 - Loss: 0.2087, Val Loss: 0.1617\n",
      "Epoch 34/125 - Loss: 0.2035, Val Loss: 0.1586\n",
      "Epoch 35/125 - Loss: 0.2045, Val Loss: 0.1560\n",
      "Epoch 36/125 - Loss: 0.2052, Val Loss: 0.1541\n",
      "Epoch 37/125 - Loss: 0.2023, Val Loss: 0.1526\n",
      "Epoch 38/125 - Loss: 0.2030, Val Loss: 0.1514\n",
      "Epoch 39/125 - Loss: 0.2007, Val Loss: 0.1505\n",
      "Epoch 40/125 - Loss: 0.2007, Val Loss: 0.1497\n",
      "Epoch 41/125 - Loss: 0.1984, Val Loss: 0.1491\n",
      "Epoch 42/125 - Loss: 0.1932, Val Loss: 0.1485\n",
      "Epoch 43/125 - Loss: 0.1975, Val Loss: 0.1479\n",
      "Epoch 44/125 - Loss: 0.1927, Val Loss: 0.1473\n",
      "Epoch 45/125 - Loss: 0.1915, Val Loss: 0.1467\n",
      "Epoch 46/125 - Loss: 0.1925, Val Loss: 0.1460\n",
      "Epoch 47/125 - Loss: 0.1916, Val Loss: 0.1453\n",
      "Epoch 48/125 - Loss: 0.1875, Val Loss: 0.1447\n",
      "Epoch 49/125 - Loss: 0.1860, Val Loss: 0.1442\n",
      "Epoch 50/125 - Loss: 0.1854, Val Loss: 0.1437\n",
      "Epoch 51/125 - Loss: 0.1857, Val Loss: 0.1432\n",
      "Epoch 52/125 - Loss: 0.1836, Val Loss: 0.1428\n",
      "Epoch 53/125 - Loss: 0.1853, Val Loss: 0.1424\n",
      "Epoch 54/125 - Loss: 0.1821, Val Loss: 0.1421\n",
      "Epoch 55/125 - Loss: 0.1820, Val Loss: 0.1417\n",
      "Epoch 56/125 - Loss: 0.1812, Val Loss: 0.1413\n",
      "Epoch 57/125 - Loss: 0.1799, Val Loss: 0.1411\n",
      "Epoch 58/125 - Loss: 0.1806, Val Loss: 0.1408\n",
      "Epoch 59/125 - Loss: 0.1788, Val Loss: 0.1405\n",
      "Epoch 60/125 - Loss: 0.1793, Val Loss: 0.1403\n",
      "Epoch 61/125 - Loss: 0.1748, Val Loss: 0.1401\n",
      "Epoch 62/125 - Loss: 0.1730, Val Loss: 0.1400\n",
      "Epoch 63/125 - Loss: 0.1750, Val Loss: 0.1399\n",
      "Epoch 64/125 - Loss: 0.1721, Val Loss: 0.1397\n",
      "Epoch 65/125 - Loss: 0.1709, Val Loss: 0.1395\n",
      "Epoch 66/125 - Loss: 0.1720, Val Loss: 0.1392\n",
      "Epoch 67/125 - Loss: 0.1727, Val Loss: 0.1390\n",
      "Epoch 68/125 - Loss: 0.1721, Val Loss: 0.1387\n",
      "Epoch 69/125 - Loss: 0.1677, Val Loss: 0.1385\n",
      "Epoch 70/125 - Loss: 0.1690, Val Loss: 0.1383\n",
      "Epoch 71/125 - Loss: 0.1690, Val Loss: 0.1380\n",
      "Epoch 72/125 - Loss: 0.1665, Val Loss: 0.1379\n",
      "Epoch 73/125 - Loss: 0.1681, Val Loss: 0.1378\n",
      "Epoch 74/125 - Loss: 0.1648, Val Loss: 0.1375\n",
      "Epoch 75/125 - Loss: 0.1646, Val Loss: 0.1372\n",
      "Epoch 76/125 - Loss: 0.1674, Val Loss: 0.1370\n",
      "Epoch 77/125 - Loss: 0.1652, Val Loss: 0.1369\n",
      "Epoch 78/125 - Loss: 0.1641, Val Loss: 0.1369\n",
      "Epoch 79/125 - Loss: 0.1633, Val Loss: 0.1367\n",
      "Epoch 80/125 - Loss: 0.1630, Val Loss: 0.1365\n",
      "Epoch 81/125 - Loss: 0.1626, Val Loss: 0.1364\n",
      "Epoch 82/125 - Loss: 0.1638, Val Loss: 0.1362\n",
      "Epoch 83/125 - Loss: 0.1626, Val Loss: 0.1361\n",
      "Epoch 84/125 - Loss: 0.1630, Val Loss: 0.1359\n",
      "Epoch 85/125 - Loss: 0.1587, Val Loss: 0.1358\n",
      "Epoch 86/125 - Loss: 0.1595, Val Loss: 0.1358\n",
      "Epoch 87/125 - Loss: 0.1596, Val Loss: 0.1356\n",
      "Epoch 88/125 - Loss: 0.1581, Val Loss: 0.1353\n",
      "Epoch 89/125 - Loss: 0.1597, Val Loss: 0.1350\n",
      "Epoch 90/125 - Loss: 0.1603, Val Loss: 0.1348\n",
      "Epoch 91/125 - Loss: 0.1562, Val Loss: 0.1345\n",
      "Epoch 92/125 - Loss: 0.1590, Val Loss: 0.1344\n",
      "Epoch 93/125 - Loss: 0.1590, Val Loss: 0.1342\n",
      "Epoch 94/125 - Loss: 0.1573, Val Loss: 0.1338\n",
      "Epoch 95/125 - Loss: 0.1584, Val Loss: 0.1338\n",
      "Epoch 96/125 - Loss: 0.1585, Val Loss: 0.1335\n",
      "Epoch 97/125 - Loss: 0.1540, Val Loss: 0.1333\n",
      "Epoch 98/125 - Loss: 0.1527, Val Loss: 0.1337\n",
      "Epoch 99/125 - Loss: 0.1544, Val Loss: 0.1333\n",
      "Epoch 100/125 - Loss: 0.1541, Val Loss: 0.1325\n",
      "Epoch 101/125 - Loss: 0.1529, Val Loss: 0.1325\n",
      "Epoch 102/125 - Loss: 0.1522, Val Loss: 0.1323\n",
      "Epoch 103/125 - Loss: 0.1513, Val Loss: 0.1321\n",
      "Epoch 104/125 - Loss: 0.1511, Val Loss: 0.1321\n",
      "Epoch 105/125 - Loss: 0.1521, Val Loss: 0.1321\n",
      "Epoch 106/125 - Loss: 0.1535, Val Loss: 0.1317\n",
      "Epoch 107/125 - Loss: 0.1531, Val Loss: 0.1314\n",
      "Epoch 108/125 - Loss: 0.1507, Val Loss: 0.1310\n",
      "Epoch 109/125 - Loss: 0.1527, Val Loss: 0.1307\n",
      "Epoch 110/125 - Loss: 0.1496, Val Loss: 0.1305\n",
      "Epoch 111/125 - Loss: 0.1503, Val Loss: 0.1302\n",
      "Epoch 112/125 - Loss: 0.1494, Val Loss: 0.1300\n",
      "Epoch 113/125 - Loss: 0.1501, Val Loss: 0.1298\n",
      "Epoch 114/125 - Loss: 0.1494, Val Loss: 0.1295\n",
      "Epoch 115/125 - Loss: 0.1475, Val Loss: 0.1292\n",
      "Epoch 116/125 - Loss: 0.1476, Val Loss: 0.1290\n",
      "Epoch 117/125 - Loss: 0.1492, Val Loss: 0.1288\n",
      "Epoch 118/125 - Loss: 0.1466, Val Loss: 0.1286\n",
      "Epoch 119/125 - Loss: 0.1473, Val Loss: 0.1282\n",
      "Epoch 120/125 - Loss: 0.1469, Val Loss: 0.1279\n",
      "Epoch 121/125 - Loss: 0.1461, Val Loss: 0.1277\n",
      "Epoch 122/125 - Loss: 0.1460, Val Loss: 0.1277\n",
      "Epoch 123/125 - Loss: 0.1462, Val Loss: 0.1272\n",
      "Epoch 124/125 - Loss: 0.1457, Val Loss: 0.1268\n",
      "Epoch 125/125 - Loss: 0.1446, Val Loss: 0.1265\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 108.10\n",
      "MAE: 57.31\n",
      "R²: 0.5284\n",
      "MAPE: 27.88%\n",
      "Split 2 Results - RMSE: 108.0961, MAE: 57.3121, R²: 0.5284\n",
      "Model for split 2 saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_model_split_2.pt\n",
      "\n",
      "===== Split 3/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-18\n",
      "Testing period: 2024-01-19 to 2024-01-25\n",
      "Train data: 991216 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training EnhancedNeighborBasedLSTM Model (Split 3) -----\n",
      "Preparing data for EnhancedNeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 11011 test instances\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Prepared data with 1002227 nodes and 220220 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.4735, Val Loss: 0.3490\n",
      "Epoch 2/125 - Loss: 0.3772, Val Loss: 0.3410\n",
      "Epoch 3/125 - Loss: 0.3296, Val Loss: 0.3333\n",
      "Epoch 4/125 - Loss: 0.3066, Val Loss: 0.3265\n",
      "Epoch 5/125 - Loss: 0.2999, Val Loss: 0.3205\n",
      "Epoch 6/125 - Loss: 0.2866, Val Loss: 0.3154\n",
      "Epoch 7/125 - Loss: 0.2799, Val Loss: 0.3110\n",
      "Epoch 8/125 - Loss: 0.2682, Val Loss: 0.3068\n",
      "Epoch 9/125 - Loss: 0.2682, Val Loss: 0.3021\n",
      "Epoch 10/125 - Loss: 0.2534, Val Loss: 0.2971\n",
      "Epoch 11/125 - Loss: 0.2579, Val Loss: 0.2912\n",
      "Epoch 12/125 - Loss: 0.2516, Val Loss: 0.2845\n",
      "Epoch 13/125 - Loss: 0.2513, Val Loss: 0.2769\n",
      "Epoch 14/125 - Loss: 0.2452, Val Loss: 0.2689\n",
      "Epoch 15/125 - Loss: 0.2406, Val Loss: 0.2605\n",
      "Epoch 16/125 - Loss: 0.2391, Val Loss: 0.2519\n",
      "Epoch 17/125 - Loss: 0.2341, Val Loss: 0.2437\n",
      "Epoch 18/125 - Loss: 0.2287, Val Loss: 0.2361\n",
      "Epoch 19/125 - Loss: 0.2280, Val Loss: 0.2292\n",
      "Epoch 20/125 - Loss: 0.2258, Val Loss: 0.2228\n",
      "Epoch 21/125 - Loss: 0.2215, Val Loss: 0.2170\n",
      "Epoch 22/125 - Loss: 0.2243, Val Loss: 0.2117\n",
      "Epoch 23/125 - Loss: 0.2212, Val Loss: 0.2071\n",
      "Epoch 24/125 - Loss: 0.2172, Val Loss: 0.2028\n",
      "Epoch 25/125 - Loss: 0.2133, Val Loss: 0.1987\n",
      "Epoch 26/125 - Loss: 0.2171, Val Loss: 0.1948\n",
      "Epoch 27/125 - Loss: 0.2140, Val Loss: 0.1910\n",
      "Epoch 28/125 - Loss: 0.2100, Val Loss: 0.1871\n",
      "Epoch 29/125 - Loss: 0.2097, Val Loss: 0.1832\n",
      "Epoch 30/125 - Loss: 0.2078, Val Loss: 0.1792\n",
      "Epoch 31/125 - Loss: 0.2032, Val Loss: 0.1752\n",
      "Epoch 32/125 - Loss: 0.2029, Val Loss: 0.1714\n",
      "Epoch 33/125 - Loss: 0.2021, Val Loss: 0.1678\n",
      "Epoch 34/125 - Loss: 0.1996, Val Loss: 0.1648\n",
      "Epoch 35/125 - Loss: 0.1981, Val Loss: 0.1622\n",
      "Epoch 36/125 - Loss: 0.1982, Val Loss: 0.1601\n",
      "Epoch 37/125 - Loss: 0.1986, Val Loss: 0.1585\n",
      "Epoch 38/125 - Loss: 0.1966, Val Loss: 0.1572\n",
      "Epoch 39/125 - Loss: 0.1933, Val Loss: 0.1559\n",
      "Epoch 40/125 - Loss: 0.1919, Val Loss: 0.1548\n",
      "Epoch 41/125 - Loss: 0.1913, Val Loss: 0.1538\n",
      "Epoch 42/125 - Loss: 0.1900, Val Loss: 0.1528\n",
      "Epoch 43/125 - Loss: 0.1878, Val Loss: 0.1517\n",
      "Epoch 44/125 - Loss: 0.1893, Val Loss: 0.1507\n",
      "Epoch 45/125 - Loss: 0.1863, Val Loss: 0.1497\n",
      "Epoch 46/125 - Loss: 0.1857, Val Loss: 0.1486\n",
      "Epoch 47/125 - Loss: 0.1834, Val Loss: 0.1477\n",
      "Epoch 48/125 - Loss: 0.1811, Val Loss: 0.1470\n",
      "Epoch 49/125 - Loss: 0.1819, Val Loss: 0.1464\n",
      "Epoch 50/125 - Loss: 0.1817, Val Loss: 0.1460\n",
      "Epoch 51/125 - Loss: 0.1809, Val Loss: 0.1456\n",
      "Epoch 52/125 - Loss: 0.1787, Val Loss: 0.1452\n",
      "Epoch 53/125 - Loss: 0.1765, Val Loss: 0.1449\n",
      "Epoch 54/125 - Loss: 0.1784, Val Loss: 0.1446\n",
      "Epoch 55/125 - Loss: 0.1811, Val Loss: 0.1443\n",
      "Epoch 56/125 - Loss: 0.1761, Val Loss: 0.1441\n",
      "Epoch 57/125 - Loss: 0.1768, Val Loss: 0.1439\n",
      "Epoch 58/125 - Loss: 0.1762, Val Loss: 0.1436\n",
      "Epoch 59/125 - Loss: 0.1760, Val Loss: 0.1434\n",
      "Epoch 60/125 - Loss: 0.1735, Val Loss: 0.1432\n",
      "Epoch 61/125 - Loss: 0.1739, Val Loss: 0.1429\n",
      "Epoch 62/125 - Loss: 0.1743, Val Loss: 0.1427\n",
      "Epoch 63/125 - Loss: 0.1742, Val Loss: 0.1425\n",
      "Epoch 64/125 - Loss: 0.1712, Val Loss: 0.1423\n",
      "Epoch 65/125 - Loss: 0.1699, Val Loss: 0.1421\n",
      "Epoch 66/125 - Loss: 0.1708, Val Loss: 0.1419\n",
      "Epoch 67/125 - Loss: 0.1674, Val Loss: 0.1417\n",
      "Epoch 68/125 - Loss: 0.1667, Val Loss: 0.1414\n",
      "Epoch 69/125 - Loss: 0.1683, Val Loss: 0.1413\n",
      "Epoch 70/125 - Loss: 0.1678, Val Loss: 0.1411\n",
      "Epoch 71/125 - Loss: 0.1692, Val Loss: 0.1410\n",
      "Epoch 72/125 - Loss: 0.1658, Val Loss: 0.1409\n",
      "Epoch 73/125 - Loss: 0.1655, Val Loss: 0.1407\n",
      "Epoch 74/125 - Loss: 0.1633, Val Loss: 0.1406\n",
      "Epoch 75/125 - Loss: 0.1652, Val Loss: 0.1405\n",
      "Epoch 76/125 - Loss: 0.1641, Val Loss: 0.1403\n",
      "Epoch 77/125 - Loss: 0.1653, Val Loss: 0.1400\n",
      "Epoch 78/125 - Loss: 0.1634, Val Loss: 0.1398\n",
      "Epoch 79/125 - Loss: 0.1635, Val Loss: 0.1396\n",
      "Epoch 80/125 - Loss: 0.1628, Val Loss: 0.1393\n",
      "Epoch 81/125 - Loss: 0.1627, Val Loss: 0.1391\n",
      "Epoch 82/125 - Loss: 0.1610, Val Loss: 0.1390\n",
      "Epoch 83/125 - Loss: 0.1587, Val Loss: 0.1389\n",
      "Epoch 84/125 - Loss: 0.1602, Val Loss: 0.1388\n",
      "Epoch 85/125 - Loss: 0.1583, Val Loss: 0.1389\n",
      "Epoch 86/125 - Loss: 0.1597, Val Loss: 0.1388\n",
      "Epoch 87/125 - Loss: 0.1586, Val Loss: 0.1385\n",
      "Epoch 88/125 - Loss: 0.1577, Val Loss: 0.1387\n",
      "Epoch 89/125 - Loss: 0.1589, Val Loss: 0.1391\n",
      "Epoch 90/125 - Loss: 0.1579, Val Loss: 0.1390\n",
      "Epoch 91/125 - Loss: 0.1571, Val Loss: 0.1383\n",
      "Epoch 92/125 - Loss: 0.1561, Val Loss: 0.1380\n",
      "Epoch 93/125 - Loss: 0.1555, Val Loss: 0.1378\n",
      "Epoch 94/125 - Loss: 0.1561, Val Loss: 0.1372\n",
      "Epoch 95/125 - Loss: 0.1549, Val Loss: 0.1366\n",
      "Epoch 96/125 - Loss: 0.1531, Val Loss: 0.1361\n",
      "Epoch 97/125 - Loss: 0.1534, Val Loss: 0.1359\n",
      "Epoch 98/125 - Loss: 0.1540, Val Loss: 0.1353\n",
      "Epoch 99/125 - Loss: 0.1536, Val Loss: 0.1348\n",
      "Epoch 100/125 - Loss: 0.1541, Val Loss: 0.1345\n",
      "Epoch 101/125 - Loss: 0.1540, Val Loss: 0.1343\n",
      "Epoch 102/125 - Loss: 0.1512, Val Loss: 0.1340\n",
      "Epoch 103/125 - Loss: 0.1528, Val Loss: 0.1336\n",
      "Epoch 104/125 - Loss: 0.1518, Val Loss: 0.1334\n",
      "Epoch 105/125 - Loss: 0.1501, Val Loss: 0.1331\n",
      "Epoch 106/125 - Loss: 0.1523, Val Loss: 0.1329\n",
      "Epoch 107/125 - Loss: 0.1512, Val Loss: 0.1324\n",
      "Epoch 108/125 - Loss: 0.1508, Val Loss: 0.1319\n",
      "Epoch 109/125 - Loss: 0.1506, Val Loss: 0.1314\n",
      "Epoch 110/125 - Loss: 0.1501, Val Loss: 0.1310\n",
      "Epoch 111/125 - Loss: 0.1503, Val Loss: 0.1306\n",
      "Epoch 112/125 - Loss: 0.1496, Val Loss: 0.1302\n",
      "Epoch 113/125 - Loss: 0.1483, Val Loss: 0.1300\n",
      "Epoch 114/125 - Loss: 0.1486, Val Loss: 0.1298\n",
      "Epoch 115/125 - Loss: 0.1491, Val Loss: 0.1296\n",
      "Epoch 116/125 - Loss: 0.1496, Val Loss: 0.1292\n",
      "Epoch 117/125 - Loss: 0.1464, Val Loss: 0.1290\n",
      "Epoch 118/125 - Loss: 0.1462, Val Loss: 0.1285\n",
      "Epoch 119/125 - Loss: 0.1474, Val Loss: 0.1279\n",
      "Epoch 120/125 - Loss: 0.1470, Val Loss: 0.1275\n",
      "Epoch 121/125 - Loss: 0.1467, Val Loss: 0.1271\n",
      "Epoch 122/125 - Loss: 0.1458, Val Loss: 0.1270\n",
      "Epoch 123/125 - Loss: 0.1461, Val Loss: 0.1269\n",
      "Epoch 124/125 - Loss: 0.1456, Val Loss: 0.1263\n",
      "Epoch 125/125 - Loss: 0.1441, Val Loss: 0.1260\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 108.39\n",
      "MAE: 57.64\n",
      "R²: 0.5336\n",
      "MAPE: 27.52%\n",
      "Split 3 Results - RMSE: 108.3851, MAE: 57.6434, R²: 0.5336\n",
      "Model for split 3 saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_model_split_3.pt\n",
      "\n",
      "===== Split 4/5 =====\n",
      "Training period: 2023-08-07 to 2024-01-25\n",
      "Testing period: 2024-01-26 to 2024-02-01\n",
      "Train data: 1035253 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training EnhancedNeighborBasedLSTM Model (Split 4) -----\n",
      "Preparing data for EnhancedNeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 11011 test instances\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Prepared data with 1046264 nodes and 220220 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.4555, Val Loss: 0.3506\n",
      "Epoch 2/125 - Loss: 0.3724, Val Loss: 0.3441\n",
      "Epoch 3/125 - Loss: 0.3367, Val Loss: 0.3369\n",
      "Epoch 4/125 - Loss: 0.3179, Val Loss: 0.3297\n",
      "Epoch 5/125 - Loss: 0.2975, Val Loss: 0.3231\n",
      "Epoch 6/125 - Loss: 0.2881, Val Loss: 0.3171\n",
      "Epoch 7/125 - Loss: 0.2757, Val Loss: 0.3116\n",
      "Epoch 8/125 - Loss: 0.2645, Val Loss: 0.3063\n",
      "Epoch 9/125 - Loss: 0.2614, Val Loss: 0.3009\n",
      "Epoch 10/125 - Loss: 0.2568, Val Loss: 0.2954\n",
      "Epoch 11/125 - Loss: 0.2511, Val Loss: 0.2894\n",
      "Epoch 12/125 - Loss: 0.2445, Val Loss: 0.2829\n",
      "Epoch 13/125 - Loss: 0.2430, Val Loss: 0.2760\n",
      "Epoch 14/125 - Loss: 0.2369, Val Loss: 0.2690\n",
      "Epoch 15/125 - Loss: 0.2396, Val Loss: 0.2616\n",
      "Epoch 16/125 - Loss: 0.2303, Val Loss: 0.2543\n",
      "Epoch 17/125 - Loss: 0.2290, Val Loss: 0.2471\n",
      "Epoch 18/125 - Loss: 0.2232, Val Loss: 0.2400\n",
      "Epoch 19/125 - Loss: 0.2223, Val Loss: 0.2333\n",
      "Epoch 20/125 - Loss: 0.2191, Val Loss: 0.2267\n",
      "Epoch 21/125 - Loss: 0.2181, Val Loss: 0.2206\n",
      "Epoch 22/125 - Loss: 0.2126, Val Loss: 0.2147\n",
      "Epoch 23/125 - Loss: 0.2127, Val Loss: 0.2092\n",
      "Epoch 24/125 - Loss: 0.2103, Val Loss: 0.2042\n",
      "Epoch 25/125 - Loss: 0.2090, Val Loss: 0.1993\n",
      "Epoch 26/125 - Loss: 0.2089, Val Loss: 0.1945\n",
      "Epoch 27/125 - Loss: 0.2064, Val Loss: 0.1899\n",
      "Epoch 28/125 - Loss: 0.2026, Val Loss: 0.1852\n",
      "Epoch 29/125 - Loss: 0.2029, Val Loss: 0.1810\n",
      "Epoch 30/125 - Loss: 0.1999, Val Loss: 0.1771\n",
      "Epoch 31/125 - Loss: 0.1994, Val Loss: 0.1735\n",
      "Epoch 32/125 - Loss: 0.1964, Val Loss: 0.1703\n",
      "Epoch 33/125 - Loss: 0.1980, Val Loss: 0.1674\n",
      "Epoch 34/125 - Loss: 0.1956, Val Loss: 0.1648\n",
      "Epoch 35/125 - Loss: 0.1936, Val Loss: 0.1626\n",
      "Epoch 36/125 - Loss: 0.1921, Val Loss: 0.1606\n",
      "Epoch 37/125 - Loss: 0.1911, Val Loss: 0.1590\n",
      "Epoch 38/125 - Loss: 0.1898, Val Loss: 0.1575\n",
      "Epoch 39/125 - Loss: 0.1888, Val Loss: 0.1560\n",
      "Epoch 40/125 - Loss: 0.1863, Val Loss: 0.1547\n",
      "Epoch 41/125 - Loss: 0.1890, Val Loss: 0.1534\n",
      "Epoch 42/125 - Loss: 0.1854, Val Loss: 0.1523\n",
      "Epoch 43/125 - Loss: 0.1846, Val Loss: 0.1513\n",
      "Epoch 44/125 - Loss: 0.1827, Val Loss: 0.1504\n",
      "Epoch 45/125 - Loss: 0.1822, Val Loss: 0.1496\n",
      "Epoch 46/125 - Loss: 0.1790, Val Loss: 0.1489\n",
      "Epoch 47/125 - Loss: 0.1810, Val Loss: 0.1483\n",
      "Epoch 48/125 - Loss: 0.1804, Val Loss: 0.1477\n",
      "Epoch 49/125 - Loss: 0.1791, Val Loss: 0.1472\n",
      "Epoch 50/125 - Loss: 0.1766, Val Loss: 0.1467\n",
      "Epoch 51/125 - Loss: 0.1751, Val Loss: 0.1462\n",
      "Epoch 52/125 - Loss: 0.1762, Val Loss: 0.1458\n",
      "Epoch 53/125 - Loss: 0.1741, Val Loss: 0.1455\n",
      "Epoch 54/125 - Loss: 0.1737, Val Loss: 0.1451\n",
      "Epoch 55/125 - Loss: 0.1749, Val Loss: 0.1448\n",
      "Epoch 56/125 - Loss: 0.1733, Val Loss: 0.1445\n",
      "Epoch 57/125 - Loss: 0.1732, Val Loss: 0.1442\n",
      "Epoch 58/125 - Loss: 0.1729, Val Loss: 0.1439\n",
      "Epoch 59/125 - Loss: 0.1704, Val Loss: 0.1436\n",
      "Epoch 60/125 - Loss: 0.1713, Val Loss: 0.1433\n",
      "Epoch 61/125 - Loss: 0.1708, Val Loss: 0.1430\n",
      "Epoch 62/125 - Loss: 0.1698, Val Loss: 0.1427\n",
      "Epoch 63/125 - Loss: 0.1714, Val Loss: 0.1424\n",
      "Epoch 64/125 - Loss: 0.1691, Val Loss: 0.1422\n",
      "Epoch 65/125 - Loss: 0.1687, Val Loss: 0.1420\n",
      "Epoch 66/125 - Loss: 0.1685, Val Loss: 0.1418\n",
      "Epoch 67/125 - Loss: 0.1681, Val Loss: 0.1417\n",
      "Epoch 68/125 - Loss: 0.1658, Val Loss: 0.1415\n",
      "Epoch 69/125 - Loss: 0.1672, Val Loss: 0.1412\n",
      "Epoch 70/125 - Loss: 0.1640, Val Loss: 0.1409\n",
      "Epoch 71/125 - Loss: 0.1642, Val Loss: 0.1406\n",
      "Epoch 72/125 - Loss: 0.1635, Val Loss: 0.1405\n",
      "Epoch 73/125 - Loss: 0.1646, Val Loss: 0.1403\n",
      "Epoch 74/125 - Loss: 0.1637, Val Loss: 0.1401\n",
      "Epoch 75/125 - Loss: 0.1630, Val Loss: 0.1398\n",
      "Epoch 76/125 - Loss: 0.1628, Val Loss: 0.1396\n",
      "Epoch 77/125 - Loss: 0.1613, Val Loss: 0.1398\n",
      "Epoch 78/125 - Loss: 0.1610, Val Loss: 0.1394\n",
      "Epoch 79/125 - Loss: 0.1597, Val Loss: 0.1392\n",
      "Epoch 80/125 - Loss: 0.1618, Val Loss: 0.1391\n",
      "Epoch 81/125 - Loss: 0.1614, Val Loss: 0.1388\n",
      "Epoch 82/125 - Loss: 0.1596, Val Loss: 0.1389\n",
      "Epoch 83/125 - Loss: 0.1604, Val Loss: 0.1385\n",
      "Epoch 84/125 - Loss: 0.1570, Val Loss: 0.1379\n",
      "Epoch 85/125 - Loss: 0.1597, Val Loss: 0.1376\n",
      "Epoch 86/125 - Loss: 0.1583, Val Loss: 0.1373\n",
      "Epoch 87/125 - Loss: 0.1593, Val Loss: 0.1369\n",
      "Epoch 88/125 - Loss: 0.1589, Val Loss: 0.1365\n",
      "Epoch 89/125 - Loss: 0.1564, Val Loss: 0.1363\n",
      "Epoch 90/125 - Loss: 0.1576, Val Loss: 0.1360\n",
      "Epoch 91/125 - Loss: 0.1576, Val Loss: 0.1358\n",
      "Epoch 92/125 - Loss: 0.1571, Val Loss: 0.1356\n",
      "Epoch 93/125 - Loss: 0.1564, Val Loss: 0.1355\n",
      "Epoch 94/125 - Loss: 0.1554, Val Loss: 0.1352\n",
      "Epoch 95/125 - Loss: 0.1569, Val Loss: 0.1350\n",
      "Epoch 96/125 - Loss: 0.1539, Val Loss: 0.1347\n",
      "Epoch 97/125 - Loss: 0.1532, Val Loss: 0.1344\n",
      "Epoch 98/125 - Loss: 0.1538, Val Loss: 0.1342\n",
      "Epoch 99/125 - Loss: 0.1538, Val Loss: 0.1338\n",
      "Epoch 100/125 - Loss: 0.1537, Val Loss: 0.1335\n",
      "Epoch 101/125 - Loss: 0.1530, Val Loss: 0.1332\n",
      "Epoch 102/125 - Loss: 0.1530, Val Loss: 0.1329\n",
      "Epoch 103/125 - Loss: 0.1534, Val Loss: 0.1327\n",
      "Epoch 104/125 - Loss: 0.1518, Val Loss: 0.1325\n",
      "Epoch 105/125 - Loss: 0.1524, Val Loss: 0.1322\n",
      "Epoch 106/125 - Loss: 0.1513, Val Loss: 0.1322\n",
      "Epoch 107/125 - Loss: 0.1538, Val Loss: 0.1322\n",
      "Epoch 108/125 - Loss: 0.1525, Val Loss: 0.1320\n",
      "Epoch 109/125 - Loss: 0.1517, Val Loss: 0.1317\n",
      "Epoch 110/125 - Loss: 0.1511, Val Loss: 0.1313\n",
      "Epoch 111/125 - Loss: 0.1515, Val Loss: 0.1310\n",
      "Epoch 112/125 - Loss: 0.1505, Val Loss: 0.1305\n",
      "Epoch 113/125 - Loss: 0.1494, Val Loss: 0.1300\n",
      "Epoch 114/125 - Loss: 0.1511, Val Loss: 0.1301\n",
      "Epoch 115/125 - Loss: 0.1487, Val Loss: 0.1297\n",
      "Epoch 116/125 - Loss: 0.1492, Val Loss: 0.1292\n",
      "Epoch 117/125 - Loss: 0.1497, Val Loss: 0.1288\n",
      "Epoch 118/125 - Loss: 0.1490, Val Loss: 0.1284\n",
      "Epoch 119/125 - Loss: 0.1476, Val Loss: 0.1278\n",
      "Epoch 120/125 - Loss: 0.1478, Val Loss: 0.1273\n",
      "Epoch 121/125 - Loss: 0.1461, Val Loss: 0.1271\n",
      "Epoch 122/125 - Loss: 0.1485, Val Loss: 0.1266\n",
      "Epoch 123/125 - Loss: 0.1458, Val Loss: 0.1263\n",
      "Epoch 124/125 - Loss: 0.1468, Val Loss: 0.1262\n",
      "Epoch 125/125 - Loss: 0.1469, Val Loss: 0.1260\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 109.50\n",
      "MAE: 56.75\n",
      "R²: 0.5342\n",
      "MAPE: 27.35%\n",
      "Split 4 Results - RMSE: 109.5022, MAE: 56.7538, R²: 0.5342\n",
      "Model for split 4 saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_model_split_4.pt\n",
      "\n",
      "===== Split 5/5 =====\n",
      "Training period: 2023-08-07 to 2024-02-01\n",
      "Testing period: 2024-02-02 to 2024-02-08\n",
      "Train data: 1079290 rows, 6291 unique listings\n",
      "Test data: 11011 rows, 1573 unique listings\n",
      "\n",
      "----- Training EnhancedNeighborBasedLSTM Model (Split 5) -----\n",
      "Preparing data for EnhancedNeighborBasedLSTM...\n",
      "Preparing neighbor data batch...\n",
      "Prepared neighbor data for 11011 test instances\n",
      "Building enhanced spatial graph with 11011 test listings and 10 nearest neighbors...\n",
      "Created graph with 220220 edges\n",
      "Prepared data with 1090301 nodes and 220220 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125 - Loss: 0.5846, Val Loss: 0.3532\n",
      "Epoch 2/125 - Loss: 0.4482, Val Loss: 0.3473\n",
      "Epoch 3/125 - Loss: 0.3725, Val Loss: 0.3410\n",
      "Epoch 4/125 - Loss: 0.3404, Val Loss: 0.3346\n",
      "Epoch 5/125 - Loss: 0.3233, Val Loss: 0.3288\n",
      "Epoch 6/125 - Loss: 0.3104, Val Loss: 0.3237\n",
      "Epoch 7/125 - Loss: 0.3056, Val Loss: 0.3192\n",
      "Epoch 8/125 - Loss: 0.2911, Val Loss: 0.3153\n",
      "Epoch 9/125 - Loss: 0.2727, Val Loss: 0.3117\n",
      "Epoch 10/125 - Loss: 0.2742, Val Loss: 0.3083\n",
      "Epoch 11/125 - Loss: 0.2723, Val Loss: 0.3043\n",
      "Epoch 12/125 - Loss: 0.2703, Val Loss: 0.2992\n",
      "Epoch 13/125 - Loss: 0.2590, Val Loss: 0.2929\n",
      "Epoch 14/125 - Loss: 0.2601, Val Loss: 0.2852\n",
      "Epoch 15/125 - Loss: 0.2504, Val Loss: 0.2763\n",
      "Epoch 16/125 - Loss: 0.2472, Val Loss: 0.2666\n",
      "Epoch 17/125 - Loss: 0.2448, Val Loss: 0.2562\n",
      "Epoch 18/125 - Loss: 0.2383, Val Loss: 0.2454\n",
      "Epoch 19/125 - Loss: 0.2350, Val Loss: 0.2351\n",
      "Epoch 20/125 - Loss: 0.2373, Val Loss: 0.2257\n",
      "Epoch 21/125 - Loss: 0.2322, Val Loss: 0.2175\n",
      "Epoch 22/125 - Loss: 0.2294, Val Loss: 0.2103\n",
      "Epoch 23/125 - Loss: 0.2262, Val Loss: 0.2042\n",
      "Epoch 24/125 - Loss: 0.2218, Val Loss: 0.1991\n",
      "Epoch 25/125 - Loss: 0.2228, Val Loss: 0.1947\n",
      "Epoch 26/125 - Loss: 0.2192, Val Loss: 0.1909\n",
      "Epoch 27/125 - Loss: 0.2153, Val Loss: 0.1872\n",
      "Epoch 28/125 - Loss: 0.2174, Val Loss: 0.1837\n",
      "Epoch 29/125 - Loss: 0.2146, Val Loss: 0.1801\n",
      "Epoch 30/125 - Loss: 0.2166, Val Loss: 0.1765\n",
      "Epoch 31/125 - Loss: 0.2111, Val Loss: 0.1728\n",
      "Epoch 32/125 - Loss: 0.2112, Val Loss: 0.1692\n",
      "Epoch 33/125 - Loss: 0.2056, Val Loss: 0.1656\n",
      "Epoch 34/125 - Loss: 0.2084, Val Loss: 0.1622\n",
      "Epoch 35/125 - Loss: 0.2059, Val Loss: 0.1593\n",
      "Epoch 36/125 - Loss: 0.2007, Val Loss: 0.1570\n",
      "Epoch 37/125 - Loss: 0.2014, Val Loss: 0.1551\n",
      "Epoch 38/125 - Loss: 0.2004, Val Loss: 0.1536\n",
      "Epoch 39/125 - Loss: 0.1999, Val Loss: 0.1524\n",
      "Epoch 40/125 - Loss: 0.1986, Val Loss: 0.1515\n",
      "Epoch 41/125 - Loss: 0.1960, Val Loss: 0.1507\n",
      "Epoch 42/125 - Loss: 0.1927, Val Loss: 0.1501\n",
      "Epoch 43/125 - Loss: 0.1932, Val Loss: 0.1495\n",
      "Epoch 44/125 - Loss: 0.1937, Val Loss: 0.1490\n",
      "Epoch 45/125 - Loss: 0.1943, Val Loss: 0.1485\n",
      "Epoch 46/125 - Loss: 0.1891, Val Loss: 0.1480\n",
      "Epoch 47/125 - Loss: 0.1898, Val Loss: 0.1474\n",
      "Epoch 48/125 - Loss: 0.1859, Val Loss: 0.1468\n",
      "Epoch 49/125 - Loss: 0.1857, Val Loss: 0.1462\n",
      "Epoch 50/125 - Loss: 0.1875, Val Loss: 0.1456\n",
      "Epoch 51/125 - Loss: 0.1850, Val Loss: 0.1451\n",
      "Epoch 52/125 - Loss: 0.1824, Val Loss: 0.1446\n",
      "Epoch 53/125 - Loss: 0.1821, Val Loss: 0.1442\n",
      "Epoch 54/125 - Loss: 0.1846, Val Loss: 0.1439\n",
      "Epoch 55/125 - Loss: 0.1837, Val Loss: 0.1435\n",
      "Epoch 56/125 - Loss: 0.1805, Val Loss: 0.1431\n",
      "Epoch 57/125 - Loss: 0.1823, Val Loss: 0.1428\n",
      "Epoch 58/125 - Loss: 0.1803, Val Loss: 0.1424\n",
      "Epoch 59/125 - Loss: 0.1781, Val Loss: 0.1421\n",
      "Epoch 60/125 - Loss: 0.1769, Val Loss: 0.1418\n",
      "Epoch 61/125 - Loss: 0.1763, Val Loss: 0.1415\n",
      "Epoch 62/125 - Loss: 0.1756, Val Loss: 0.1412\n",
      "Epoch 63/125 - Loss: 0.1729, Val Loss: 0.1410\n",
      "Epoch 64/125 - Loss: 0.1725, Val Loss: 0.1408\n",
      "Epoch 65/125 - Loss: 0.1727, Val Loss: 0.1405\n",
      "Epoch 66/125 - Loss: 0.1737, Val Loss: 0.1403\n",
      "Epoch 67/125 - Loss: 0.1719, Val Loss: 0.1400\n",
      "Epoch 68/125 - Loss: 0.1715, Val Loss: 0.1398\n",
      "Epoch 69/125 - Loss: 0.1689, Val Loss: 0.1395\n",
      "Epoch 70/125 - Loss: 0.1712, Val Loss: 0.1393\n",
      "Epoch 71/125 - Loss: 0.1691, Val Loss: 0.1390\n",
      "Epoch 72/125 - Loss: 0.1705, Val Loss: 0.1387\n",
      "Epoch 73/125 - Loss: 0.1698, Val Loss: 0.1384\n",
      "Epoch 74/125 - Loss: 0.1668, Val Loss: 0.1381\n",
      "Epoch 75/125 - Loss: 0.1665, Val Loss: 0.1378\n",
      "Epoch 76/125 - Loss: 0.1655, Val Loss: 0.1376\n",
      "Epoch 77/125 - Loss: 0.1672, Val Loss: 0.1374\n",
      "Epoch 78/125 - Loss: 0.1660, Val Loss: 0.1372\n",
      "Epoch 79/125 - Loss: 0.1673, Val Loss: 0.1370\n",
      "Epoch 80/125 - Loss: 0.1640, Val Loss: 0.1368\n",
      "Epoch 81/125 - Loss: 0.1634, Val Loss: 0.1364\n",
      "Epoch 82/125 - Loss: 0.1619, Val Loss: 0.1361\n",
      "Epoch 83/125 - Loss: 0.1629, Val Loss: 0.1358\n",
      "Epoch 84/125 - Loss: 0.1633, Val Loss: 0.1355\n",
      "Epoch 85/125 - Loss: 0.1629, Val Loss: 0.1352\n",
      "Epoch 86/125 - Loss: 0.1599, Val Loss: 0.1351\n",
      "Epoch 87/125 - Loss: 0.1632, Val Loss: 0.1350\n",
      "Epoch 88/125 - Loss: 0.1607, Val Loss: 0.1350\n",
      "Epoch 89/125 - Loss: 0.1614, Val Loss: 0.1349\n",
      "Epoch 90/125 - Loss: 0.1575, Val Loss: 0.1347\n",
      "Epoch 91/125 - Loss: 0.1604, Val Loss: 0.1343\n",
      "Epoch 92/125 - Loss: 0.1595, Val Loss: 0.1340\n",
      "Epoch 93/125 - Loss: 0.1584, Val Loss: 0.1340\n",
      "Epoch 94/125 - Loss: 0.1584, Val Loss: 0.1343\n",
      "Epoch 95/125 - Loss: 0.1571, Val Loss: 0.1339\n",
      "Epoch 96/125 - Loss: 0.1562, Val Loss: 0.1334\n",
      "Epoch 97/125 - Loss: 0.1576, Val Loss: 0.1331\n",
      "Epoch 98/125 - Loss: 0.1548, Val Loss: 0.1330\n",
      "Epoch 99/125 - Loss: 0.1549, Val Loss: 0.1325\n",
      "Epoch 100/125 - Loss: 0.1564, Val Loss: 0.1321\n",
      "Epoch 101/125 - Loss: 0.1534, Val Loss: 0.1319\n",
      "Epoch 102/125 - Loss: 0.1551, Val Loss: 0.1316\n",
      "Epoch 103/125 - Loss: 0.1536, Val Loss: 0.1311\n",
      "Epoch 104/125 - Loss: 0.1536, Val Loss: 0.1308\n",
      "Epoch 105/125 - Loss: 0.1539, Val Loss: 0.1304\n",
      "Epoch 106/125 - Loss: 0.1541, Val Loss: 0.1300\n",
      "Epoch 107/125 - Loss: 0.1530, Val Loss: 0.1298\n",
      "Epoch 108/125 - Loss: 0.1506, Val Loss: 0.1294\n",
      "Epoch 109/125 - Loss: 0.1519, Val Loss: 0.1291\n",
      "Epoch 110/125 - Loss: 0.1497, Val Loss: 0.1288\n",
      "Epoch 111/125 - Loss: 0.1499, Val Loss: 0.1285\n",
      "Epoch 112/125 - Loss: 0.1504, Val Loss: 0.1282\n",
      "Epoch 113/125 - Loss: 0.1494, Val Loss: 0.1280\n",
      "Epoch 114/125 - Loss: 0.1496, Val Loss: 0.1278\n",
      "Epoch 115/125 - Loss: 0.1494, Val Loss: 0.1277\n",
      "Epoch 116/125 - Loss: 0.1489, Val Loss: 0.1275\n",
      "Epoch 117/125 - Loss: 0.1480, Val Loss: 0.1273\n",
      "Epoch 118/125 - Loss: 0.1480, Val Loss: 0.1270\n",
      "Epoch 119/125 - Loss: 0.1470, Val Loss: 0.1267\n",
      "Epoch 120/125 - Loss: 0.1479, Val Loss: 0.1263\n",
      "Epoch 121/125 - Loss: 0.1474, Val Loss: 0.1260\n",
      "Epoch 122/125 - Loss: 0.1464, Val Loss: 0.1256\n",
      "Epoch 123/125 - Loss: 0.1467, Val Loss: 0.1252\n",
      "Epoch 124/125 - Loss: 0.1464, Val Loss: 0.1247\n",
      "Epoch 125/125 - Loss: 0.1448, Val Loss: 0.1242\n",
      "=== GNN Model Evaluation ===\n",
      "RMSE: 107.83\n",
      "MAE: 56.40\n",
      "R²: 0.5497\n",
      "MAPE: 27.42%\n",
      "Split 5 Results - RMSE: 107.8261, MAE: 56.3995, R²: 0.5497\n",
      "Model for split 5 saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_model_split_5.pt\n",
      "Results saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_rolling_window_results.csv\n",
      "Daily metrics saved to ./output/enhanced_neighbor_lstm_model/enhanced_neighbor_lstm_rolling_window_metrics.csv\n",
      "\n",
      "===== EnhancedNeighborBasedLSTM ROLLING WINDOW CV SUMMARY =====\n",
      "Using 6291 listings for training and 1573 listings for testing\n",
      "\n",
      "=== Overall Metrics ===\n",
      "RMSE: 108.6296\n",
      "MAE: 57.2626\n",
      "R²: 0.5373\n",
      "MAPE: 27.5429%\n",
      "\n",
      "=== Split Performance ===\n",
      " split       rmse       mae       r2  n_samples\n",
      "     0 109.327963 58.204386 0.539972      11011\n",
      "     1 108.096148 57.312066 0.528372      11011\n",
      "     2 108.385077 57.643430 0.533594      11011\n",
      "     3 109.502233 56.753776 0.534209      11011\n",
      "     4 107.826068 56.399504 0.549668      11011\n",
      "EnhancedNeighborBasedLSTM model with rolling window CV completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Enhanced NeighborBasedLSTM with spatial GAT layers\n",
    "class EnhancedNeighborBasedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, spatial_features_dim, temporal_features_dim, property_features_dim, \n",
    "                 max_neighbors=5, lstm_hidden_dim=16, hidden_dim=64, dropout=0.3, heads=4, edge_dim=1):\n",
    "        super(EnhancedNeighborBasedLSTM, self).__init__()\n",
    "        \n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        \n",
    "        # GAT layers for spatial relationship processing\n",
    "        gat_out_dim = self.head_dim * heads\n",
    "        self.gat1 = GATv2Conv(spatial_features_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat2 = GATv2Conv(gat_out_dim, self.head_dim, heads=heads, edge_dim=edge_dim)\n",
    "        \n",
    "        # Batch normalization for GAT layers\n",
    "        self.bn1 = nn.BatchNorm1d(gat_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(gat_out_dim)\n",
    "        \n",
    "        # Optional dimension adjustment if needed\n",
    "        self.dim_adjust = None\n",
    "        if gat_out_dim != hidden_dim:\n",
    "            self.dim_adjust = nn.Linear(gat_out_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM for processing neighbor price histories (unchanged)\n",
    "        self.neighbor_lstm = nn.LSTM(\n",
    "            input_size=1,  # Single feature per timestep (price)\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for neighbor contributions (unchanged)\n",
    "        self.attention = nn.Linear(lstm_hidden_dim * 2, 1)\n",
    "        \n",
    "        # NEW: Add projection layer to match LSTM output dimension with hidden_dim\n",
    "        self.neighbor_projection = nn.Linear(lstm_hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Processing for property features (unchanged)\n",
    "        self.property_layer1 = nn.Linear(property_features_dim, hidden_dim)\n",
    "        self.property_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.property_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.property_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Processing for temporal features (unchanged)\n",
    "        self.temporal_layer1 = nn.Linear(temporal_features_dim, hidden_dim)\n",
    "        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.temporal_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Feature fusion mechanism - now including spatial features\n",
    "        self.fusion_layer = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.fusion_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout + 0.1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Unpack the data\n",
    "        property_features = data.property_features\n",
    "        temporal_features = data.temporal_features\n",
    "        neighbor_histories = data.neighbor_histories\n",
    "        neighbor_mask = data.neighbor_mask\n",
    "        \n",
    "        # Spatial graph data\n",
    "        x = data.x  # Spatial features\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        \n",
    "        batch_size = property_features.shape[0]\n",
    "        \n",
    "        # Process spatial features with GAT\n",
    "        spatial_features = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = F.elu(spatial_features)\n",
    "        spatial_features = self.bn1(spatial_features)\n",
    "        spatial_features = self.dropout(spatial_features)\n",
    "        \n",
    "        # Second GAT layer with residual connection\n",
    "        spatial_features_res = spatial_features\n",
    "        spatial_features = self.gat2(spatial_features, edge_index, edge_attr=edge_attr)\n",
    "        spatial_features = self.bn2(spatial_features)\n",
    "        \n",
    "        # Add residual connection if dimensions match\n",
    "        if spatial_features.shape == spatial_features_res.shape:\n",
    "            spatial_features = spatial_features + spatial_features_res\n",
    "        \n",
    "        # Apply dimension adjustment if needed\n",
    "        if self.dim_adjust is not None:\n",
    "            spatial_features = self.dim_adjust(spatial_features)\n",
    "        \n",
    "        # Only use spatial features for test listings (val_mask)\n",
    "        spatial_features = spatial_features[data.val_mask]\n",
    "        \n",
    "        # Process property features with residual connection (unchanged)\n",
    "        prop_out = F.relu(self.property_layer1(property_features))\n",
    "        prop_out = self.property_bn1(prop_out)\n",
    "        prop_out = self.dropout(prop_out)\n",
    "        prop_out_res = prop_out\n",
    "        prop_out = F.relu(self.property_layer2(prop_out))\n",
    "        prop_out = self.property_bn2(prop_out)\n",
    "        prop_out = prop_out + prop_out_res  # Residual connection\n",
    "        \n",
    "        # Process temporal features with residual connection (unchanged)\n",
    "        temp_out = F.relu(self.temporal_layer1(temporal_features))\n",
    "        temp_out = self.temporal_bn1(temp_out)\n",
    "        temp_out = self.dropout(temp_out)\n",
    "        temp_out_res = temp_out\n",
    "        temp_out = F.relu(self.temporal_layer2(temp_out))\n",
    "        temp_out = self.temporal_bn2(temp_out)\n",
    "        temp_out = temp_out + temp_out_res  # Residual connection\n",
    "        \n",
    "        # Process neighbor histories with LSTM (unchanged)\n",
    "        seq_len = neighbor_histories.size(2)\n",
    "        reshaped_histories = neighbor_histories.view(batch_size * self.max_neighbors, seq_len, 1)\n",
    "        \n",
    "        lstm_out, (h_n, _) = self.neighbor_lstm(reshaped_histories)\n",
    "        \n",
    "        h_forward = h_n[0]\n",
    "        h_backward = h_n[1]\n",
    "        h_combined = torch.cat([h_forward, h_backward], dim=1)\n",
    "        \n",
    "        h_combined = h_combined.view(batch_size, self.max_neighbors, -1)\n",
    "        \n",
    "        # Apply attention to weight the neighbors' contributions (unchanged)\n",
    "        attention_scores = self.attention(h_combined)\n",
    "        attention_scores = attention_scores.squeeze(-1)\n",
    "        \n",
    "        attention_scores = attention_scores.masked_fill(~neighbor_mask, -1e9)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(-1)\n",
    "        \n",
    "        weighted_features = h_combined * attention_weights\n",
    "        neighbor_context = weighted_features.sum(dim=1)  # [batch, lstm_hidden_dim*2]\n",
    "        \n",
    "        # NEW: Project neighbor context to match hidden_dim\n",
    "        neighbor_context = self.neighbor_projection(neighbor_context)\n",
    "        \n",
    "        # Dynamic feature fusion with learned weights\n",
    "        concatenated_features = torch.cat([spatial_features, prop_out, temp_out, neighbor_context], dim=1)\n",
    "        fused_features = self.fusion_layer(concatenated_features)\n",
    "        fused_features = self.fusion_bn(fused_features)\n",
    "        fused_features = F.relu(fused_features)  # Add non-linearity for better feature extraction\n",
    "        \n",
    "        # Final prediction layers\n",
    "        out = F.relu(self.fc1(fused_features))\n",
    "        out = self.fc1_bn(out)\n",
    "        out = self.dropout_heavy(out)\n",
    "        price_prediction = self.fc2(out)\n",
    "        \n",
    "        return price_prediction\n",
    "    \n",
    "def build_enhanced_spatial_graph_for_new_listings(train_data, test_data, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Build a graph with edge weights based on both geographic and feature similarity\n",
    "    for new listings\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    train_coords = train_data[['latitude', 'longitude']].values\n",
    "    test_coords = test_data[['latitude', 'longitude']].values\n",
    "    \n",
    "    print(f\"Building enhanced spatial graph with {len(test_coords)} test listings and {k} nearest neighbors...\")\n",
    "    \n",
    "    # Extract and normalize key features for similarity calculation\n",
    "    features = ['accommodates', 'bedrooms', 'bathrooms']\n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        scaler = StandardScaler()\n",
    "        train_features = scaler.fit_transform(train_data[available_features].fillna(0))\n",
    "        test_features = scaler.transform(test_data[available_features].fillna(0))\n",
    "    else:\n",
    "        # Fallback if no features are available\n",
    "        print(\"Warning: No property features available for similarity calculation\")\n",
    "        train_features = np.ones((len(train_coords), 1))\n",
    "        test_features = np.ones((len(test_coords), 1))\n",
    "    \n",
    "    # Find k nearest neighbors for each test listing\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(train_coords)))\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Create edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for test_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(indices, distances)):\n",
    "        test_feat = test_features[test_idx]\n",
    "        \n",
    "        for train_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "            # Calculate feature similarity (cosine similarity)\n",
    "            train_feat = train_features[train_idx]\n",
    "            feat_norm_product = np.linalg.norm(test_feat) * np.linalg.norm(train_feat)\n",
    "            \n",
    "            if feat_norm_product > 1e-8:  # Avoid division by zero\n",
    "                feat_sim = np.dot(test_feat, train_feat) / feat_norm_product\n",
    "            else:\n",
    "                feat_sim = 0.0\n",
    "            \n",
    "            # Normalize distance for better numerical stability\n",
    "            geo_weight = 1.0 / (distance + 1e-6)\n",
    "            \n",
    "            # Combined weight: (1-α) * geo_weight + α * feature_weight\n",
    "            combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "            \n",
    "            # Add edge from test listing to train listing\n",
    "            edge_index.append([test_idx + len(train_data), train_idx])\n",
    "            edge_attr.append([combined_weight])\n",
    "            \n",
    "            # Add reverse edge\n",
    "            edge_index.append([train_idx, test_idx + len(train_data)])\n",
    "            edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Add edges between training listings (optional, for smaller datasets)\n",
    "    if len(train_coords) <= 5000:\n",
    "        train_nn = NearestNeighbors(n_neighbors=min(5, len(train_coords) - 1))\n",
    "        train_nn.fit(train_coords)\n",
    "        train_distances, train_indices = train_nn.kneighbors(train_coords)\n",
    "        \n",
    "        for train_idx, (neighbor_indices, neighbor_distances) in enumerate(zip(train_indices, train_distances)):\n",
    "            for neighbor_idx, distance in zip(neighbor_indices, neighbor_distances):\n",
    "                if train_idx != neighbor_idx:  # Skip self-loops\n",
    "                    # Calculate feature similarity\n",
    "                    train_feat_i = train_features[train_idx]\n",
    "                    train_feat_j = train_features[neighbor_idx]\n",
    "                    \n",
    "                    feat_norm_product = np.linalg.norm(train_feat_i) * np.linalg.norm(train_feat_j)\n",
    "                    if feat_norm_product > 1e-8:\n",
    "                        feat_sim = np.dot(train_feat_i, train_feat_j) / feat_norm_product\n",
    "                    else:\n",
    "                        feat_sim = 0.0\n",
    "                    \n",
    "                    geo_weight = 1.0 / (distance + 1e-6)\n",
    "                    combined_weight = (1 - feature_weight) * geo_weight + feature_weight * max(0, feat_sim)\n",
    "                    \n",
    "                    edge_index.append([train_idx, neighbor_idx])\n",
    "                    edge_attr.append([combined_weight])\n",
    "    \n",
    "    # Convert to tensors with explicit dtype\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index_tensor.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index_tensor, edge_attr_tensor\n",
    "\n",
    "def prepare_data_for_enhanced_neighbor_lstm(train_data, test_data, neighbor_dict, \n",
    "                                         spatial_features, temporal_features, property_features,\n",
    "                                         property_scaler=None, temporal_scaler=None, \n",
    "                                         spatial_scaler=None, target_scaler=None, \n",
    "                                         max_neighbors=5, seq_length=30, k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Prepare data for the EnhancedNeighborBasedLSTM model with spatial GAT\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for EnhancedNeighborBasedLSTM...\")\n",
    "    \n",
    "    # Initialize or use provided scalers\n",
    "    if property_scaler is None:\n",
    "        property_scaler = StandardScaler()\n",
    "        property_scaler.fit(train_data[property_features])\n",
    "    \n",
    "    if temporal_scaler is None:\n",
    "        temporal_scaler = StandardScaler()\n",
    "        temporal_scaler.fit(train_data[temporal_features])\n",
    "        \n",
    "    if spatial_scaler is None:\n",
    "        spatial_scaler = StandardScaler()\n",
    "        spatial_scaler.fit(train_data[spatial_features])\n",
    "    \n",
    "    if target_scaler is None:\n",
    "        target_scaler = StandardScaler()\n",
    "        target_scaler.fit(train_data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Scale property features\n",
    "    X_train_property = property_scaler.transform(train_data[property_features]).astype(np.float32)\n",
    "    X_test_property = property_scaler.transform(test_data[property_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale temporal features\n",
    "    X_train_temporal = temporal_scaler.transform(train_data[temporal_features]).astype(np.float32)\n",
    "    X_test_temporal = temporal_scaler.transform(test_data[temporal_features]).astype(np.float32)\n",
    "    \n",
    "    # Scale spatial features\n",
    "    X_train_spatial = spatial_scaler.transform(train_data[spatial_features]).astype(np.float32)\n",
    "    X_test_spatial = spatial_scaler.transform(test_data[spatial_features]).astype(np.float32)\n",
    "    \n",
    "    # Prepare neighbor histories for test data\n",
    "    neighbor_histories, neighbor_masks = prepare_neighbor_data_batch(\n",
    "        test_data, train_data, neighbor_dict, max_neighbors, seq_length\n",
    "    )\n",
    "    \n",
    "    # Scale the target variable\n",
    "    y_train = target_scaler.transform(train_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    y_test = target_scaler.transform(test_data['price'].values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    \n",
    "    # Build spatial graph\n",
    "    edge_index, edge_attr = build_enhanced_spatial_graph_for_new_listings(\n",
    "        train_data, test_data, k=k, feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Combine spatial features for graph\n",
    "    X_combined_spatial = np.vstack([X_train_spatial, X_test_spatial])\n",
    "    \n",
    "    # Create data object\n",
    "    data_obj = Data(\n",
    "        x=torch.FloatTensor(X_combined_spatial),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        property_features=torch.FloatTensor(X_test_property),\n",
    "        temporal_features=torch.FloatTensor(X_test_temporal),\n",
    "        neighbor_histories=torch.FloatTensor(neighbor_histories),\n",
    "        neighbor_mask=torch.BoolTensor(neighbor_masks),\n",
    "        y=torch.FloatTensor(y_test.reshape(-1, 1)),\n",
    "        train_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool),\n",
    "        val_mask=torch.zeros(len(X_combined_spatial), dtype=torch.bool),\n",
    "    )\n",
    "    \n",
    "    # Set masks\n",
    "    data_obj.train_mask[:len(X_train_spatial)] = True\n",
    "    data_obj.val_mask[len(X_train_spatial):] = True\n",
    "    \n",
    "    print(f\"Prepared data with {len(X_combined_spatial)} nodes and {edge_index.shape[1]} edges\")\n",
    "    \n",
    "    return data_obj, property_scaler, temporal_scaler, spatial_scaler, target_scaler\n",
    "\n",
    "def train_enhanced_neighbor_lstm_model(train_data, test_data, neighbor_dict, spatial_features,\n",
    "                                    temporal_features, property_features, max_neighbors=5, \n",
    "                                    seq_length=30, lstm_hidden_dim=16, hidden_dim=64,\n",
    "                                    epochs=50, lr=0.001, device='cuda', k=10, feature_weight=0.3):\n",
    "    \"\"\"\n",
    "    Train the EnhancedNeighborBasedLSTM model\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Training EnhancedNeighborBasedLSTM Model =====\")\n",
    "    print(f\"LSTM hidden dimension: {lstm_hidden_dim}, Max neighbors: {max_neighbors}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    data_obj, property_scaler, temporal_scaler, spatial_scaler, target_scaler = prepare_data_for_enhanced_neighbor_lstm(\n",
    "        train_data, test_data, neighbor_dict, spatial_features, temporal_features, property_features,\n",
    "        max_neighbors=max_neighbors, seq_length=seq_length, k=k, feature_weight=feature_weight\n",
    "    )\n",
    "    \n",
    "    # Move data to device\n",
    "    data_obj = data_obj.to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedNeighborBasedLSTM(\n",
    "        input_dim=1,  # Single price feature\n",
    "        spatial_features_dim=len(spatial_features),\n",
    "        temporal_features_dim=len(temporal_features),\n",
    "        property_features_dim=len(property_features),\n",
    "        max_neighbors=max_neighbors,\n",
    "        lstm_hidden_dim=lstm_hidden_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3,\n",
    "        heads=4,\n",
    "        edge_dim=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data_obj)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(out, data_obj.y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data_obj)\n",
    "            val_loss = criterion(val_out, data_obj.y)\n",
    "            \n",
    "            # Convert predictions back to original scale for metrics\n",
    "            val_pred_orig = np.expm1(target_scaler.inverse_transform(val_out.cpu().numpy()))\n",
    "            val_true_orig = np.expm1(target_scaler.inverse_transform(data_obj.y.cpu().numpy()))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true_orig, val_pred_orig))\n",
    "            val_mae = mean_absolute_error(val_true_orig, val_pred_orig)\n",
    "            \n",
    "        # Store history\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "              f\"RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory management\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, property_scaler, temporal_scaler, spatial_scaler, target_scaler, history\n",
    "\n",
    "def run_enhanced_neighbor_lstm_with_rolling_window_cv(train_path, train_ids_path, test_ids_path, neighbor_csv_path,\n",
    "                                                   output_dir=None, window_size=35, n_splits=5,\n",
    "                                                   max_neighbors=5, seq_length=30, lstm_hidden_dim=8,\n",
    "                                                   hidden_dim=64, epochs=50, lr=0.001, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run EnhancedNeighborBasedLSTM model with rolling window cross-validation\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {os.path.basename(train_path)}\")\n",
    "    \n",
    "    # Create output directory if not exists\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        \n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        # Load neighbor data\n",
    "        neighbor_dict = load_neighbor_data(neighbor_csv_path)\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # For testing - take only a small sample of listings if specified\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.8), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.2), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Filter data to include only dates in the desired range\n",
    "        start_date = pd.to_datetime('2023-07-08')\n",
    "        end_date = pd.to_datetime('2024-02-08')\n",
    "        train_data = train_data[(train_data['date'] >= start_date) & (train_data['date'] <= end_date)]\n",
    "        \n",
    "        # Apply log transformation to price\n",
    "        train_data = apply_price_transformation(train_data)\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Check for NaN values in the dataset and fill them\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    # Fill with median for numeric columns\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    # For non-numeric, fill with mode\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Define feature groups\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        property_features = [\n",
    "            'accommodates', 'bedrooms', 'bathrooms',\n",
    "            'amenity_count', 'luxury_score', 'essential_score'\n",
    "        ]\n",
    "        \n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_data.columns]\n",
    "        property_features = [f for f in property_features if f in train_data.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_data.columns]\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(property_features)} property features, and {len(temporal_features)} temporal features\")\n",
    "        \n",
    "        # Get unique dates and ensure they're properly sorted\n",
    "        unique_dates = sorted(train_data['date'].dt.date.unique())\n",
    "        \n",
    "        # Create explicit test periods - last 35 days split into 5 equal parts (7 days each)\n",
    "        last_35_days = unique_dates[-window_size:]\n",
    "        \n",
    "        # Define explicit test periods - each 7 days\n",
    "        test_periods = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = i * (window_size // n_splits)\n",
    "            end_idx = start_idx + (window_size // n_splits)\n",
    "            # Make sure we don't go beyond the available data\n",
    "            if end_idx <= len(last_35_days):\n",
    "                test_periods.append((last_35_days[start_idx], last_35_days[end_idx-1]))\n",
    "        \n",
    "        # Adjust n_splits if we couldn't create enough test periods\n",
    "        n_splits = len(test_periods)\n",
    "        \n",
    "        print(f\"Created {n_splits} test periods:\")\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"  Period {i+1}: {test_start} to {test_end}\")\n",
    "        \n",
    "        # Storage for results\n",
    "        cv_results = []\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        split_metrics = []\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Run time series cross-validation using our explicit test periods\n",
    "        for i, (test_start, test_end) in enumerate(test_periods):\n",
    "            print(f\"\\n===== Split {i+1}/{n_splits} =====\")\n",
    "            \n",
    "            # Define training period: everything before test_start\n",
    "            train_end = pd.to_datetime(test_start) - pd.Timedelta(days=1)\n",
    "            train_end_date = train_end.date()\n",
    "            \n",
    "            print(f\"Training period: {unique_dates[0]} to {train_end_date}\")\n",
    "            print(f\"Testing period: {test_start} to {test_end}\")\n",
    "            \n",
    "            # Split by date first\n",
    "            train_date_mask = train_data['date'].dt.date <= train_end_date\n",
    "            test_date_mask = (train_data['date'].dt.date >= test_start) & (train_data['date'].dt.date <= test_end)\n",
    "            \n",
    "            date_filtered_train = train_data[train_date_mask]\n",
    "            date_filtered_test = train_data[test_date_mask]\n",
    "            \n",
    "            # Now further split by listing IDs\n",
    "            train_id_mask = date_filtered_train['listing_id'].isin(train_listing_ids)\n",
    "            test_id_mask = date_filtered_test['listing_id'].isin(test_listing_ids)\n",
    "            \n",
    "            split_train_data = date_filtered_train[train_id_mask].copy()\n",
    "            split_test_data = date_filtered_test[test_id_mask].copy()\n",
    "            \n",
    "            print(f\"Train data: {len(split_train_data)} rows, {len(split_train_data['listing_id'].unique())} unique listings\")\n",
    "            print(f\"Test data: {len(split_test_data)} rows, {len(split_test_data['listing_id'].unique())} unique listings\")\n",
    "            \n",
    "            # Check if we have enough data for this split\n",
    "            if len(split_train_data) < 100 or len(split_test_data) < 10:\n",
    "                print(f\"Insufficient data for split {i+1}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Manage memory before training\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train model for this split\n",
    "            try:\n",
    "                print(f\"\\n----- Training EnhancedNeighborBasedLSTM Model (Split {i+1}) -----\")\n",
    "                \n",
    "                # Prepare data for enhanced model\n",
    "                data_obj, property_scaler, temporal_scaler, spatial_scaler, target_scaler = prepare_data_for_enhanced_neighbor_lstm(\n",
    "                    split_train_data, split_test_data, neighbor_dict, \n",
    "                    spatial_features, temporal_features, property_features,\n",
    "                    max_neighbors=max_neighbors, seq_length=seq_length\n",
    "                )\n",
    "                \n",
    "                # Move data to device\n",
    "                data_obj = data_obj.to(device)\n",
    "                \n",
    "                # Initialize enhanced model\n",
    "                model = EnhancedNeighborBasedLSTM(\n",
    "                    input_dim=1,\n",
    "                    spatial_features_dim=len(spatial_features),\n",
    "                    temporal_features_dim=len(temporal_features),\n",
    "                    property_features_dim=len(property_features),\n",
    "                    max_neighbors=max_neighbors,\n",
    "                    lstm_hidden_dim=lstm_hidden_dim,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    dropout=0.3,\n",
    "                    heads=4,\n",
    "                    edge_dim=1\n",
    "                ).to(device)\n",
    "                \n",
    "                # Initialize optimizer and loss\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.HuberLoss(delta=1.0)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "                )\n",
    "                \n",
    "                # Training loop\n",
    "                best_val_loss = float('inf')\n",
    "                best_model_state = None\n",
    "                patience = 10\n",
    "                counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    out = model(data_obj)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(out, data_obj.y)\n",
    "                    \n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_out = model(data_obj)\n",
    "                        val_loss = criterion(val_out, data_obj.y)\n",
    "                        \n",
    "                        # Print progress\n",
    "                        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "                        \n",
    "                        # Learning rate scheduling\n",
    "                        scheduler.step(val_loss)\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_model_state = model.state_dict().copy()\n",
    "                            counter = 0\n",
    "                        else:\n",
    "                            counter += 1\n",
    "                        \n",
    "                        if counter >= patience:\n",
    "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                            break\n",
    "                    \n",
    "                    # Memory management\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Load best model\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                \n",
    "                # Get predictions\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(data_obj)\n",
    "                    predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "                    predictions_orig = np.expm1(predictions_np)\n",
    "                \n",
    "                # Get actual test values (original scale)\n",
    "                test_actuals = split_test_data['original_price'].values if 'original_price' in split_test_data.columns else split_test_data['price'].values\n",
    "                \n",
    "                # Evaluate predictions\n",
    "                metrics = evaluate_gnn_predictions(test_actuals, predictions_orig.flatten(), print_results=True)\n",
    "                \n",
    "                print(f\"Split {i+1} Results - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "                \n",
    "                # Store results for this split\n",
    "                split_results = pd.DataFrame({\n",
    "                    'split': i,\n",
    "                    'date': split_test_data['date'],\n",
    "                    'listing_id': split_test_data['listing_id'],\n",
    "                    'price': test_actuals,\n",
    "                    'predicted': predictions_orig.flatten(),\n",
    "                    'error': test_actuals - predictions_orig.flatten(),\n",
    "                    'abs_error': np.abs(test_actuals - predictions_orig.flatten()),\n",
    "                    'pct_error': np.abs((test_actuals - predictions_orig.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "                })\n",
    "                \n",
    "                cv_results.append(split_results)\n",
    "                all_predictions.extend(predictions_orig.flatten())\n",
    "                all_targets.extend(test_actuals)\n",
    "                \n",
    "                # Save model for this split if output_dir is provided\n",
    "                if output_dir:\n",
    "                    model_path = os.path.join(output_dir, f'enhanced_neighbor_lstm_model_split_{i+1}.pt')\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"Model for split {i+1} saved to {model_path}\")\n",
    "                \n",
    "                # Store metrics for this split\n",
    "                split_metrics.append({\n",
    "                    'split': i,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mae': metrics['mae'],\n",
    "                    'r2': metrics['r2'],\n",
    "                    'mape': metrics['mape'],\n",
    "                    'n_samples': len(test_actuals)\n",
    "                })\n",
    "                \n",
    "                # Memory management after each split\n",
    "                del model, property_scaler, temporal_scaler, spatial_scaler, target_scaler\n",
    "                del predictions_orig, data_obj, split_train_data, split_test_data\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in split {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if not cv_results:\n",
    "            print(\"No valid splits completed. Check your data and parameters.\")\n",
    "            return None\n",
    "                \n",
    "        all_results = pd.concat(cv_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        all_targets_array = np.array(all_targets)\n",
    "        all_predictions_array = np.array(all_predictions)\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(all_targets_array, all_predictions_array)),\n",
    "            'mae': mean_absolute_error(all_targets_array, all_predictions_array),\n",
    "            'r2': r2_score(all_targets_array, all_predictions_array),\n",
    "            'mape': np.mean(np.abs((all_targets_array - all_predictions_array) / (all_targets_array + 1e-8))) * 100\n",
    "        }\n",
    "        \n",
    "        # Calculate daily metrics\n",
    "        all_results['date_str'] = pd.to_datetime(all_results['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        daily_metrics = []\n",
    "        for day, group in all_results.groupby('date_str'):\n",
    "            y_true_day = group['price']\n",
    "            y_pred_day = group['predicted']\n",
    "            \n",
    "            daily_metrics.append({\n",
    "                'date': day,\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_day, y_pred_day)),\n",
    "                'mae': mean_absolute_error(y_true_day, y_pred_day),\n",
    "                'r2': r2_score(y_true_day, y_pred_day) if len(set(y_true_day)) > 1 else np.nan,\n",
    "                'mape': np.mean(np.abs((y_true_day - y_pred_day) / (y_true_day + 1e-8))) * 100,\n",
    "                'n_samples': len(y_true_day)\n",
    "            })\n",
    "        \n",
    "        daily_metrics_df = pd.DataFrame(daily_metrics)\n",
    "        daily_metrics_df['date'] = pd.to_datetime(daily_metrics_df['date'])\n",
    "        daily_metrics_df = daily_metrics_df.sort_values('date')\n",
    "        \n",
    "        split_metrics_df = pd.DataFrame(split_metrics)\n",
    "        \n",
    "        # Create a results dictionary\n",
    "        evaluation_results = {\n",
    "            'overall_metrics': overall_metrics,\n",
    "            'split_metrics': split_metrics_df,\n",
    "            'daily_metrics': daily_metrics_df,\n",
    "            'all_results': all_results,\n",
    "            'train_listings': len(train_listing_ids),\n",
    "            'test_listings': len(test_listing_ids)\n",
    "        }\n",
    "        \n",
    "        # Save results if output directory is provided\n",
    "        if output_dir:\n",
    "            # Save all results\n",
    "            results_file = os.path.join(output_dir, 'enhanced_neighbor_lstm_rolling_window_results.csv')\n",
    "            all_results.to_csv(results_file, index=False)\n",
    "            print(f\"Results saved to {results_file}\")\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics_file = os.path.join(output_dir, 'enhanced_neighbor_lstm_rolling_window_metrics.csv')\n",
    "            daily_metrics_df.to_csv(metrics_file, index=False)\n",
    "            print(f\"Daily metrics saved to {metrics_file}\")\n",
    "            \n",
    "            # Save summary\n",
    "            with open(os.path.join(output_dir, 'enhanced_neighbor_lstm_cv_summary.txt'), 'w') as f:\n",
    "                f.write(f\"EnhancedNeighborBasedLSTM Rolling Window CV Model Summary\\n\")\n",
    "                f.write(f\"=================================\\n\\n\")\n",
    "                f.write(f\"Window size: {window_size} days\\n\")\n",
    "                f.write(f\"Number of splits: {n_splits}\\n\")\n",
    "                f.write(f\"Training period: {unique_dates[0]} to {unique_dates[-1]}\\n\")\n",
    "                f.write(f\"Number of training listings: {len(train_listing_ids)}\\n\")\n",
    "                f.write(f\"Number of test listings: {len(test_listing_ids)}\\n\\n\")\n",
    "                f.write(f\"LSTM hidden dimension: {lstm_hidden_dim}\\n\")\n",
    "                f.write(f\"Maximum neighbors per listing: {max_neighbors}\\n\\n\")\n",
    "                f.write(f\"Overall Metrics:\\n\")\n",
    "                for k, v in overall_metrics.items():\n",
    "                    f.write(f\"  {k}: {v:.6f}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== EnhancedNeighborBasedLSTM ROLLING WINDOW CV SUMMARY =====\")\n",
    "        print(f\"Using {len(train_listing_ids)} listings for training and {len(test_listing_ids)} listings for testing\")\n",
    "        \n",
    "        print(\"\\n=== Overall Metrics ===\")\n",
    "        print(f\"RMSE: {overall_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {overall_metrics['mae']:.4f}\")\n",
    "        print(f\"R²: {overall_metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE: {overall_metrics['mape']:.4f}%\")\n",
    "        \n",
    "        print(\"\\n=== Split Performance ===\")\n",
    "        print(split_metrics_df[['split', 'rmse', 'mae', 'r2', 'n_samples']].to_string(index=False))\n",
    "        \n",
    "        # Return evaluation results\n",
    "        return evaluation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in rolling window CV: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_enhanced_neighbor_lstm_model(train_path, train_ids_path, test_ids_path, neighbor_csv_path, \n",
    "                                  output_dir=None, lstm_hidden_dim=8, hidden_dim=64, max_neighbors=5,\n",
    "                                  seq_length=30, epochs=50, lr=0.001, sample_size=None):\n",
    "    \"\"\"\n",
    "    A minimal wrapper to use the enhanced spatial model instead of the original\n",
    "    \"\"\"\n",
    "    # Use the same initialization as the original function\n",
    "    try:\n",
    "        # Create output directory if not exists\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load training data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        \n",
    "        # Load listing IDs for train/test split\n",
    "        print(\"Loading train/test listing IDs...\")\n",
    "        with open(train_ids_path, 'r') as f:\n",
    "            train_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        with open(test_ids_path, 'r') as f:\n",
    "            test_listing_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(train_listing_ids)} train IDs and {len(test_listing_ids)} test IDs\")\n",
    "        \n",
    "        # Load neighbor data\n",
    "        neighbor_dict = load_neighbor_data(neighbor_csv_path)\n",
    "        \n",
    "        # Sample size handling (unchanged)\n",
    "        if sample_size:\n",
    "            print(f\"Limiting to {sample_size} random listings for testing\")\n",
    "            np.random.seed(42)\n",
    "            selected_train = np.random.choice(train_listing_ids, int(sample_size * 0.8), replace=False)\n",
    "            selected_test = np.random.choice(test_listing_ids, int(sample_size * 0.2), replace=False)\n",
    "            train_listing_ids = selected_train.tolist()\n",
    "            test_listing_ids = selected_test.tolist()\n",
    "        \n",
    "        # Data preprocessing (unchanged)\n",
    "        if 'date' in train_data.columns and not pd.api.types.is_datetime64_any_dtype(train_data['date']):\n",
    "            train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "        \n",
    "        # Create calculated features\n",
    "        print(\"Creating calculated features...\")\n",
    "        train_data = create_calculated_features(train_data)\n",
    "        \n",
    "        # Handle NaN values (unchanged)\n",
    "        nan_columns = train_data.columns[train_data.isna().any()].tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"Warning: Found NaN values in columns: {nan_columns}\")\n",
    "            print(\"Filling NaN values with column means/medians\")\n",
    "            \n",
    "            for col in nan_columns:\n",
    "                if np.issubdtype(train_data[col].dtype, np.number):\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "                else:\n",
    "                    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "        \n",
    "        # Split data into train and test based on listing IDs (unchanged)\n",
    "        train_mask = train_data['listing_id'].isin(train_listing_ids)\n",
    "        test_mask = train_data['listing_id'].isin(test_listing_ids)\n",
    "        \n",
    "        train_df = train_data[train_mask].copy()\n",
    "        test_df = train_data[test_mask].copy()\n",
    "        \n",
    "        print(f\"Train data: {len(train_df)} rows, {len(train_df['listing_id'].unique())} unique listings\")\n",
    "        print(f\"Test data: {len(test_df)} rows, {len(test_df['listing_id'].unique())} unique listings\")\n",
    "        \n",
    "        # Define feature groups, now including spatial features\n",
    "        spatial_features = [\n",
    "            'latitude', 'longitude'\n",
    "        ]\n",
    "        \n",
    "        property_features = [\n",
    "            'accommodates', 'bedrooms', 'bathrooms',\n",
    "            'amenity_count', 'luxury_score', 'essential_score', 'bedroom_ratio'\n",
    "        ]\n",
    "        \n",
    "        temporal_features = [\n",
    "            'DTF_day_of_week', 'DTF_month', 'DTF_is_weekend',\n",
    "            'DTF_season_sin', 'DTF_season_cos'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature lists only contain columns that exist in the dataset\n",
    "        spatial_features = [f for f in spatial_features if f in train_df.columns]\n",
    "        property_features = [f for f in property_features if f in train_df.columns]\n",
    "        temporal_features = [f for f in temporal_features if f in train_df.columns]\n",
    "        \n",
    "        print(f\"Using {len(spatial_features)} spatial features, {len(property_features)} property features, and {len(temporal_features)} temporal features\")\n",
    "        \n",
    "        # Apply log transformation to prices\n",
    "        train_df = apply_price_transformation(train_df)\n",
    "        test_df = apply_price_transformation(test_df)\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Memory management before training\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train enhanced model\n",
    "        print(\"\\n===== Training Enhanced NeighborBasedLSTM Model with Spatial GAT =====\")\n",
    "        model, property_scaler, temporal_scaler, spatial_scaler, target_scaler, history = train_enhanced_neighbor_lstm_model(\n",
    "            train_df, test_df, neighbor_dict, \n",
    "            spatial_features, temporal_features, property_features,\n",
    "            max_neighbors=max_neighbors, \n",
    "            seq_length=seq_length, \n",
    "            lstm_hidden_dim=lstm_hidden_dim,\n",
    "            hidden_dim=hidden_dim, \n",
    "            epochs=epochs, \n",
    "            lr=lr, \n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Plot training history (unchanged)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['val_rmse'], label='Validation RMSE')\n",
    "        plt.title('Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['lr'], label='Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('LR')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, 'enhanced_neighbor_lstm_training_history.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # For evaluation, we can use the last predicted values\n",
    "        data_obj, _, _, _, _ = prepare_data_for_enhanced_neighbor_lstm(\n",
    "            train_df, test_df, neighbor_dict, \n",
    "            spatial_features, temporal_features, property_features,\n",
    "            property_scaler, temporal_scaler, spatial_scaler, target_scaler,\n",
    "            max_neighbors=max_neighbors, seq_length=seq_length\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_obj = data_obj.to(device)\n",
    "            predictions = model(data_obj)\n",
    "            predictions_np = target_scaler.inverse_transform(predictions.cpu().numpy())\n",
    "            predictions_orig = np.expm1(predictions_np)\n",
    "        \n",
    "        # Get actual test values\n",
    "        test_actuals = test_df['original_price'].values if 'original_price' in test_df.columns else test_df['price'].values\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        test_metrics = evaluate_gnn_predictions(test_actuals, predictions_orig.flatten(), print_results=True)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_gnn_results(test_actuals, predictions_orig.flatten(), history, output_dir)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        if output_dir:\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'enhanced_neighbor_lstm_model.pt'))\n",
    "            torch.save({\n",
    "                'property_scaler': property_scaler,\n",
    "                'temporal_scaler': temporal_scaler,\n",
    "                'spatial_scaler': spatial_scaler,\n",
    "                'target_scaler': target_scaler,\n",
    "                'max_neighbors': max_neighbors,\n",
    "                'seq_length': seq_length,\n",
    "                'lstm_hidden_dim': lstm_hidden_dim\n",
    "            }, os.path.join(output_dir, 'enhanced_neighbor_lstm_scalers.pt'))\n",
    "            print(f\"Model and scalers saved to {output_dir}\")\n",
    "            \n",
    "            # Save test predictions\n",
    "            test_results = pd.DataFrame({\n",
    "                'listing_id': test_df['listing_id'].values,\n",
    "                'date': test_df['date'].values,\n",
    "                'actual': test_actuals,\n",
    "                'predicted': predictions_orig.flatten(),\n",
    "                'error': test_actuals - predictions_orig.flatten(),\n",
    "                'abs_error': np.abs(test_actuals - predictions_orig.flatten()),\n",
    "                'pct_error': np.abs((test_actuals - predictions_orig.flatten()) / (test_actuals + 1e-8)) * 100\n",
    "            })\n",
    "            test_results.to_csv(os.path.join(output_dir, 'enhanced_neighbor_lstm_test_predictions.csv'), index=False)\n",
    "            print(f\"Test predictions saved to {os.path.join(output_dir, 'enhanced_neighbor_lstm_test_predictions.csv')}\")\n",
    "        \n",
    "        # Return model and metrics\n",
    "        return model, property_scaler, temporal_scaler, spatial_scaler, target_scaler, test_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Enhanced NeighborBasedLSTM model training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    train_path = \"train_up3.csv\"\n",
    "    train_ids_path = \"train_ids.txt\"\n",
    "    test_ids_path = \"test_ids.txt\"\n",
    "    neighbor_csv_path = \"./neighbor_data/neighbor_dict.csv\"  # Path to neighbor information CSV\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./output/enhanced_neighbor_lstm_model\"  # Change directory name to reflect enhanced model\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters\n",
    "    lstm_hidden_dim = 32         # Hidden dimension for LSTM (8 or 16)\n",
    "    hidden_dim = 64             # Hidden dimension size for rest of model\n",
    "    max_neighbors = 5           # Maximum number of neighbors to consider per listing\n",
    "    seq_length = 60             # Sequence length for neighbor price history\n",
    "    epochs = 125                # Maximum number of epochs\n",
    "    lr = 0.001                  # Learning rate\n",
    "    \n",
    "    # Choose between different run modes\n",
    "    run_mode = \"rolling_window\"  # Options: \"single\", \"rolling_window\", \"compare_dims\"\n",
    "    \n",
    "    try:\n",
    "        if run_mode == \"single\":\n",
    "            # Change this line to use the enhanced model\n",
    "            result_tuple = run_enhanced_neighbor_lstm_model(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                neighbor_csv_path=neighbor_csv_path,\n",
    "                output_dir=output_dir,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                epochs=epochs,\n",
    "                lr=lr,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "            )\n",
    "            \n",
    "            if result_tuple:\n",
    "                model, property_scaler, temporal_scaler, spatial_scaler, target_scaler, test_metrics = result_tuple\n",
    "                print(\"Enhanced NeighborBasedLSTM model training completed successfully!\")\n",
    "                \n",
    "                # Print summary of model\n",
    "                print(\"\\n===== Model Summary =====\")\n",
    "                total_param = sum(p.numel() for p in model.parameters())\n",
    "                lstm_param = sum(p.numel() for name, p in model.named_parameters() if 'lstm' in name)\n",
    "                gat_param = sum(p.numel() for name, p in model.named_parameters() if 'gat' in name)\n",
    "                print(f\"Total model parameters: {total_param:,}\")\n",
    "                print(f\"LSTM parameters: {lstm_param:,}\")\n",
    "                print(f\"GAT parameters: {gat_param:,}\")\n",
    "                print(f\"LSTM parameters as % of total: {lstm_param/total_param*100:.2f}%\")\n",
    "                print(f\"GAT parameters as % of total: {gat_param/total_param*100:.2f}%\")\n",
    "                \n",
    "        # [rest of the if-elif blocks remain unchanged]\n",
    "        elif run_mode == \"rolling_window\":\n",
    "            # Run with rolling window cross-validation using enhanced model\n",
    "            results = run_enhanced_neighbor_lstm_with_rolling_window_cv(\n",
    "                train_path=train_path,\n",
    "                train_ids_path=train_ids_path,\n",
    "                test_ids_path=test_ids_path,\n",
    "                neighbor_csv_path=neighbor_csv_path,\n",
    "                output_dir=output_dir,\n",
    "                window_size=35,  # 5 weeks\n",
    "                n_splits=5,\n",
    "                max_neighbors=max_neighbors,\n",
    "                seq_length=seq_length,\n",
    "                lstm_hidden_dim=lstm_hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                epochs=epochs,\n",
    "                lr=lr,\n",
    "                sample_size=None  # Set to a number for testing or None for full dataset\n",
    "                )\n",
    "            print(\"EnhancedNeighborBasedLSTM model with rolling window CV completed successfully!\")       \n",
    "    except Exception as e:\n",
    "        print(f\"Error running Enhanced NeighborBasedLSTM model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
